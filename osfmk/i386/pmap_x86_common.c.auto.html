<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>pmap_x86_common.c</title>
<style type="text/css">
.enscript-comment { font-style: italic; color: rgb(178,34,34); }
.enscript-function-name { font-weight: bold; color: rgb(0,0,255); }
.enscript-variable-name { font-weight: bold; color: rgb(184,134,11); }
.enscript-keyword { font-weight: bold; color: rgb(160,32,240); }
.enscript-reference { font-weight: bold; color: rgb(95,158,160); }
.enscript-string { font-weight: bold; color: rgb(188,143,143); }
.enscript-builtin { font-weight: bold; color: rgb(218,112,214); }
.enscript-type { font-weight: bold; color: rgb(34,139,34); }
.enscript-highlight { text-decoration: underline; color: 0; }
</style>
</head>
<body id="top">
<h1 style="margin:8px;" id="f1">pmap_x86_common.c&nbsp;&nbsp;&nbsp;<span style="font-weight: normal; font-size: 0.5em;">[<a href="?txt">plain text</a>]</span></h1>
<hr/>
<div></div>
<pre>
<span class="enscript-comment">/*
 * Copyright (c) 2000-2012 Apple Inc. All rights reserved.
 *
 * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
 * 
 * This file contains Original Code and/or Modifications of Original Code
 * as defined in and that are subject to the Apple Public Source License
 * Version 2.0 (the 'License'). You may not use this file except in
 * compliance with the License. The rights granted to you under the License
 * may not be used to create, or enable the creation or redistribution of,
 * unlawful or unlicensed copies of an Apple operating system, or to
 * circumvent, violate, or enable the circumvention or violation of, any
 * terms of an Apple operating system software license agreement.
 * 
 * Please obtain a copy of the License at
 * <a href="http://www.opensource.apple.com/apsl/">http://www.opensource.apple.com/apsl/</a> and read it before using this file.
 * 
 * The Original Code and all software distributed under the License are
 * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
 * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
 * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
 * Please see the License for the specific language governing rights and
 * limitations under the License.
 * 
 * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
 */</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach_assert.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/pmap.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_map.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/ledger.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;i386/pmap_internal.h&gt;</span>

<span class="enscript-type">void</span>		pmap_remove_range(
			pmap_t		pmap,
			vm_map_offset_t	va,
			pt_entry_t	*spte,
			pt_entry_t	*epte);

<span class="enscript-type">void</span>		pmap_remove_range_options(
			pmap_t		pmap,
			vm_map_offset_t	va,
			pt_entry_t	*spte,
			pt_entry_t	*epte,
			<span class="enscript-type">int</span>		options);

<span class="enscript-type">void</span>		pmap_reusable_range(
			pmap_t		pmap,
			vm_map_offset_t	va,
			pt_entry_t	*spte,
			pt_entry_t	*epte,
			boolean_t	reusable);

uint32_t pmap_update_clear_pte_count;

<span class="enscript-comment">/*
 * The Intel platform can nest at the PDE level, so NBPDE (i.e. 2MB) at a time,
 * on a NBPDE boundary.
 */</span>

<span class="enscript-comment">/* These symbols may be referenced directly by VM */</span>
uint64_t pmap_nesting_size_min = NBPDE;
uint64_t pmap_nesting_size_max = 0 - (uint64_t)NBPDE;

<span class="enscript-comment">/*
 *	kern_return_t pmap_nest(grand, subord, va_start, size)
 *
 *	grand  = the pmap that we will nest subord into
 *	subord = the pmap that goes into the grand
 *	va_start  = start of range in pmap to be inserted
 *	nstart  = start of range in pmap nested pmap
 *	size   = Size of nest area (up to 16TB)
 *
 *	Inserts a pmap into another.  This is used to implement shared segments.
 *
 *	Note that we depend upon higher level VM locks to insure that things don't change while
 *	we are doing this.  For example, VM should not be doing any pmap enters while it is nesting
 *	or do 2 nests at once.
 */</span>

<span class="enscript-comment">/*
 * This routine can nest subtrees either at the PDPT level (1GiB) or at the
 * PDE level (2MiB). We currently disallow disparate offsets for the &quot;subord&quot;
 * container and the &quot;grand&quot; parent. A minor optimization to consider for the
 * future: make the &quot;subord&quot; truly a container rather than a full-fledged
 * pagetable hierarchy which can be unnecessarily sparse (DRK).
 */</span>

kern_return_t <span class="enscript-function-name">pmap_nest</span>(pmap_t grand, pmap_t subord, addr64_t va_start, addr64_t nstart, uint64_t size) {
	vm_map_offset_t	vaddr, nvaddr;
	pd_entry_t	*pde,*npde;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	i;
	uint64_t	num_pde;

	assert(!is_ept_pmap(grand));
	assert(!is_ept_pmap(subord));

	<span class="enscript-keyword">if</span> ((size &amp; (pmap_nesting_size_min-1)) ||
	    (va_start &amp; (pmap_nesting_size_min-1)) ||
	    (nstart &amp; (pmap_nesting_size_min-1)) ||
	    ((size &gt;&gt; 28) &gt; 65536))	<span class="enscript-comment">/* Max size we can nest is 16TB */</span>
		<span class="enscript-keyword">return</span> KERN_INVALID_VALUE;

	<span class="enscript-keyword">if</span>(size == 0) {
		panic(<span class="enscript-string">&quot;pmap_nest: size is invalid - %016llX\n&quot;</span>, size);
	}

	<span class="enscript-keyword">if</span> (va_start != nstart)
		panic(<span class="enscript-string">&quot;pmap_nest: va_start(0x%llx) != nstart(0x%llx)\n&quot;</span>, va_start, nstart);

	PMAP_TRACE(PMAP_CODE(PMAP__NEST) | DBG_FUNC_START,
	(uintptr_t) grand, (uintptr_t) subord,
	    (uintptr_t) (va_start&gt;&gt;32), (uintptr_t) va_start, 0);

	nvaddr = (vm_map_offset_t)nstart;
	num_pde = size &gt;&gt; PDESHIFT;

	PMAP_LOCK(subord);

	subord-&gt;pm_shared = TRUE;

	<span class="enscript-keyword">for</span> (i = 0; i &lt; num_pde;) {
		<span class="enscript-keyword">if</span> (((nvaddr &amp; PDPTMASK) == 0) &amp;&amp; (num_pde - i) &gt;= NPDEPG &amp;&amp; cpu_64bit) {

			npde = pmap64_pdpt(subord, nvaddr);

			<span class="enscript-keyword">while</span> (0 == npde || ((*npde &amp; INTEL_PTE_VALID) == 0)) {
				PMAP_UNLOCK(subord);
				pmap_expand_pdpt(subord, nvaddr, PMAP_EXPAND_OPTIONS_NONE);
				PMAP_LOCK(subord);
				npde = pmap64_pdpt(subord, nvaddr);
			}
			*npde |= INTEL_PDPTE_NESTED;
			nvaddr += NBPDPT;
			i += (uint32_t)NPDEPG;
		}
		<span class="enscript-keyword">else</span> {
			npde = pmap_pde(subord, nvaddr);

			<span class="enscript-keyword">while</span> (0 == npde || ((*npde &amp; INTEL_PTE_VALID) == 0)) {
				PMAP_UNLOCK(subord);
				pmap_expand(subord, nvaddr, PMAP_EXPAND_OPTIONS_NONE);
				PMAP_LOCK(subord);
				npde = pmap_pde(subord, nvaddr);
			}
			nvaddr += NBPDE;
			i++;
		}
	}

	PMAP_UNLOCK(subord);

	vaddr = (vm_map_offset_t)va_start;

	PMAP_LOCK(grand);

	<span class="enscript-keyword">for</span> (i = 0;i &lt; num_pde;) {
		pd_entry_t tpde;

		<span class="enscript-keyword">if</span> (((vaddr &amp; PDPTMASK) == 0) &amp;&amp; ((num_pde - i) &gt;= NPDEPG) &amp;&amp; cpu_64bit) {
			npde = pmap64_pdpt(subord, vaddr);
			<span class="enscript-keyword">if</span> (npde == 0)
				panic(<span class="enscript-string">&quot;pmap_nest: no PDPT, subord %p nstart 0x%llx&quot;</span>, subord, vaddr);
			tpde = *npde;
			pde = pmap64_pdpt(grand, vaddr);
			<span class="enscript-keyword">if</span> (0 == pde) {
				PMAP_UNLOCK(grand);
				pmap_expand_pml4(grand, vaddr, PMAP_EXPAND_OPTIONS_NONE);
				PMAP_LOCK(grand);
				pde = pmap64_pdpt(grand, vaddr);
			}
			<span class="enscript-keyword">if</span> (pde == 0)
				panic(<span class="enscript-string">&quot;pmap_nest: no PDPT, grand  %p vaddr 0x%llx&quot;</span>, grand, vaddr);
			pmap_store_pte(pde, tpde);
			vaddr += NBPDPT;
			i += (uint32_t) NPDEPG;
		}
		<span class="enscript-keyword">else</span> {
			npde = pmap_pde(subord, nstart);
			<span class="enscript-keyword">if</span> (npde == 0)
				panic(<span class="enscript-string">&quot;pmap_nest: no npde, subord %p nstart 0x%llx&quot;</span>, subord, nstart);
			tpde = *npde;
			nstart += NBPDE;
			pde = pmap_pde(grand, vaddr);
			<span class="enscript-keyword">if</span> ((0 == pde) &amp;&amp; cpu_64bit) {
				PMAP_UNLOCK(grand);
				pmap_expand_pdpt(grand, vaddr, PMAP_EXPAND_OPTIONS_NONE);
				PMAP_LOCK(grand);
				pde = pmap_pde(grand, vaddr);
			}

			<span class="enscript-keyword">if</span> (pde == 0)
				panic(<span class="enscript-string">&quot;pmap_nest: no pde, grand  %p vaddr 0x%llx&quot;</span>, grand, vaddr);
			vaddr += NBPDE;
			pmap_store_pte(pde, tpde);
			i++;
		}
	}

	PMAP_UNLOCK(grand);

	PMAP_TRACE(PMAP_CODE(PMAP__NEST) | DBG_FUNC_END, 0, 0, 0, 0, 0);

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 *	kern_return_t pmap_unnest(grand, vaddr)
 *
 *	grand  = the pmap that we will un-nest subord from
 *	vaddr  = start of range in pmap to be unnested
 *
 *	Removes a pmap from another.  This is used to implement shared segments.
 */</span>

kern_return_t <span class="enscript-function-name">pmap_unnest</span>(pmap_t grand, addr64_t vaddr, uint64_t size) {
			
	pd_entry_t *pde;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> i;
	uint64_t num_pde;
	addr64_t va_start, va_end;
	uint64_t npdpt = PMAP_INVALID_PDPTNUM;

	PMAP_TRACE(PMAP_CODE(PMAP__UNNEST) | DBG_FUNC_START,
	    (uintptr_t) grand, 
	    (uintptr_t) (vaddr&gt;&gt;32), (uintptr_t) vaddr, 0, 0);

	<span class="enscript-keyword">if</span> ((size &amp; (pmap_nesting_size_min-1)) ||
	    (vaddr &amp; (pmap_nesting_size_min-1))) {
		panic(<span class="enscript-string">&quot;pmap_unnest(%p,0x%llx,0x%llx): unaligned...\n&quot;</span>,
		    grand, vaddr, size);
	}

	assert(!is_ept_pmap(grand));

	<span class="enscript-comment">/* align everything to PDE boundaries */</span>
	va_start = vaddr &amp; ~(NBPDE-1);
	va_end = (vaddr + size + NBPDE - 1) &amp; ~(NBPDE-1);
	size = va_end - va_start;

	PMAP_LOCK(grand);

	num_pde = size &gt;&gt; PDESHIFT;
	vaddr = va_start;

	<span class="enscript-keyword">for</span> (i = 0; i &lt; num_pde; ) {
		<span class="enscript-keyword">if</span> ((pdptnum(grand, vaddr) != npdpt) &amp;&amp; cpu_64bit) {
			npdpt = pdptnum(grand, vaddr);
			pde = pmap64_pdpt(grand, vaddr);
			<span class="enscript-keyword">if</span> (pde &amp;&amp; (*pde &amp; INTEL_PDPTE_NESTED)) {
				pmap_store_pte(pde, (pd_entry_t)0);
				i += (uint32_t) NPDEPG;
				vaddr += NBPDPT;
				<span class="enscript-keyword">continue</span>;
			}
		}
		pde = pmap_pde(grand, (vm_map_offset_t)vaddr);
		<span class="enscript-keyword">if</span> (pde == 0)
			panic(<span class="enscript-string">&quot;pmap_unnest: no pde, grand %p vaddr 0x%llx\n&quot;</span>, grand, vaddr);
		pmap_store_pte(pde, (pd_entry_t)0);
		i++;
		vaddr += NBPDE;
	}

	PMAP_UPDATE_TLBS(grand, va_start, va_end);

	PMAP_UNLOCK(grand);
		
	PMAP_TRACE(PMAP_CODE(PMAP__UNNEST) | DBG_FUNC_END, 0, 0, 0, 0, 0);

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

kern_return_t
<span class="enscript-function-name">pmap_unnest_options</span>(
	pmap_t grand,
	addr64_t vaddr,
	__unused uint64_t size,
	__unused <span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> options) {
	<span class="enscript-keyword">return</span> pmap_unnest(grand, vaddr, size);
}

<span class="enscript-comment">/* Invoked by the Mach VM to determine the platform specific unnest region */</span>

boolean_t <span class="enscript-function-name">pmap_adjust_unnest_parameters</span>(pmap_t p, vm_map_offset_t *s, vm_map_offset_t *e) {
	pd_entry_t *pdpte;
	boolean_t rval = FALSE;

	<span class="enscript-keyword">if</span> (!cpu_64bit)
		<span class="enscript-keyword">return</span> rval;

	PMAP_LOCK(p);

	pdpte = pmap64_pdpt(p, *s);
	<span class="enscript-keyword">if</span> (pdpte &amp;&amp; (*pdpte &amp; INTEL_PDPTE_NESTED)) {
		*s &amp;= ~(NBPDPT -1);
		rval = TRUE;
	}

	pdpte = pmap64_pdpt(p, *e);
	<span class="enscript-keyword">if</span> (pdpte &amp;&amp; (*pdpte &amp; INTEL_PDPTE_NESTED)) {
		*e = ((*e + NBPDPT) &amp; ~(NBPDPT -1));
		rval = TRUE;
	}

	PMAP_UNLOCK(p);

	<span class="enscript-keyword">return</span> rval;
}

<span class="enscript-comment">/*
 * pmap_find_phys returns the (4K) physical page number containing a
 * given virtual address in a given pmap.
 * Note that pmap_pte may return a pde if this virtual address is
 * mapped by a large page and this is taken into account in order
 * to return the correct page number in this case.
 */</span>
ppnum_t
<span class="enscript-function-name">pmap_find_phys</span>(pmap_t pmap, addr64_t va)
{
	pt_entry_t	*ptp;
	pd_entry_t	*pdep;
	ppnum_t		ppn = 0;
	pd_entry_t	pde;
	pt_entry_t	pte;
	boolean_t	is_ept;

	is_ept = is_ept_pmap(pmap);

	mp_disable_preemption();

	<span class="enscript-comment">/* This refcount test is a band-aid--several infrastructural changes
	 * are necessary to eliminate invocation of this routine from arbitrary
	 * contexts.
	 */</span>
	
	<span class="enscript-keyword">if</span> (!pmap-&gt;ref_count)
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">pfp_exit</span>;

	pdep = pmap_pde(pmap, va);

	<span class="enscript-keyword">if</span> ((pdep != PD_ENTRY_NULL) &amp;&amp; ((pde = *pdep) &amp; PTE_VALID_MASK(is_ept))) {
		<span class="enscript-keyword">if</span> (pde &amp; PTE_PS) {
			ppn = (ppnum_t) i386_btop(pte_to_pa(pde));
			ppn += (ppnum_t) ptenum(va);
		}
		<span class="enscript-keyword">else</span> {
			ptp = pmap_pte(pmap, va);
			<span class="enscript-keyword">if</span> ((PT_ENTRY_NULL != ptp) &amp;&amp; (((pte = *ptp) &amp; PTE_VALID_MASK(is_ept)) != 0)) {
				ppn = (ppnum_t) i386_btop(pte_to_pa(pte));
			}
		}
	}
<span class="enscript-reference">pfp_exit</span>:	
	mp_enable_preemption();

        <span class="enscript-keyword">return</span> ppn;
}

<span class="enscript-comment">/*
 * Update cache attributes for all extant managed mappings.
 * Assumes PV for this page is locked, and that the page
 * is managed. We assume that this physical page may be mapped in
 * both EPT and normal Intel PTEs, so we convert the attributes
 * to the corresponding format for each pmap.
 *
 * We assert that the passed set of attributes is a subset of the
 * PHYS_CACHEABILITY_MASK.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_update_cache_attributes_locked</span>(ppnum_t pn, <span class="enscript-type">unsigned</span> attributes) {
	pv_rooted_entry_t	pv_h, pv_e;
	pv_hashed_entry_t       pvh_e, nexth;
	vm_map_offset_t vaddr;
	pmap_t	pmap;
	pt_entry_t	*ptep;
	boolean_t	is_ept;
	<span class="enscript-type">unsigned</span>	ept_attributes;
	
	assert(IS_MANAGED_PAGE(pn));
	assert(((~PHYS_CACHEABILITY_MASK) &amp; attributes) == 0);

	<span class="enscript-comment">/* We don't support the PTA bit for EPT PTEs */</span>
	<span class="enscript-keyword">if</span> (attributes &amp; INTEL_PTE_NCACHE)
		ept_attributes = INTEL_EPT_NCACHE;
	<span class="enscript-keyword">else</span>
		ept_attributes = INTEL_EPT_WB;

	pv_h = pai_to_pvh(pn);
	<span class="enscript-comment">/* TODO: translate the PHYS_* bits to PTE bits, while they're
	 * currently identical, they may not remain so
	 * Potential optimization (here and in page_protect),
	 * parallel shootdowns, check for redundant
	 * attribute modifications.
	 */</span>
	
	<span class="enscript-comment">/*
	 * Alter attributes on all mappings
	 */</span>
	<span class="enscript-keyword">if</span> (pv_h-&gt;pmap != PMAP_NULL) {
		pv_e = pv_h;
		pvh_e = (pv_hashed_entry_t)pv_e;

		<span class="enscript-keyword">do</span> {
			pmap = pv_e-&gt;pmap;
			vaddr = pv_e-&gt;va;
			ptep = pmap_pte(pmap, vaddr);
			
			<span class="enscript-keyword">if</span> (0 == ptep)
				panic(<span class="enscript-string">&quot;pmap_update_cache_attributes_locked: Missing PTE, pmap: %p, pn: 0x%x vaddr: 0x%llx kernel_pmap: %p&quot;</span>, pmap, pn, vaddr, kernel_pmap);

			is_ept = is_ept_pmap(pmap);

			nexth = (pv_hashed_entry_t)queue_next(&amp;pvh_e-&gt;qlink);
			<span class="enscript-keyword">if</span> (!is_ept) {
				pmap_update_pte(ptep, PHYS_CACHEABILITY_MASK, attributes);
			} <span class="enscript-keyword">else</span> {
				pmap_update_pte(ptep, INTEL_EPT_CACHE_MASK, ept_attributes);
			}
			PMAP_UPDATE_TLBS(pmap, vaddr, vaddr + PAGE_SIZE);
			pvh_e = nexth;
		} <span class="enscript-keyword">while</span> ((pv_e = (pv_rooted_entry_t)nexth) != pv_h);
	}
}

<span class="enscript-type">void</span> <span class="enscript-function-name">x86_filter_TLB_coherency_interrupts</span>(boolean_t dofilter) {
	assert(ml_get_interrupts_enabled() == 0 || get_preemption_level() != 0);

	<span class="enscript-keyword">if</span> (dofilter) {
		CPU_CR3_MARK_INACTIVE();
	} <span class="enscript-keyword">else</span> {
		CPU_CR3_MARK_ACTIVE();
		mfence();
		<span class="enscript-keyword">if</span> (current_cpu_datap()-&gt;cpu_tlb_invalid)
			process_pmap_updates();
	}
}


<span class="enscript-comment">/*
 *	Insert the given physical page (p) at
 *	the specified virtual address (v) in the
 *	target physical map with the protection requested.
 *
 *	If specified, the page will be wired down, meaning
 *	that the related pte cannot be reclaimed.
 *
 *	NB:  This is the only routine which MAY NOT lazy-evaluate
 *	or lose information.  That is, this routine must actually
 *	insert this page into the given map NOW.
 */</span>

<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_enter</span>(
	<span class="enscript-type">register</span> pmap_t		pmap,
 	vm_map_offset_t		vaddr,
	ppnum_t                 pn,
	vm_prot_t		prot,
	vm_prot_t		fault_type,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> 		flags,
	boolean_t		wired)
{
	(<span class="enscript-type">void</span>) pmap_enter_options(pmap, vaddr, pn, prot, fault_type, flags, wired, PMAP_EXPAND_OPTIONS_NONE, NULL);
}


kern_return_t
<span class="enscript-function-name">pmap_enter_options</span>(
	<span class="enscript-type">register</span> pmap_t		pmap,
 	vm_map_offset_t		vaddr,
	ppnum_t                 pn,
	vm_prot_t		prot,
	__unused vm_prot_t	fault_type,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> 		flags,
	boolean_t		wired,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		options,
	<span class="enscript-type">void</span>			*arg)
{
	pt_entry_t		*pte;
	pv_rooted_entry_t	pv_h;
	ppnum_t			pai;
	pv_hashed_entry_t	pvh_e;
	pv_hashed_entry_t	pvh_new;
	pt_entry_t		template;
	pmap_paddr_t		old_pa;
	pmap_paddr_t		pa = (pmap_paddr_t) i386_ptob(pn);
	boolean_t		need_tlbflush = FALSE;
	boolean_t		set_NX;
	<span class="enscript-type">char</span>			oattr;
	boolean_t		old_pa_locked;
	<span class="enscript-comment">/* 2MiB mappings are confined to x86_64 by VM */</span>
	boolean_t		superpage = flags &amp; VM_MEM_SUPERPAGE;
	vm_object_t		delpage_pm_obj = NULL;
	uint64_t		delpage_pde_index = 0;
	pt_entry_t		old_pte;
	kern_return_t		kr_expand;
	boolean_t		is_ept;

	pmap_intr_assert();

	<span class="enscript-keyword">if</span> (pmap == PMAP_NULL)
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;

	is_ept = is_ept_pmap(pmap);

	<span class="enscript-comment">/* N.B. We can be supplied a zero page frame in the NOENTER case, it's an
	 * unused value for that scenario.
	 */</span>
	assert(pn != vm_page_fictitious_addr);

	<span class="enscript-keyword">if</span> (pn == vm_page_guard_addr)
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;

	PMAP_TRACE(PMAP_CODE(PMAP__ENTER) | DBG_FUNC_START,
	    pmap,
	    (uint32_t) (vaddr &gt;&gt; 32), (uint32_t) vaddr,
	    pn, prot);

	<span class="enscript-keyword">if</span> ((prot &amp; VM_PROT_EXECUTE) || !nx_enabled || !pmap-&gt;nx_enabled)
		set_NX = FALSE;
	<span class="enscript-keyword">else</span>
		set_NX = TRUE;

	<span class="enscript-keyword">if</span> (__improbable(set_NX &amp;&amp; (pmap == kernel_pmap) &amp;&amp; ((pmap_disable_kstack_nx &amp;&amp; (flags &amp; VM_MEM_STACK)) || (pmap_disable_kheap_nx &amp;&amp; !(flags &amp; VM_MEM_STACK))))) {
		set_NX = FALSE;
	}

	<span class="enscript-comment">/*
	 *	Must allocate a new pvlist entry while we're unlocked;
	 *	zalloc may cause pageout (which will lock the pmap system).
	 *	If we determine we need a pvlist entry, we will unlock
	 *	and allocate one.  Then we will retry, throughing away
	 *	the allocated entry later (if we no longer need it).
	 */</span>

	pvh_new = PV_HASHED_ENTRY_NULL;
<span class="enscript-reference">Retry</span>:
	pvh_e = PV_HASHED_ENTRY_NULL;

	PMAP_LOCK(pmap);

	<span class="enscript-comment">/*
	 *	Expand pmap to include this pte.  Assume that
	 *	pmap is always expanded to include enough hardware
	 *	pages to map one VM page.
	 */</span>
	 <span class="enscript-keyword">if</span>(superpage) {
	 	<span class="enscript-keyword">while</span> ((pte = pmap64_pde(pmap, vaddr)) == PD_ENTRY_NULL) {
			<span class="enscript-comment">/* need room for another pde entry */</span>
			PMAP_UNLOCK(pmap);
			kr_expand = pmap_expand_pdpt(pmap, vaddr, options);
			<span class="enscript-keyword">if</span> (kr_expand != KERN_SUCCESS)
				<span class="enscript-keyword">return</span> kr_expand;
			PMAP_LOCK(pmap);
		}
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-keyword">while</span> ((pte = pmap_pte(pmap, vaddr)) == PT_ENTRY_NULL) {
			<span class="enscript-comment">/*
			 * Must unlock to expand the pmap
			 * going to grow pde level page(s)
			 */</span>
			PMAP_UNLOCK(pmap);
			kr_expand = pmap_expand(pmap, vaddr, options);
			<span class="enscript-keyword">if</span> (kr_expand != KERN_SUCCESS)
				<span class="enscript-keyword">return</span> kr_expand;
			PMAP_LOCK(pmap);
		}
	}
	<span class="enscript-keyword">if</span> (options &amp; PMAP_EXPAND_OPTIONS_NOENTER) {
		PMAP_UNLOCK(pmap);
		<span class="enscript-keyword">return</span> KERN_SUCCESS;
	}

	<span class="enscript-keyword">if</span> (superpage &amp;&amp; *pte &amp;&amp; !(*pte &amp; PTE_PS)) {
		<span class="enscript-comment">/*
		 * There is still an empty page table mapped that
		 * was used for a previous base page mapping.
		 * Remember the PDE and the PDE index, so that we
		 * can free the page at the end of this function.
		 */</span>
		delpage_pde_index = pdeidx(pmap, vaddr);
		delpage_pm_obj = pmap-&gt;pm_obj;
		*pte = 0;
	}

	old_pa = pte_to_pa(*pte);
	pai = pa_index(old_pa);
	old_pa_locked = FALSE;

	<span class="enscript-keyword">if</span> (old_pa == 0 &amp;&amp;
	    (*pte &amp; PTE_COMPRESSED)) {
		<span class="enscript-comment">/* one less &quot;compressed&quot; */</span>
		OSAddAtomic64(-1, &amp;pmap-&gt;stats.compressed);
		<span class="enscript-comment">/* marker will be cleared below */</span>
	}

	<span class="enscript-comment">/*
	 * if we have a previous managed page, lock the pv entry now. after
	 * we lock it, check to see if someone beat us to the lock and if so
	 * drop the lock
	 */</span>
	<span class="enscript-keyword">if</span> ((0 != old_pa) &amp;&amp; IS_MANAGED_PAGE(pai)) {
		LOCK_PVH(pai);
		old_pa_locked = TRUE;
		old_pa = pte_to_pa(*pte);
		<span class="enscript-keyword">if</span> (0 == old_pa) {
			UNLOCK_PVH(pai);	<span class="enscript-comment">/* another path beat us to it */</span>
			old_pa_locked = FALSE;
		}
	}

	<span class="enscript-comment">/*
	 *	Special case if the incoming physical page is already mapped
	 *	at this address.
	 */</span>
	<span class="enscript-keyword">if</span> (old_pa == pa) {
		pt_entry_t old_attributes =
		    *pte &amp; ~(PTE_REF(is_ept) | PTE_MOD(is_ept));

		<span class="enscript-comment">/*
	         *	May be changing its wired attribute or protection
	         */</span>

		template =  pa_to_pte(pa);

		<span class="enscript-comment">/* ?: WORTH ASSERTING THAT AT LEAST ONE RWX (implicit valid) PASSED FOR EPT? */</span>
		<span class="enscript-keyword">if</span> (!is_ept) {
			template |= INTEL_PTE_VALID;
		} <span class="enscript-keyword">else</span> {
			template |= INTEL_EPT_IPTA;
		}

		template |= pmap_get_cache_attributes(pa_index(pa), is_ept);

		<span class="enscript-comment">/*
		 * We don't support passing VM_MEM_NOT_CACHEABLE flags for EPT PTEs
		 */</span>
		<span class="enscript-keyword">if</span> (!is_ept &amp;&amp; (VM_MEM_NOT_CACHEABLE ==
		    (flags &amp; (VM_MEM_NOT_CACHEABLE | VM_WIMG_USE_DEFAULT)))) {
			<span class="enscript-keyword">if</span> (!(flags &amp; VM_MEM_GUARDED))
				template |= INTEL_PTE_PTA;
			template |= INTEL_PTE_NCACHE;
		}
		<span class="enscript-keyword">if</span> (pmap != kernel_pmap &amp;&amp; !is_ept)
			template |= INTEL_PTE_USER;

		<span class="enscript-keyword">if</span> (prot &amp; VM_PROT_READ)
			template |= PTE_READ(is_ept);

		<span class="enscript-keyword">if</span> (prot &amp; VM_PROT_WRITE) {
			template |= PTE_WRITE(is_ept);
			<span class="enscript-keyword">if</span> (is_ept &amp;&amp; !pmap_ept_support_ad) {
				template |= PTE_MOD(is_ept);
				<span class="enscript-keyword">if</span> (old_pa_locked) {
					assert(IS_MANAGED_PAGE(pai));
					pmap_phys_attributes[pai] |= PHYS_MODIFIED;
				}
			}
		}
		<span class="enscript-keyword">if</span> (prot &amp; VM_PROT_EXECUTE) {
			assert(set_NX == 0);
			template = pte_set_ex(template, is_ept);
		}

		<span class="enscript-keyword">if</span> (set_NX)
			template = pte_remove_ex(template, is_ept);

		<span class="enscript-keyword">if</span> (wired) {
			template |= PTE_WIRED;
			<span class="enscript-keyword">if</span> (!iswired(old_attributes))  {
				OSAddAtomic(+1, &amp;pmap-&gt;stats.wired_count);
				pmap_ledger_credit(pmap, task_ledgers.wired_mem, PAGE_SIZE);
			}
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> (iswired(old_attributes)) {
				assert(pmap-&gt;stats.wired_count &gt;= 1);
				OSAddAtomic(-1, &amp;pmap-&gt;stats.wired_count);
				pmap_ledger_debit(pmap, task_ledgers.wired_mem, PAGE_SIZE);
			}
		}

		<span class="enscript-keyword">if</span> (superpage)		<span class="enscript-comment">/* this path can not be used */</span>
			template |= PTE_PS;	<span class="enscript-comment">/* to change the page size! */</span>

		<span class="enscript-keyword">if</span> (old_attributes == template)
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">dont_update_pte</span>;

		<span class="enscript-comment">/* Determine delta, PV locked */</span>
		need_tlbflush =
		    ((old_attributes ^ template) != PTE_WIRED);
		
		<span class="enscript-keyword">if</span> (need_tlbflush == TRUE &amp;&amp; !(old_attributes &amp; PTE_WRITE(is_ept))) {
			<span class="enscript-keyword">if</span> ((old_attributes ^ template) == PTE_WRITE(is_ept))
				need_tlbflush = FALSE;
		}

		<span class="enscript-comment">/* For hardware that doesn't have EPT AD support, we always set REFMOD for EPT PTEs */</span>
		<span class="enscript-keyword">if</span> (is_ept &amp;&amp; !pmap_ept_support_ad) {
			template |= PTE_REF(is_ept);
			<span class="enscript-keyword">if</span> (old_pa_locked) {
				assert(IS_MANAGED_PAGE(pai));
				pmap_phys_attributes[pai] |= PHYS_REFERENCED;
			}
		}

		<span class="enscript-comment">/* store modified PTE and preserve RC bits */</span>
		pt_entry_t npte, opte;;
		<span class="enscript-keyword">do</span> {
			opte = *pte;
			npte = template | (opte &amp; (PTE_REF(is_ept) | PTE_MOD(is_ept)));
		} <span class="enscript-keyword">while</span> (!pmap_cmpx_pte(pte, opte, npte));
<span class="enscript-reference">dont_update_pte</span>:
		<span class="enscript-keyword">if</span> (old_pa_locked) {
			UNLOCK_PVH(pai);
			old_pa_locked = FALSE;
		}
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">Done</span>;
	}

	<span class="enscript-comment">/*
	 *	Outline of code from here:
	 *	   1) If va was mapped, update TLBs, remove the mapping
	 *	      and remove old pvlist entry.
	 *	   2) Add pvlist entry for new mapping
	 *	   3) Enter new mapping.
	 *
	 *	If the old physical page is not managed step 1) is skipped
	 *	(except for updating the TLBs), and the mapping is
	 *	overwritten at step 3).  If the new physical page is not
	 *	managed, step 2) is skipped.
	 */</span>

	<span class="enscript-keyword">if</span> (old_pa != (pmap_paddr_t) 0) {

		<span class="enscript-comment">/*
	         *	Don't do anything to pages outside valid memory here.
	         *	Instead convince the code that enters a new mapping
	         *	to overwrite the old one.
	         */</span>

		<span class="enscript-comment">/* invalidate the PTE */</span>
		pmap_update_pte(pte, PTE_VALID_MASK(is_ept), 0);
		<span class="enscript-comment">/* propagate invalidate everywhere */</span>
		PMAP_UPDATE_TLBS(pmap, vaddr, vaddr + PAGE_SIZE);
		<span class="enscript-comment">/* remember reference and change */</span>
		old_pte	= *pte;
		oattr = (<span class="enscript-type">char</span>) (old_pte &amp; (PTE_MOD(is_ept) | PTE_REF(is_ept)));
		<span class="enscript-comment">/* completely invalidate the PTE */</span>
		pmap_store_pte(pte, 0);

		<span class="enscript-keyword">if</span> (IS_MANAGED_PAGE(pai)) {
			pmap_assert(old_pa_locked == TRUE);
			pmap_ledger_debit(pmap, task_ledgers.phys_mem, PAGE_SIZE);
			pmap_ledger_debit(pmap, task_ledgers.phys_footprint, PAGE_SIZE);			
			assert(pmap-&gt;stats.resident_count &gt;= 1);
			OSAddAtomic(-1, &amp;pmap-&gt;stats.resident_count);
			<span class="enscript-keyword">if</span> (pmap != kernel_pmap) {
				<span class="enscript-keyword">if</span> (IS_REUSABLE_PAGE(pai)) {
					assert(pmap-&gt;stats.reusable &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.reusable);
				} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (IS_INTERNAL_PAGE(pai)) {
					assert(pmap-&gt;stats.internal &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.internal);
				} <span class="enscript-keyword">else</span> {
					assert(pmap-&gt;stats.external &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.external);
				}
			}
			<span class="enscript-keyword">if</span> (iswired(*pte)) {
				assert(pmap-&gt;stats.wired_count &gt;= 1);
				OSAddAtomic(-1, &amp;pmap-&gt;stats.wired_count);
				pmap_ledger_debit(pmap, task_ledgers.wired_mem,
				    PAGE_SIZE);
			}

			<span class="enscript-keyword">if</span> (!is_ept) {
				pmap_phys_attributes[pai] |= oattr;
			} <span class="enscript-keyword">else</span> {
				pmap_phys_attributes[pai] |= ept_refmod_to_physmap(oattr);
			}

			<span class="enscript-comment">/*
			 *	Remove the mapping from the pvlist for
			 *	this physical page.
			 *      We'll end up with either a rooted pv or a
			 *      hashed pv
			 */</span>
			pvh_e = pmap_pv_remove(pmap, vaddr, (ppnum_t *) &amp;pai, &amp;old_pte);

		} <span class="enscript-keyword">else</span> {

			<span class="enscript-comment">/*
			 *	old_pa is not managed.
			 *	Do removal part of accounting.
			 */</span>

			<span class="enscript-keyword">if</span> (pmap != kernel_pmap) {
#<span class="enscript-reference">if</span> 00
				assert(pmap-&gt;stats.device &gt; 0);
				OSAddAtomic(-1, &amp;pmap-&gt;stats.device);
#<span class="enscript-reference">endif</span>
			}
			<span class="enscript-keyword">if</span> (iswired(*pte)) {
				assert(pmap-&gt;stats.wired_count &gt;= 1);
				OSAddAtomic(-1, &amp;pmap-&gt;stats.wired_count);
				pmap_ledger_debit(pmap, task_ledgers.wired_mem, PAGE_SIZE);
			}
		}
	}

	<span class="enscript-comment">/*
	 * if we had a previously managed paged locked, unlock it now
	 */</span>
	<span class="enscript-keyword">if</span> (old_pa_locked) {
		UNLOCK_PVH(pai);
		old_pa_locked = FALSE;
	}

	pai = pa_index(pa);	<span class="enscript-comment">/* now working with new incoming phys page */</span>
	<span class="enscript-keyword">if</span> (IS_MANAGED_PAGE(pai)) {

		<span class="enscript-comment">/*
	         *	Step 2) Enter the mapping in the PV list for this
	         *	physical page.
	         */</span>
		pv_h = pai_to_pvh(pai);

		LOCK_PVH(pai);

		<span class="enscript-keyword">if</span> (pv_h-&gt;pmap == PMAP_NULL) {
			<span class="enscript-comment">/*
			 *	No mappings yet, use rooted pv
			 */</span>
			pv_h-&gt;va = vaddr;
			pv_h-&gt;pmap = pmap;
			queue_init(&amp;pv_h-&gt;qlink);

			<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_INTERNAL) {
				pmap_phys_attributes[pai] |= PHYS_INTERNAL;
			} <span class="enscript-keyword">else</span> {
				pmap_phys_attributes[pai] &amp;= ~PHYS_INTERNAL;
			}
			<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_REUSABLE) {
				pmap_phys_attributes[pai] |= PHYS_REUSABLE;
			} <span class="enscript-keyword">else</span> {
				pmap_phys_attributes[pai] &amp;= ~PHYS_REUSABLE;
			}
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/*
			 *	Add new pv_hashed_entry after header.
			 */</span>
			<span class="enscript-keyword">if</span> ((PV_HASHED_ENTRY_NULL == pvh_e) &amp;&amp; pvh_new) {
				pvh_e = pvh_new;
				pvh_new = PV_HASHED_ENTRY_NULL;
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (PV_HASHED_ENTRY_NULL == pvh_e) {
				PV_HASHED_ALLOC(&amp;pvh_e);
				<span class="enscript-keyword">if</span> (PV_HASHED_ENTRY_NULL == pvh_e) {
					<span class="enscript-comment">/*
					 * the pv list is empty. if we are on
					 * the kernel pmap we'll use one of
					 * the special private kernel pv_e's,
					 * else, we need to unlock
					 * everything, zalloc a pv_e, and
					 * restart bringing in the pv_e with
					 * us.
					 */</span>
					<span class="enscript-keyword">if</span> (kernel_pmap == pmap) {
						PV_HASHED_KERN_ALLOC(&amp;pvh_e);
					} <span class="enscript-keyword">else</span> {
						UNLOCK_PVH(pai);
						PMAP_UNLOCK(pmap);
						pmap_pv_throttle(pmap);
						pvh_new = (pv_hashed_entry_t) zalloc(pv_hashed_list_zone);
						<span class="enscript-keyword">goto</span> <span class="enscript-reference">Retry</span>;
					}
				}
			}
			
			<span class="enscript-keyword">if</span> (PV_HASHED_ENTRY_NULL == pvh_e)
				panic(<span class="enscript-string">&quot;Mapping alias chain exhaustion, possibly induced by numerous kernel virtual double mappings&quot;</span>);

			pvh_e-&gt;va = vaddr;
			pvh_e-&gt;pmap = pmap;
			pvh_e-&gt;ppn = pn;
			pv_hash_add(pvh_e, pv_h);

			<span class="enscript-comment">/*
			 *	Remember that we used the pvlist entry.
			 */</span>
			pvh_e = PV_HASHED_ENTRY_NULL;
		}

		<span class="enscript-comment">/*
	         * only count the mapping
	         * for 'managed memory'
	         */</span>
		pmap_ledger_credit(pmap, task_ledgers.phys_mem, PAGE_SIZE);
		pmap_ledger_credit(pmap, task_ledgers.phys_footprint, PAGE_SIZE);		
		OSAddAtomic(+1,  &amp;pmap-&gt;stats.resident_count);
		<span class="enscript-keyword">if</span> (pmap-&gt;stats.resident_count &gt; pmap-&gt;stats.resident_max) {
			pmap-&gt;stats.resident_max = pmap-&gt;stats.resident_count;
		}
		<span class="enscript-keyword">if</span> (pmap != kernel_pmap) {
			<span class="enscript-keyword">if</span> (IS_REUSABLE_PAGE(pai)) {
				OSAddAtomic(+1, &amp;pmap-&gt;stats.reusable);
				PMAP_STATS_PEAK(pmap-&gt;stats.reusable);
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (IS_INTERNAL_PAGE(pai)) {
				OSAddAtomic(+1, &amp;pmap-&gt;stats.internal);
				PMAP_STATS_PEAK(pmap-&gt;stats.internal);
			} <span class="enscript-keyword">else</span> {
				OSAddAtomic(+1, &amp;pmap-&gt;stats.external);
				PMAP_STATS_PEAK(pmap-&gt;stats.external);
			}
		}
	} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (last_managed_page == 0) {
		<span class="enscript-comment">/* Account for early mappings created before &quot;managed pages&quot;
		 * are determined. Consider consulting the available DRAM map.
		 */</span>
		pmap_ledger_credit(pmap, task_ledgers.phys_mem, PAGE_SIZE);
		pmap_ledger_credit(pmap, task_ledgers.phys_footprint, PAGE_SIZE);		
		OSAddAtomic(+1,  &amp;pmap-&gt;stats.resident_count);
		<span class="enscript-keyword">if</span> (pmap != kernel_pmap) {
#<span class="enscript-reference">if</span> 00
			OSAddAtomic(+1, &amp;pmap-&gt;stats.device);
			PMAP_STATS_PEAK(pmap-&gt;stats.device);
#<span class="enscript-reference">endif</span>
		}
	}
	<span class="enscript-comment">/*
	 * Step 3) Enter the mapping.
	 *
	 *	Build a template to speed up entering -
	 *	only the pfn changes.
	 */</span>
	template = pa_to_pte(pa);

	<span class="enscript-keyword">if</span> (!is_ept) {
		template |= INTEL_PTE_VALID;
	} <span class="enscript-keyword">else</span> {
		template |= INTEL_EPT_IPTA;
	}


	<span class="enscript-comment">/*
	 * DRK: It may be worth asserting on cache attribute flags that diverge
	 * from the existing physical page attributes.
	 */</span>

	template |= pmap_get_cache_attributes(pa_index(pa), is_ept);

	<span class="enscript-comment">/*
	 * We don't support passing VM_MEM_NOT_CACHEABLE flags for EPT PTEs
	 */</span>
	<span class="enscript-keyword">if</span> (!is_ept &amp;&amp; (flags &amp; VM_MEM_NOT_CACHEABLE)) {
		<span class="enscript-keyword">if</span> (!(flags &amp; VM_MEM_GUARDED))
			template |= INTEL_PTE_PTA;
		template |= INTEL_PTE_NCACHE;
	}
	<span class="enscript-keyword">if</span> (pmap != kernel_pmap &amp;&amp; !is_ept)
		template |= INTEL_PTE_USER;
	<span class="enscript-keyword">if</span> (prot &amp; VM_PROT_READ)
		template |= PTE_READ(is_ept);
	<span class="enscript-keyword">if</span> (prot &amp; VM_PROT_WRITE) {
		template |= PTE_WRITE(is_ept);
		<span class="enscript-keyword">if</span> (is_ept &amp;&amp; !pmap_ept_support_ad) {
			template |= PTE_MOD(is_ept);
			<span class="enscript-keyword">if</span> (IS_MANAGED_PAGE(pai))
				pmap_phys_attributes[pai] |= PHYS_MODIFIED;
		}
	}
	<span class="enscript-keyword">if</span> (prot &amp; VM_PROT_EXECUTE) {
		assert(set_NX == 0);
		template = pte_set_ex(template, is_ept);
	}

	<span class="enscript-keyword">if</span> (set_NX)
		template = pte_remove_ex(template, is_ept);
	<span class="enscript-keyword">if</span> (wired) {
		template |= INTEL_PTE_WIRED;
		OSAddAtomic(+1,  &amp; pmap-&gt;stats.wired_count);
		pmap_ledger_credit(pmap, task_ledgers.wired_mem, PAGE_SIZE);
	}
	<span class="enscript-keyword">if</span> (superpage)
		template |= INTEL_PTE_PS;

	<span class="enscript-comment">/* For hardware that doesn't have EPT AD support, we always set REFMOD for EPT PTEs */</span>
	<span class="enscript-keyword">if</span> (is_ept &amp;&amp; !pmap_ept_support_ad) {
		template |= PTE_REF(is_ept);
		<span class="enscript-keyword">if</span> (IS_MANAGED_PAGE(pai))
			pmap_phys_attributes[pai] |= PHYS_REFERENCED;
	}

	pmap_store_pte(pte, template);

	<span class="enscript-comment">/*
	 * if this was a managed page we delayed unlocking the pv until here
	 * to prevent pmap_page_protect et al from finding it until the pte
	 * has been stored
	 */</span>
	<span class="enscript-keyword">if</span> (IS_MANAGED_PAGE(pai)) {
		UNLOCK_PVH(pai);
	}
<span class="enscript-reference">Done</span>:
	<span class="enscript-keyword">if</span> (need_tlbflush == TRUE) {
		<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_NOFLUSH)
			PMAP_UPDATE_TLBS_DELAYED(pmap, vaddr, vaddr + PAGE_SIZE, (pmap_flush_context *)arg);
		<span class="enscript-keyword">else</span>
			PMAP_UPDATE_TLBS(pmap, vaddr, vaddr + PAGE_SIZE);
	}
	<span class="enscript-keyword">if</span> (pvh_e != PV_HASHED_ENTRY_NULL) {
		PV_HASHED_FREE_LIST(pvh_e, pvh_e, 1);
	}
	<span class="enscript-keyword">if</span> (pvh_new != PV_HASHED_ENTRY_NULL) {
		PV_HASHED_KERN_FREE_LIST(pvh_new, pvh_new, 1);
	}
	PMAP_UNLOCK(pmap);

	<span class="enscript-keyword">if</span> (delpage_pm_obj) {
		vm_page_t m;

		vm_object_lock(delpage_pm_obj);
		m = vm_page_lookup(delpage_pm_obj, (delpage_pde_index * PAGE_SIZE));
		<span class="enscript-keyword">if</span> (m == VM_PAGE_NULL)
		    panic(<span class="enscript-string">&quot;pmap_enter: pte page not in object&quot;</span>);
		VM_PAGE_FREE(m);
		vm_object_unlock(delpage_pm_obj);
		OSAddAtomic(-1,  &amp;inuse_ptepages_count);
		PMAP_ZINFO_PFREE(pmap, PAGE_SIZE);
	}

	PMAP_TRACE(PMAP_CODE(PMAP__ENTER) | DBG_FUNC_END, 0, 0, 0, 0, 0);
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 *	Remove a range of hardware page-table entries.
 *	The entries given are the first (inclusive)
 *	and last (exclusive) entries for the VM pages.
 *	The virtual address is the va for the first pte.
 *
 *	The pmap must be locked.
 *	If the pmap is not the kernel pmap, the range must lie
 *	entirely within one pte-page.  This is NOT checked.
 *	Assumes that the pte-page exists.
 */</span>

<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_remove_range</span>(
	pmap_t			pmap,
	vm_map_offset_t		start_vaddr,
	pt_entry_t		*spte,
	pt_entry_t		*epte)
{
	pmap_remove_range_options(pmap, start_vaddr, spte, epte,
				  PMAP_OPTIONS_REMOVE);
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_remove_range_options</span>(
	pmap_t			pmap,
	vm_map_offset_t		start_vaddr,
	pt_entry_t		*spte,
	pt_entry_t		*epte,
	<span class="enscript-type">int</span>			options)
{
	pt_entry_t		*cpte;
	pv_hashed_entry_t       pvh_et = PV_HASHED_ENTRY_NULL;
	pv_hashed_entry_t       pvh_eh = PV_HASHED_ENTRY_NULL;
	pv_hashed_entry_t       pvh_e;
	<span class="enscript-type">int</span>			pvh_cnt = 0;
	<span class="enscript-type">int</span>			num_removed, num_unwired, num_found, num_invalid;
	<span class="enscript-type">int</span>			num_device, num_external, num_internal, num_reusable;
	uint64_t		num_compressed;
	ppnum_t			pai;
	pmap_paddr_t		pa;
	vm_map_offset_t		vaddr;
	boolean_t		is_ept = is_ept_pmap(pmap);

	num_removed = 0;
	num_unwired = 0;
	num_found   = 0;
	num_invalid = 0;
	num_device  = 0;
	num_external = 0;
	num_internal = 0;
	num_reusable = 0;
	num_compressed = 0;
	<span class="enscript-comment">/* invalidate the PTEs first to &quot;freeze&quot; them */</span>
	<span class="enscript-keyword">for</span> (cpte = spte, vaddr = start_vaddr;
	     cpte &lt; epte;
	     cpte++, vaddr += PAGE_SIZE_64) {
		pt_entry_t p = *cpte;

		pa = pte_to_pa(p);
		<span class="enscript-keyword">if</span> (pa == 0) {
			<span class="enscript-keyword">if</span> (pmap != kernel_pmap &amp;&amp;
			    (options &amp; PMAP_OPTIONS_REMOVE) &amp;&amp;
			    (p &amp; PTE_COMPRESSED)) {
				<span class="enscript-comment">/* one less &quot;compressed&quot; */</span>
				num_compressed++;
				<span class="enscript-comment">/* clear marker */</span>
				<span class="enscript-comment">/* XXX probably does not need to be atomic! */</span>
				pmap_update_pte(cpte, PTE_COMPRESSED, 0);
			}
			<span class="enscript-keyword">continue</span>;
		}
		num_found++;

		<span class="enscript-keyword">if</span> (iswired(p))
			num_unwired++;
		
		pai = pa_index(pa);

		<span class="enscript-keyword">if</span> (!IS_MANAGED_PAGE(pai)) {
			<span class="enscript-comment">/*
			 *	Outside range of managed physical memory.
			 *	Just remove the mappings.
			 */</span>
			pmap_store_pte(cpte, 0);
			num_device++;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-keyword">if</span> ((p &amp; PTE_VALID_MASK(is_ept)) == 0)
			num_invalid++;

		<span class="enscript-comment">/* invalidate the PTE */</span>
		pmap_update_pte(cpte, PTE_VALID_MASK(is_ept), 0);
	}

	<span class="enscript-keyword">if</span> (num_found == 0) {
		<span class="enscript-comment">/* nothing was changed: we're done */</span>
	        <span class="enscript-keyword">goto</span> <span class="enscript-reference">update_counts</span>;
	}

	<span class="enscript-comment">/* propagate the invalidates to other CPUs */</span>

	PMAP_UPDATE_TLBS(pmap, start_vaddr, vaddr);

	<span class="enscript-keyword">for</span> (cpte = spte, vaddr = start_vaddr;
	     cpte &lt; epte;
	     cpte++, vaddr += PAGE_SIZE_64) {

		pa = pte_to_pa(*cpte);
		<span class="enscript-keyword">if</span> (pa == 0)
			<span class="enscript-keyword">continue</span>;

		pai = pa_index(pa);

		LOCK_PVH(pai);

		pa = pte_to_pa(*cpte);
		<span class="enscript-keyword">if</span> (pa == 0) {
			UNLOCK_PVH(pai);
			<span class="enscript-keyword">continue</span>;
		}
		num_removed++;
		<span class="enscript-keyword">if</span> (IS_REUSABLE_PAGE(pai)) {
			num_reusable++;
		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (IS_INTERNAL_PAGE(pai)) {
			num_internal++;
		} <span class="enscript-keyword">else</span> {
			num_external++;
		}

		<span class="enscript-comment">/*
	       	 * Get the modify and reference bits, then
	       	 * nuke the entry in the page table
	       	 */</span>
		<span class="enscript-comment">/* remember reference and change */</span>
		pmap_phys_attributes[pai] |=
			(<span class="enscript-type">char</span>) (*cpte &amp; (PHYS_MODIFIED | PHYS_REFERENCED));

		<span class="enscript-comment">/*
	      	 * Remove the mapping from the pvlist for this physical page.
	         */</span>
		pvh_e = pmap_pv_remove(pmap, vaddr, (ppnum_t *) &amp;pai, cpte);

		<span class="enscript-comment">/* completely invalidate the PTE */</span>
		pmap_store_pte(cpte, 0);

		UNLOCK_PVH(pai);

		<span class="enscript-keyword">if</span> (pvh_e != PV_HASHED_ENTRY_NULL) {
			pvh_e-&gt;qlink.next = (queue_entry_t) pvh_eh;
			pvh_eh = pvh_e;

			<span class="enscript-keyword">if</span> (pvh_et == PV_HASHED_ENTRY_NULL) {
				pvh_et = pvh_e;
			}
			pvh_cnt++;
		}
	} <span class="enscript-comment">/* for loop */</span>

	<span class="enscript-keyword">if</span> (pvh_eh != PV_HASHED_ENTRY_NULL) {
		PV_HASHED_FREE_LIST(pvh_eh, pvh_et, pvh_cnt);
	}
<span class="enscript-reference">update_counts</span>:
	<span class="enscript-comment">/*
	 *	Update the counts
	 */</span>
#<span class="enscript-reference">if</span> <span class="enscript-variable-name">TESTING</span>
	<span class="enscript-keyword">if</span> (pmap-&gt;stats.resident_count &lt; num_removed)
	        panic(<span class="enscript-string">&quot;pmap_remove_range: resident_count&quot;</span>);
#<span class="enscript-reference">endif</span>
	pmap_ledger_debit(pmap, task_ledgers.phys_mem, machine_ptob(num_removed));
	pmap_ledger_debit(pmap, task_ledgers.phys_footprint, machine_ptob(num_removed));	
	assert(pmap-&gt;stats.resident_count &gt;= num_removed);
	OSAddAtomic(-num_removed,  &amp;pmap-&gt;stats.resident_count);

	<span class="enscript-keyword">if</span> (pmap != kernel_pmap) {
#<span class="enscript-reference">if</span> 00
		assert(pmap-&gt;stats.device &gt;= num_device);
		<span class="enscript-keyword">if</span> (num_device)
			OSAddAtomic(-num_device, &amp;pmap-&gt;stats.device);
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* 00 */</span>
		assert(pmap-&gt;stats.external &gt;= num_external);
		<span class="enscript-keyword">if</span> (num_external)
			OSAddAtomic(-num_external, &amp;pmap-&gt;stats.external);
		assert(pmap-&gt;stats.internal &gt;= num_internal);
		<span class="enscript-keyword">if</span> (num_internal)
			OSAddAtomic(-num_internal, &amp;pmap-&gt;stats.internal);
		assert(pmap-&gt;stats.reusable &gt;= num_reusable);
		<span class="enscript-keyword">if</span> (num_reusable)
			OSAddAtomic(-num_reusable, &amp;pmap-&gt;stats.reusable);
		assert(pmap-&gt;stats.compressed &gt;= num_compressed);
		<span class="enscript-keyword">if</span> (num_compressed)
			OSAddAtomic64(-num_compressed, &amp;pmap-&gt;stats.compressed);
	}

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">TESTING</span>
	<span class="enscript-keyword">if</span> (pmap-&gt;stats.wired_count &lt; num_unwired)
	        panic(<span class="enscript-string">&quot;pmap_remove_range: wired_count&quot;</span>);
#<span class="enscript-reference">endif</span>
	assert(pmap-&gt;stats.wired_count &gt;= num_unwired);
	OSAddAtomic(-num_unwired,  &amp;pmap-&gt;stats.wired_count);
	pmap_ledger_debit(pmap, task_ledgers.wired_mem, machine_ptob(num_unwired));

	<span class="enscript-keyword">return</span>;
}


<span class="enscript-comment">/*
 *	Remove the given range of addresses
 *	from the specified map.
 *
 *	It is assumed that the start and end are properly
 *	rounded to the hardware page size.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_remove</span>(
	pmap_t		map,
	addr64_t	s64,
	addr64_t	e64)
{
	pmap_remove_options(map, s64, e64, PMAP_OPTIONS_REMOVE);
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_remove_options</span>(
	pmap_t		map,
	addr64_t	s64,
	addr64_t	e64,
	<span class="enscript-type">int</span>		options)
{
	pt_entry_t     *pde;
	pt_entry_t     *spte, *epte;
	addr64_t        l64;
	uint64_t        deadline;
	boolean_t	is_ept;

	pmap_intr_assert();

	<span class="enscript-keyword">if</span> (map == PMAP_NULL || s64 == e64)
		<span class="enscript-keyword">return</span>;

	is_ept = is_ept_pmap(map);

	PMAP_TRACE(PMAP_CODE(PMAP__REMOVE) | DBG_FUNC_START,
		   map,
		   (uint32_t) (s64 &gt;&gt; 32), s64,
		   (uint32_t) (e64 &gt;&gt; 32), e64);


	PMAP_LOCK(map);

#<span class="enscript-reference">if</span> 0
	<span class="enscript-comment">/*
	 * Check that address range in the kernel does not overlap the stacks.
	 * We initialize local static min/max variables once to avoid making
	 * 2 function calls for every remove. Note also that these functions
	 * both return 0 before kernel stacks have been initialized, and hence
	 * the panic is not triggered in this case.
	 */</span>
	<span class="enscript-keyword">if</span> (map == kernel_pmap) {
		<span class="enscript-type">static</span> vm_offset_t kernel_stack_min = 0;
		<span class="enscript-type">static</span> vm_offset_t kernel_stack_max = 0;

		<span class="enscript-keyword">if</span> (kernel_stack_min == 0) {
			kernel_stack_min = min_valid_stack_address();
			kernel_stack_max = max_valid_stack_address();
		}
		<span class="enscript-keyword">if</span> ((kernel_stack_min &lt;= s64 &amp;&amp; s64 &lt; kernel_stack_max) ||
		    (kernel_stack_min &lt; e64 &amp;&amp; e64 &lt;= kernel_stack_max))
			panic(<span class="enscript-string">&quot;pmap_remove() attempted in kernel stack&quot;</span>);
	}
#<span class="enscript-reference">else</span>

	<span class="enscript-comment">/*
	 * The values of kernel_stack_min and kernel_stack_max are no longer
	 * relevant now that we allocate kernel stacks in the kernel map,
	 * so the old code above no longer applies.  If we wanted to check that
	 * we weren't removing a mapping of a page in a kernel stack we'd 
	 * mark the PTE with an unused bit and check that here.
	 */</span>

#<span class="enscript-reference">endif</span>

	deadline = rdtsc64() + max_preemption_latency_tsc;

	<span class="enscript-keyword">while</span> (s64 &lt; e64) {
		l64 = (s64 + pde_mapped_size) &amp; ~(pde_mapped_size - 1);
		<span class="enscript-keyword">if</span> (l64 &gt; e64)
			l64 = e64;
		pde = pmap_pde(map, s64);

		<span class="enscript-keyword">if</span> (pde &amp;&amp; (*pde &amp; PTE_VALID_MASK(is_ept))) {
			<span class="enscript-keyword">if</span> (*pde &amp; PTE_PS) {
				<span class="enscript-comment">/*
				 * If we're removing a superpage, pmap_remove_range()
				 * must work on level 2 instead of level 1; and we're
				 * only passing a single level 2 entry instead of a
				 * level 1 range.
				 */</span>
				spte = pde;
				epte = spte+1; <span class="enscript-comment">/* excluded */</span>
			} <span class="enscript-keyword">else</span> {
				spte = pmap_pte(map, (s64 &amp; ~(pde_mapped_size - 1)));
				spte = &amp;spte[ptenum(s64)];
				epte = &amp;spte[intel_btop(l64 - s64)];
			}
			pmap_remove_range_options(map, s64, spte, epte,
						  options);
		}
		s64 = l64;

		<span class="enscript-keyword">if</span> (s64 &lt; e64 &amp;&amp; rdtsc64() &gt;= deadline) {
			PMAP_UNLOCK(map)
			    <span class="enscript-comment">/* TODO: Rapid release/reacquisition can defeat
			     * the &quot;backoff&quot; intent here; either consider a
			     * fair spinlock, or a scheme whereby each lock
			     * attempt marks the processor as within a spinlock
			     * acquisition, and scan CPUs here to determine
			     * if a backoff is necessary, to avoid sacrificing
			     * performance in the common case.
			     */</span>
			PMAP_LOCK(map)
			deadline = rdtsc64() + max_preemption_latency_tsc;
		}
	}

	PMAP_UNLOCK(map);

	PMAP_TRACE(PMAP_CODE(PMAP__REMOVE) | DBG_FUNC_END,
		   map, 0, 0, 0, 0);

}

<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_page_protect</span>(
        ppnum_t         pn,
	vm_prot_t	prot)
{
	pmap_page_protect_options(pn, prot, 0, NULL);
}

<span class="enscript-comment">/*
 *	Routine:	pmap_page_protect_options
 *
 *	Function:
 *		Lower the permission for all mappings to a given
 *		page.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_page_protect_options</span>(
        ppnum_t         pn,
	vm_prot_t	prot,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	options,
	<span class="enscript-type">void</span>		*arg)
{
	pv_hashed_entry_t	pvh_eh = PV_HASHED_ENTRY_NULL;
	pv_hashed_entry_t	pvh_et = PV_HASHED_ENTRY_NULL;
	pv_hashed_entry_t	nexth;
	<span class="enscript-type">int</span>			pvh_cnt = 0;
	pv_rooted_entry_t	pv_h;
	pv_rooted_entry_t	pv_e;
	pv_hashed_entry_t	pvh_e;
	pt_entry_t		*pte;
	<span class="enscript-type">int</span>			pai;
	pmap_t			pmap;
	boolean_t		remove;
	pt_entry_t		new_pte_value;
	boolean_t		is_ept;

	pmap_intr_assert();
	assert(pn != vm_page_fictitious_addr);
	<span class="enscript-keyword">if</span> (pn == vm_page_guard_addr)
		<span class="enscript-keyword">return</span>;

	pai = ppn_to_pai(pn);

	<span class="enscript-keyword">if</span> (!IS_MANAGED_PAGE(pai)) {
		<span class="enscript-comment">/*
	         *	Not a managed page.
	         */</span>
		<span class="enscript-keyword">return</span>;
	}
	PMAP_TRACE(PMAP_CODE(PMAP__PAGE_PROTECT) | DBG_FUNC_START,
		   pn, prot, 0, 0, 0);

	<span class="enscript-comment">/*
	 * Determine the new protection.
	 */</span>
	<span class="enscript-keyword">switch</span> (prot) {
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_PROT_READ</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_PROT_READ</span> | VM_PROT_EXECUTE:
		remove = FALSE;
		<span class="enscript-keyword">break</span>;
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_PROT_ALL</span>:
		<span class="enscript-keyword">return</span>;		<span class="enscript-comment">/* nothing to do */</span>
	<span class="enscript-reference">default</span>:
		remove = TRUE;
		<span class="enscript-keyword">break</span>;
	}

	pv_h = pai_to_pvh(pai);

	LOCK_PVH(pai);


	<span class="enscript-comment">/*
	 * Walk down PV list, if any, changing or removing all mappings.
	 */</span>
	<span class="enscript-keyword">if</span> (pv_h-&gt;pmap == PMAP_NULL)
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;

	pv_e = pv_h;
	pvh_e = (pv_hashed_entry_t) pv_e;	<span class="enscript-comment">/* cheat */</span>

	<span class="enscript-keyword">do</span> {
		vm_map_offset_t vaddr;

		<span class="enscript-keyword">if</span> ((options &amp; PMAP_OPTIONS_COMPRESSOR_IFF_MODIFIED) &amp;&amp;
		    (pmap_phys_attributes[pai] &amp; PHYS_MODIFIED)) {
			<span class="enscript-comment">/* page was modified, so it will be compressed */</span>
			options &amp;= ~PMAP_OPTIONS_COMPRESSOR_IFF_MODIFIED;
			options |= PMAP_OPTIONS_COMPRESSOR;
		}

		pmap = pv_e-&gt;pmap;
		is_ept = is_ept_pmap(pmap);
		vaddr = pv_e-&gt;va;
		pte = pmap_pte(pmap, vaddr);

		pmap_assert2((pa_index(pte_to_pa(*pte)) == pn),
		    <span class="enscript-string">&quot;pmap_page_protect: PTE mismatch, pn: 0x%x, pmap: %p, vaddr: 0x%llx, pte: 0x%llx&quot;</span>, pn, pmap, vaddr, *pte);

		<span class="enscript-keyword">if</span> (0 == pte) {
			panic(<span class="enscript-string">&quot;pmap_page_protect() &quot;</span>
				<span class="enscript-string">&quot;pmap=%p pn=0x%x vaddr=0x%llx\n&quot;</span>,
				pmap, pn, vaddr);
		}
		nexth = (pv_hashed_entry_t) queue_next(&amp;pvh_e-&gt;qlink);

		<span class="enscript-comment">/*
		 * Remove the mapping if new protection is NONE
		 */</span>
		<span class="enscript-keyword">if</span> (remove) {

			<span class="enscript-comment">/* Remove per-pmap wired count */</span>
			<span class="enscript-keyword">if</span> (iswired(*pte)) {
				OSAddAtomic(-1, &amp;pmap-&gt;stats.wired_count);
				pmap_ledger_debit(pmap, task_ledgers.wired_mem, PAGE_SIZE);
			}

			<span class="enscript-keyword">if</span> (pmap != kernel_pmap &amp;&amp;
			    (options &amp; PMAP_OPTIONS_COMPRESSOR) &amp;&amp;
			    IS_INTERNAL_PAGE(pai)) {
				<span class="enscript-comment">/* mark this PTE as having been &quot;reclaimed&quot; */</span>
				new_pte_value = PTE_COMPRESSED;
			} <span class="enscript-keyword">else</span> {
				new_pte_value = 0;
			}

			<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_NOREFMOD) {
				pmap_store_pte(pte, new_pte_value);

				<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_NOFLUSH)
					PMAP_UPDATE_TLBS_DELAYED(pmap, vaddr, vaddr + PAGE_SIZE, (pmap_flush_context *)arg);
				<span class="enscript-keyword">else</span>
					PMAP_UPDATE_TLBS(pmap, vaddr, vaddr + PAGE_SIZE);
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/*
				 * Remove the mapping, collecting dirty bits.
				 */</span>
				pmap_update_pte(pte, PTE_VALID_MASK(is_ept), 0);

				PMAP_UPDATE_TLBS(pmap, vaddr, vaddr+PAGE_SIZE);
				<span class="enscript-keyword">if</span> ((options &amp;
				     PMAP_OPTIONS_COMPRESSOR_IFF_MODIFIED) &amp;&amp;
				    ! (pmap_phys_attributes[pai] &amp;
				       PHYS_MODIFIED) &amp;&amp;
				    (*pte &amp; PHYS_MODIFIED)) {
					<span class="enscript-comment">/*
					 * Page is actually &quot;modified&quot; and
					 * will be compressed.  Start
					 * accounting for it as &quot;compressed&quot;.
					 */</span>
					options &amp;= ~PMAP_OPTIONS_COMPRESSOR_IFF_MODIFIED;
					options |= PMAP_OPTIONS_COMPRESSOR;
					new_pte_value = PTE_COMPRESSED;
				}
				<span class="enscript-keyword">if</span> (!is_ept) {
					pmap_phys_attributes[pai] |=
						*pte &amp; (PHYS_MODIFIED|PHYS_REFERENCED);
				} <span class="enscript-keyword">else</span> {
					pmap_phys_attributes[pai] |=
						ept_refmod_to_physmap((*pte &amp; (INTEL_EPT_REF | INTEL_EPT_MOD))) &amp; (PHYS_MODIFIED | PHYS_REFERENCED);
				}
				pmap_store_pte(pte, new_pte_value);
			}

			<span class="enscript-keyword">if</span> (new_pte_value == PTE_COMPRESSED) {
				<span class="enscript-comment">/* one more &quot;compressed&quot; page */</span>
				OSAddAtomic64(+1, &amp;pmap-&gt;stats.compressed);
				PMAP_STATS_PEAK(pmap-&gt;stats.compressed);
				pmap-&gt;stats.compressed_lifetime++;
			}

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">TESTING</span>
			<span class="enscript-keyword">if</span> (pmap-&gt;stats.resident_count &lt; 1)
				panic(<span class="enscript-string">&quot;pmap_page_protect: resident_count&quot;</span>);
#<span class="enscript-reference">endif</span>
			pmap_ledger_debit(pmap, task_ledgers.phys_mem, PAGE_SIZE);
			assert(pmap-&gt;stats.resident_count &gt;= 1);
			OSAddAtomic(-1,  &amp;pmap-&gt;stats.resident_count);
			<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_COMPRESSOR) {
				<span class="enscript-comment">/*
				 * This removal is only being done so we can send this page to
				 * the compressor; therefore it mustn't affect total task footprint.
				 */</span>
				pmap_ledger_credit(pmap, task_ledgers.internal_compressed, PAGE_SIZE);
			} <span class="enscript-keyword">else</span> {
				pmap_ledger_debit(pmap, task_ledgers.phys_footprint, PAGE_SIZE);
			}

			<span class="enscript-keyword">if</span> (pmap != kernel_pmap) {
				<span class="enscript-keyword">if</span> (IS_REUSABLE_PAGE(pai)) {
					assert(pmap-&gt;stats.reusable &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.reusable);
				} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (IS_INTERNAL_PAGE(pai)) {
					assert(pmap-&gt;stats.internal &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.internal);
				} <span class="enscript-keyword">else</span> {
					assert(pmap-&gt;stats.external &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.external);
				}
			}

			<span class="enscript-comment">/*
		         * Deal with the pv_rooted_entry.
		         */</span>

			<span class="enscript-keyword">if</span> (pv_e == pv_h) {
				<span class="enscript-comment">/*
				 * Fix up head later.
				 */</span>
				pv_h-&gt;pmap = PMAP_NULL;
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/*
				 * Delete this entry.
				 */</span>
				pv_hash_remove(pvh_e);
				pvh_e-&gt;qlink.next = (queue_entry_t) pvh_eh;
				pvh_eh = pvh_e;

				<span class="enscript-keyword">if</span> (pvh_et == PV_HASHED_ENTRY_NULL)
					pvh_et = pvh_e;
				pvh_cnt++;
			}
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/*
		         * Write-protect, after opportunistic refmod collect
		         */</span>
			<span class="enscript-keyword">if</span> (!is_ept) {
				pmap_phys_attributes[pai] |=
					*pte &amp; (PHYS_MODIFIED|PHYS_REFERENCED);
			} <span class="enscript-keyword">else</span> {
				pmap_phys_attributes[pai] |=
					ept_refmod_to_physmap((*pte &amp; (INTEL_EPT_REF | INTEL_EPT_MOD))) &amp; (PHYS_MODIFIED | PHYS_REFERENCED);
			}
			pmap_update_pte(pte, PTE_WRITE(is_ept), 0);

			<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_NOFLUSH)
				PMAP_UPDATE_TLBS_DELAYED(pmap, vaddr, vaddr + PAGE_SIZE, (pmap_flush_context *)arg);
			<span class="enscript-keyword">else</span>
				PMAP_UPDATE_TLBS(pmap, vaddr, vaddr+PAGE_SIZE);
		}
		pvh_e = nexth;
	} <span class="enscript-keyword">while</span> ((pv_e = (pv_rooted_entry_t) nexth) != pv_h);


	<span class="enscript-comment">/*
	 * If pv_head mapping was removed, fix it up.
	 */</span>
	<span class="enscript-keyword">if</span> (pv_h-&gt;pmap == PMAP_NULL) {
		pvh_e = (pv_hashed_entry_t) queue_next(&amp;pv_h-&gt;qlink);

		<span class="enscript-keyword">if</span> (pvh_e != (pv_hashed_entry_t) pv_h) {
			pv_hash_remove(pvh_e);
			pv_h-&gt;pmap = pvh_e-&gt;pmap;
			pv_h-&gt;va = pvh_e-&gt;va;
			pvh_e-&gt;qlink.next = (queue_entry_t) pvh_eh;
			pvh_eh = pvh_e;

			<span class="enscript-keyword">if</span> (pvh_et == PV_HASHED_ENTRY_NULL)
				pvh_et = pvh_e;
			pvh_cnt++;
		}
	}
	<span class="enscript-keyword">if</span> (pvh_eh != PV_HASHED_ENTRY_NULL) {
		PV_HASHED_FREE_LIST(pvh_eh, pvh_et, pvh_cnt);
	}
<span class="enscript-reference">done</span>:
	UNLOCK_PVH(pai);

	PMAP_TRACE(PMAP_CODE(PMAP__PAGE_PROTECT) | DBG_FUNC_END,
		   0, 0, 0, 0, 0);
}


<span class="enscript-comment">/*
 *	Clear specified attribute bits.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">phys_attribute_clear</span>(
	ppnum_t		pn,
	<span class="enscript-type">int</span>		bits,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	options,
	<span class="enscript-type">void</span>		*arg)
{
	pv_rooted_entry_t	pv_h;
	pv_hashed_entry_t	pv_e;
	pt_entry_t		*pte;
	<span class="enscript-type">int</span>			pai;
	pmap_t			pmap;
	<span class="enscript-type">char</span>			attributes = 0;
	boolean_t		is_internal, is_reusable, is_ept;
	<span class="enscript-type">int</span>			ept_bits_to_clear;
	boolean_t		ept_keep_global_mod = FALSE;

	<span class="enscript-keyword">if</span> ((bits &amp; PHYS_MODIFIED) &amp;&amp;
	    (options &amp; PMAP_OPTIONS_NOFLUSH) &amp;&amp;
	    arg == NULL) {
		panic(<span class="enscript-string">&quot;phys_attribute_clear(0x%x,0x%x,0x%x,%p): &quot;</span>
		      <span class="enscript-string">&quot;should not clear 'modified' without flushing TLBs\n&quot;</span>,
		      pn, bits, options, arg);
	}

	<span class="enscript-comment">/* We only support converting MOD and REF bits for EPT PTEs in this function */</span>
	assert((bits &amp; ~(PHYS_REFERENCED | PHYS_MODIFIED)) == 0);

	ept_bits_to_clear = (<span class="enscript-type">unsigned</span>)physmap_refmod_to_ept(bits &amp; (PHYS_MODIFIED | PHYS_REFERENCED));

	pmap_intr_assert();
	assert(pn != vm_page_fictitious_addr);
	<span class="enscript-keyword">if</span> (pn == vm_page_guard_addr)
		<span class="enscript-keyword">return</span>;

	pai = ppn_to_pai(pn);

	<span class="enscript-keyword">if</span> (!IS_MANAGED_PAGE(pai)) {
		<span class="enscript-comment">/*
		 *	Not a managed page.
		 */</span>
		<span class="enscript-keyword">return</span>;
	}

	PMAP_TRACE(PMAP_CODE(PMAP__ATTRIBUTE_CLEAR) | DBG_FUNC_START,
		   pn, bits, 0, 0, 0);

	pv_h = pai_to_pvh(pai);

	LOCK_PVH(pai);


	<span class="enscript-comment">/*
	 * Walk down PV list, clearing all modify or reference bits.
	 * We do not have to lock the pv_list because we have
	 * the per-pmap lock
	 */</span>
	<span class="enscript-keyword">if</span> (pv_h-&gt;pmap != PMAP_NULL) {
		<span class="enscript-comment">/*
		 * There are some mappings.
		 */</span>

		is_internal = IS_INTERNAL_PAGE(pai);
		is_reusable = IS_REUSABLE_PAGE(pai);

		pv_e = (pv_hashed_entry_t)pv_h;

		<span class="enscript-keyword">do</span> {
			vm_map_offset_t	va;
			<span class="enscript-type">char</span> pte_bits;

			pmap = pv_e-&gt;pmap;
			is_ept = is_ept_pmap(pmap);
			va = pv_e-&gt;va;
			pte_bits = 0;

			<span class="enscript-keyword">if</span> (bits) {
				pte = pmap_pte(pmap, va);
				<span class="enscript-comment">/* grab ref/mod bits from this PTE */</span>
				pte_bits = (*pte &amp; (PTE_REF(is_ept) | PTE_MOD(is_ept)));
				<span class="enscript-comment">/* propagate to page's global attributes */</span>
				<span class="enscript-keyword">if</span> (!is_ept) {
					attributes |= pte_bits;
				} <span class="enscript-keyword">else</span> {
					attributes |= ept_refmod_to_physmap(pte_bits);
					<span class="enscript-keyword">if</span> (!pmap_ept_support_ad &amp;&amp; (pte_bits &amp; INTEL_EPT_MOD)) {
						ept_keep_global_mod = TRUE;
					}
				}
				<span class="enscript-comment">/* which bits to clear for this PTE? */</span>
				<span class="enscript-keyword">if</span> (!is_ept) {
					pte_bits &amp;= bits;
				} <span class="enscript-keyword">else</span> {
					pte_bits &amp;= ept_bits_to_clear;
				}
			}

			 <span class="enscript-comment">/*
			  * Clear modify and/or reference bits.
			  */</span>
			<span class="enscript-keyword">if</span> (pte_bits) {
				pmap_update_pte(pte, bits, 0);

				<span class="enscript-comment">/* Ensure all processors using this translation
				 * invalidate this TLB entry. The invalidation
				 * *must* follow the PTE update, to ensure that
				 * the TLB shadow of the 'D' bit (in particular)
				 * is synchronized with the updated PTE.
				 */</span>
				<span class="enscript-keyword">if</span> (! (options &amp; PMAP_OPTIONS_NOFLUSH)) {
					<span class="enscript-comment">/* flush TLBS now */</span>
					PMAP_UPDATE_TLBS(pmap,
							 va,
							 va + PAGE_SIZE);
				} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (arg) {
					<span class="enscript-comment">/* delayed TLB flush: add &quot;pmap&quot; info */</span>
					PMAP_UPDATE_TLBS_DELAYED(
						pmap,
						va,
						va + PAGE_SIZE,
						(pmap_flush_context *)arg);
				} <span class="enscript-keyword">else</span> {
					<span class="enscript-comment">/* no TLB flushing at all */</span>
				}
			}

			<span class="enscript-comment">/* update pmap &quot;reusable&quot; stats */</span>
			<span class="enscript-keyword">if</span> ((options &amp; PMAP_OPTIONS_CLEAR_REUSABLE) &amp;&amp;
			    is_reusable &amp;&amp;
			    pmap != kernel_pmap) {
				<span class="enscript-comment">/* one less &quot;reusable&quot; */</span>
				assert(pmap-&gt;stats.reusable &gt; 0);
				OSAddAtomic(-1, &amp;pmap-&gt;stats.reusable);
				<span class="enscript-keyword">if</span> (is_internal) {
					<span class="enscript-comment">/* one more &quot;internal&quot; */</span>
					OSAddAtomic(+1, &amp;pmap-&gt;stats.internal);
					PMAP_STATS_PEAK(pmap-&gt;stats.internal);
				} <span class="enscript-keyword">else</span> {
					<span class="enscript-comment">/* one more &quot;external&quot; */</span>
					OSAddAtomic(+1, &amp;pmap-&gt;stats.external);
					PMAP_STATS_PEAK(pmap-&gt;stats.external);
				}
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> ((options &amp; PMAP_OPTIONS_SET_REUSABLE) &amp;&amp;
				   !is_reusable &amp;&amp;
				   pmap != kernel_pmap) {
				<span class="enscript-comment">/* one more &quot;reusable&quot; */</span>
				OSAddAtomic(+1, &amp;pmap-&gt;stats.reusable);
				PMAP_STATS_PEAK(pmap-&gt;stats.reusable);
				<span class="enscript-keyword">if</span> (is_internal) {
					<span class="enscript-comment">/* one less &quot;internal&quot; */</span>
					assert(pmap-&gt;stats.internal &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.internal);
				} <span class="enscript-keyword">else</span> {
					<span class="enscript-comment">/* one less &quot;external&quot; */</span>
					assert(pmap-&gt;stats.external &gt; 0);
					OSAddAtomic(-1, &amp;pmap-&gt;stats.external);
				}
			}

			pv_e = (pv_hashed_entry_t)queue_next(&amp;pv_e-&gt;qlink);

		} <span class="enscript-keyword">while</span> (pv_e != (pv_hashed_entry_t)pv_h);
	}
	<span class="enscript-comment">/* Opportunistic refmod collection, annulled
	 * if both REF and MOD are being cleared.
	 */</span>

	pmap_phys_attributes[pai] |= attributes;

	<span class="enscript-keyword">if</span> (ept_keep_global_mod) {
		<span class="enscript-comment">/*
		 * If the hardware doesn't support AD bits for EPT PTEs and someone is
		 * requesting that we clear the modified bit for a phys page, we need
		 * to ensure that there are no EPT mappings for the page with the
		 * modified bit set. If there are, we cannot clear the global modified bit.
		 */</span>
		bits &amp;= ~PHYS_MODIFIED;
	}
	pmap_phys_attributes[pai] &amp;= ~(bits);

	<span class="enscript-comment">/* update this page's &quot;reusable&quot; status */</span>
	<span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_CLEAR_REUSABLE) {
		pmap_phys_attributes[pai] &amp;= ~PHYS_REUSABLE;
	} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (options &amp; PMAP_OPTIONS_SET_REUSABLE) {
		pmap_phys_attributes[pai] |= PHYS_REUSABLE;
	}

	UNLOCK_PVH(pai);

	PMAP_TRACE(PMAP_CODE(PMAP__ATTRIBUTE_CLEAR) | DBG_FUNC_END,
		   0, 0, 0, 0, 0);
}

<span class="enscript-comment">/*
 *	Check specified attribute bits.
 */</span>
<span class="enscript-type">int</span>
<span class="enscript-function-name">phys_attribute_test</span>(
	ppnum_t		pn,
	<span class="enscript-type">int</span>		bits)
{
	pv_rooted_entry_t	pv_h;
	pv_hashed_entry_t	pv_e;
	pt_entry_t		*pte;
	<span class="enscript-type">int</span>			pai;
	pmap_t			pmap;
	<span class="enscript-type">int</span>			attributes = 0;
	boolean_t		is_ept;

	pmap_intr_assert();
	assert(pn != vm_page_fictitious_addr);
	assert((bits &amp; ~(PHYS_MODIFIED | PHYS_REFERENCED)) == 0);
	<span class="enscript-keyword">if</span> (pn == vm_page_guard_addr)
		<span class="enscript-keyword">return</span> 0;

	pai = ppn_to_pai(pn);

	<span class="enscript-keyword">if</span> (!IS_MANAGED_PAGE(pai)) {
		<span class="enscript-comment">/*
		 *	Not a managed page.
		 */</span>
		<span class="enscript-keyword">return</span> 0;
	}

	<span class="enscript-comment">/*
	 * Fast check...  if bits already collected
	 * no need to take any locks...
	 * if not set, we need to recheck after taking
	 * the lock in case they got pulled in while
	 * we were waiting for the lock
	 */</span>
	<span class="enscript-keyword">if</span> ((pmap_phys_attributes[pai] &amp; bits) == bits)
		<span class="enscript-keyword">return</span> bits;

	pv_h = pai_to_pvh(pai);

	LOCK_PVH(pai);

	attributes = pmap_phys_attributes[pai] &amp; bits;


	<span class="enscript-comment">/*
	 * Walk down PV list, checking the mappings until we
	 * reach the end or we've found the desired attributes.
	 */</span>
	<span class="enscript-keyword">if</span> (attributes != bits &amp;&amp;
	    pv_h-&gt;pmap != PMAP_NULL) {
		<span class="enscript-comment">/*
		 * There are some mappings.
		 */</span>
		pv_e = (pv_hashed_entry_t)pv_h;
		<span class="enscript-keyword">do</span> {
			vm_map_offset_t va;

			pmap = pv_e-&gt;pmap;
			is_ept = is_ept_pmap(pmap);
			va = pv_e-&gt;va;
			<span class="enscript-comment">/*
	 		 * pick up modify and/or reference bits from mapping
			 */</span>

			pte = pmap_pte(pmap, va);
			<span class="enscript-keyword">if</span> (!is_ept) {
				attributes |= (<span class="enscript-type">int</span>)(*pte &amp; bits);
			} <span class="enscript-keyword">else</span> {
				attributes |= (<span class="enscript-type">int</span>)(ept_refmod_to_physmap((*pte &amp; (INTEL_EPT_REF | INTEL_EPT_MOD))) &amp; (PHYS_MODIFIED | PHYS_REFERENCED));

			}

			pv_e = (pv_hashed_entry_t)queue_next(&amp;pv_e-&gt;qlink);

		} <span class="enscript-keyword">while</span> ((attributes != bits) &amp;&amp;
			 (pv_e != (pv_hashed_entry_t)pv_h));
	}
	pmap_phys_attributes[pai] |= attributes;

	UNLOCK_PVH(pai);
	<span class="enscript-keyword">return</span> (attributes);
}

<span class="enscript-comment">/*
 *	Routine:	pmap_change_wiring
 *	Function:	Change the wiring attribute for a map/virtual-address
 *			pair.
 *	In/out conditions:
 *			The mapping must already exist in the pmap.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_change_wiring</span>(
	pmap_t		map,
	vm_map_offset_t	vaddr,
	boolean_t	wired)
{
	pt_entry_t	*pte;

	PMAP_LOCK(map);

	<span class="enscript-keyword">if</span> ((pte = pmap_pte(map, vaddr)) == PT_ENTRY_NULL)
		panic(<span class="enscript-string">&quot;pmap_change_wiring: pte missing&quot;</span>);

	<span class="enscript-keyword">if</span> (wired &amp;&amp; !iswired(*pte)) {
		<span class="enscript-comment">/*
		 * wiring down mapping
		 */</span>
		pmap_ledger_credit(map, task_ledgers.wired_mem, PAGE_SIZE);
		OSAddAtomic(+1,  &amp;map-&gt;stats.wired_count);
		pmap_update_pte(pte, 0, PTE_WIRED);
	}
	<span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (!wired &amp;&amp; iswired(*pte)) {
		<span class="enscript-comment">/*
		 * unwiring mapping
		 */</span>
		assert(map-&gt;stats.wired_count &gt;= 1);
		OSAddAtomic(-1,  &amp;map-&gt;stats.wired_count);
		pmap_ledger_debit(map, task_ledgers.wired_mem, PAGE_SIZE);
		pmap_update_pte(pte, PTE_WIRED, 0);
	}

	PMAP_UNLOCK(map);
}

<span class="enscript-comment">/*
 *	&quot;Backdoor&quot; direct map routine for early mappings.
 * 	Useful for mapping memory outside the range
 *      Sets A, D and NC if requested
 */</span>

vm_offset_t
<span class="enscript-function-name">pmap_map_bd</span>(
	vm_offset_t	virt,
	vm_map_offset_t	start_addr,
	vm_map_offset_t	end_addr,
	vm_prot_t	prot,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	flags)
{
	pt_entry_t	template;
	pt_entry_t	*pte;
	spl_t           spl;
	vm_offset_t	base = virt;
	template = pa_to_pte(start_addr)
		| INTEL_PTE_REF
		| INTEL_PTE_MOD
		| INTEL_PTE_WIRED
		| INTEL_PTE_VALID;

	<span class="enscript-keyword">if</span> ((flags &amp; (VM_MEM_NOT_CACHEABLE | VM_WIMG_USE_DEFAULT)) == VM_MEM_NOT_CACHEABLE) {
		template |= INTEL_PTE_NCACHE;
		<span class="enscript-keyword">if</span> (!(flags &amp; (VM_MEM_GUARDED)))
			template |= INTEL_PTE_PTA;
	}

#<span class="enscript-reference">if</span>    <span class="enscript-reference">defined</span>(<span class="enscript-variable-name">__x86_64__</span>)
	<span class="enscript-keyword">if</span> ((prot &amp; VM_PROT_EXECUTE) == 0)
		template |= INTEL_PTE_NX;
#<span class="enscript-reference">endif</span>

	<span class="enscript-keyword">if</span> (prot &amp; VM_PROT_WRITE)
		template |= INTEL_PTE_WRITE;

	<span class="enscript-keyword">while</span> (start_addr &lt; end_addr) {
	        spl = splhigh();
		pte = pmap_pte(kernel_pmap, (vm_map_offset_t)virt);
		<span class="enscript-keyword">if</span> (pte == PT_ENTRY_NULL) {
			panic(<span class="enscript-string">&quot;pmap_map_bd: Invalid kernel address\n&quot;</span>);
		}
		pmap_store_pte(pte, template);
		splx(spl);
		pte_increment_pa(template);
		virt += PAGE_SIZE;
		start_addr += PAGE_SIZE;
	}
	flush_tlb_raw();
	PMAP_UPDATE_TLBS(kernel_pmap, base, base + end_addr - start_addr);
	<span class="enscript-keyword">return</span>(virt);
}

<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>
<span class="enscript-function-name">pmap_query_resident</span>(
	pmap_t		pmap,
	addr64_t	s64,
	addr64_t	e64,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	*compressed_count_p)
{
	pt_entry_t     *pde;
	pt_entry_t     *spte, *epte;
	addr64_t        l64;
	uint64_t        deadline;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	result;
	boolean_t	is_ept;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	compressed_count;

	pmap_intr_assert();

	<span class="enscript-keyword">if</span> (pmap == PMAP_NULL || pmap == kernel_pmap || s64 == e64) {
		<span class="enscript-keyword">if</span> (compressed_count_p) {
			*compressed_count_p = 0;
		}
		<span class="enscript-keyword">return</span> 0;
	}

	is_ept = is_ept_pmap(pmap);

	PMAP_TRACE(PMAP_CODE(PMAP__QUERY_RESIDENT) | DBG_FUNC_START,
		   pmap,
		   (uint32_t) (s64 &gt;&gt; 32), s64,
		   (uint32_t) (e64 &gt;&gt; 32), e64);

	result = 0;
	compressed_count = 0;

	PMAP_LOCK(pmap);

	deadline = rdtsc64() + max_preemption_latency_tsc;

	<span class="enscript-keyword">while</span> (s64 &lt; e64) {
		l64 = (s64 + pde_mapped_size) &amp; ~(pde_mapped_size - 1);
		<span class="enscript-keyword">if</span> (l64 &gt; e64)
			l64 = e64;
		pde = pmap_pde(pmap, s64);

		<span class="enscript-keyword">if</span> (pde &amp;&amp; (*pde &amp; PTE_VALID_MASK(is_ept))) {
			<span class="enscript-keyword">if</span> (*pde &amp; PTE_PS) {
				<span class="enscript-comment">/* superpage: not supported */</span>
			} <span class="enscript-keyword">else</span> {
				spte = pmap_pte(pmap,
						(s64 &amp; ~(pde_mapped_size - 1)));
				spte = &amp;spte[ptenum(s64)];
				epte = &amp;spte[intel_btop(l64 - s64)];

				<span class="enscript-keyword">for</span> (; spte &lt; epte; spte++) {
					<span class="enscript-keyword">if</span> (pte_to_pa(*spte) != 0) {
						result++;
					} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (*spte &amp; PTE_COMPRESSED) {
						compressed_count++;
					}
				}

			}
		}
		s64 = l64;

		<span class="enscript-keyword">if</span> (s64 &lt; e64 &amp;&amp; rdtsc64() &gt;= deadline) {
			PMAP_UNLOCK(pmap);
			PMAP_LOCK(pmap);
			deadline = rdtsc64() + max_preemption_latency_tsc;
		}
	}

	PMAP_UNLOCK(pmap);

	PMAP_TRACE(PMAP_CODE(PMAP__QUERY_RESIDENT) | DBG_FUNC_END,
		   pmap, 0, 0, 0, 0);

	<span class="enscript-keyword">if</span> (compressed_count_p) {
		*compressed_count_p = compressed_count;
	}
	<span class="enscript-keyword">return</span> result;
}

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">MACH_ASSERT</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">pmap_set_process</span>(
	__unused pmap_t pmap,
	__unused <span class="enscript-type">int</span> pid,
	__unused <span class="enscript-type">char</span> *procname)
{
}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* MACH_ASSERT */</span>
</pre>
<hr />
</body></html>