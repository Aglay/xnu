<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>vm_map.c</title>
<style type="text/css">
.enscript-comment { font-style: italic; color: rgb(178,34,34); }
.enscript-function-name { font-weight: bold; color: rgb(0,0,255); }
.enscript-variable-name { font-weight: bold; color: rgb(184,134,11); }
.enscript-keyword { font-weight: bold; color: rgb(160,32,240); }
.enscript-reference { font-weight: bold; color: rgb(95,158,160); }
.enscript-string { font-weight: bold; color: rgb(188,143,143); }
.enscript-builtin { font-weight: bold; color: rgb(218,112,214); }
.enscript-type { font-weight: bold; color: rgb(34,139,34); }
.enscript-highlight { text-decoration: underline; color: 0; }
</style>
</head>
<body id="top">
<h1 style="margin:8px;" id="f1">vm_map.c&nbsp;&nbsp;&nbsp;<span style="font-weight: normal; font-size: 0.5em;">[<a href="?txt">plain text</a>]</span></h1>
<hr/>
<div></div>
<pre>
<span class="enscript-comment">/*
 * Copyright (c) 2000-2012 Apple Inc. All rights reserved.
 *
 * @APPLE_OSREFERENCE_LICENSE_HEADER_START@
 * 
 * This file contains Original Code and/or Modifications of Original Code
 * as defined in and that are subject to the Apple Public Source License
 * Version 2.0 (the 'License'). You may not use this file except in
 * compliance with the License. The rights granted to you under the License
 * may not be used to create, or enable the creation or redistribution of,
 * unlawful or unlicensed copies of an Apple operating system, or to
 * circumvent, violate, or enable the circumvention or violation of, any
 * terms of an Apple operating system software license agreement.
 * 
 * Please obtain a copy of the License at
 * <a href="http://www.opensource.apple.com/apsl/">http://www.opensource.apple.com/apsl/</a> and read it before using this file.
 * 
 * The Original Code and all software distributed under the License are
 * distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
 * EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
 * INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
 * Please see the License for the specific language governing rights and
 * limitations under the License.
 * 
 * @APPLE_OSREFERENCE_LICENSE_HEADER_END@
 */</span>
<span class="enscript-comment">/*
 * @OSF_COPYRIGHT@
 */</span>
<span class="enscript-comment">/* 
 * Mach Operating System
 * Copyright (c) 1991,1990,1989,1988,1987 Carnegie Mellon University
 * All Rights Reserved.
 * 
 * Permission to use, copy, modify and distribute this software and its
 * documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS &quot;AS IS&quot;
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND FOR
 * ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 * 
 *  Software Distribution Coordinator  or  <a href="mailto:Software.Distribution@CS.CMU.EDU">Software.Distribution@CS.CMU.EDU</a>
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 * 
 * any improvements or extensions that they make and grant Carnegie Mellon
 * the rights to redistribute these changes.
 */</span>
<span class="enscript-comment">/*
 */</span>
<span class="enscript-comment">/*
 *	File:	vm/vm_map.c
 *	Author:	Avadis Tevanian, Jr., Michael Wayne Young
 *	Date:	1985
 *
 *	Virtual memory mapping module.
 */</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;task_swapper.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach_assert.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_options.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;libkern/OSAtomic.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/kern_return.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/port.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/vm_attributes.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/vm_param.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/vm_behavior.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/vm_statistics.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/memory_object.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/mach_vm.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;machine/cpu_capabilities.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/sdt.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/assert.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/counters.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/kalloc.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/zalloc.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/cpm.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_compressor_pager.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_init.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_fault.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_map.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_object.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_page.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_pageout.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_kern.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;ipc/ipc_port.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/sched_prim.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/misc_protos.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;kern/xpr.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/vm_map_server.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;mach/mach_host_server.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_protos.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_purgeable_internal.h&gt;</span>

#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_protos.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_shared_region.h&gt;</span>
#<span class="enscript-reference">include</span> <span class="enscript-string">&lt;vm/vm_map_store.h&gt;</span>


<span class="enscript-type">extern</span> u_int32_t <span class="enscript-function-name">random</span>(<span class="enscript-type">void</span>);	<span class="enscript-comment">/* from &lt;libkern/libkern.h&gt; */</span>
<span class="enscript-comment">/* Internal prototypes
 */</span>

<span class="enscript-type">static</span> <span class="enscript-type">void</span> <span class="enscript-function-name">vm_map_simplify_range</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end);	<span class="enscript-comment">/* forward */</span>

<span class="enscript-type">static</span> boolean_t	vm_map_range_check(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end,
	vm_map_entry_t	*entry);

<span class="enscript-type">static</span> vm_map_entry_t	_vm_map_entry_create(
	<span class="enscript-type">struct</span> vm_map_header	*map_header, boolean_t map_locked);

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		_vm_map_entry_dispose(
	<span class="enscript-type">struct</span> vm_map_header	*map_header,
	vm_map_entry_t		entry);

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		vm_map_pmap_enter(
	vm_map_t		map,
	vm_map_offset_t 	addr,
	vm_map_offset_t		end_addr,
	vm_object_t 		object,
	vm_object_offset_t	offset,
	vm_prot_t		protection);

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		_vm_map_clip_end(
	<span class="enscript-type">struct</span> vm_map_header	*map_header,
	vm_map_entry_t		entry,
	vm_map_offset_t		end);

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		_vm_map_clip_start(
	<span class="enscript-type">struct</span> vm_map_header	*map_header,
	vm_map_entry_t		entry,
	vm_map_offset_t		start);

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		vm_map_entry_delete(
	vm_map_t	map,
	vm_map_entry_t	entry);

<span class="enscript-type">static</span> kern_return_t	vm_map_delete(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end,
	<span class="enscript-type">int</span>		flags,
	vm_map_t	zap_map);

<span class="enscript-type">static</span> kern_return_t	vm_map_copy_overwrite_unaligned(
	vm_map_t	dst_map,
	vm_map_entry_t	entry,
	vm_map_copy_t	copy,
	vm_map_address_t start,
	boolean_t	discard_on_success);

<span class="enscript-type">static</span> kern_return_t	vm_map_copy_overwrite_aligned(
	vm_map_t	dst_map,
	vm_map_entry_t	tmp_entry,
	vm_map_copy_t	copy,
	vm_map_offset_t start,
	pmap_t		pmap);

<span class="enscript-type">static</span> kern_return_t	vm_map_copyin_kernel_buffer(
	vm_map_t	src_map,
	vm_map_address_t src_addr,
	vm_map_size_t	len,
	boolean_t	src_destroy,
	vm_map_copy_t	*copy_result);  <span class="enscript-comment">/* OUT */</span>

<span class="enscript-type">static</span> kern_return_t	vm_map_copyout_kernel_buffer(
	vm_map_t	map,
	vm_map_address_t *addr,	<span class="enscript-comment">/* IN/OUT */</span>
	vm_map_copy_t	copy,
	boolean_t	overwrite,
	boolean_t	consume_on_success);

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		vm_map_fork_share(
	vm_map_t	old_map,
	vm_map_entry_t	old_entry,
	vm_map_t	new_map);

<span class="enscript-type">static</span> boolean_t	vm_map_fork_copy(
	vm_map_t	old_map,
	vm_map_entry_t	*old_entry_p,
	vm_map_t	new_map);

<span class="enscript-type">void</span>		vm_map_region_top_walk(
	vm_map_entry_t		   entry,
	vm_region_top_info_t       top);

<span class="enscript-type">void</span>		vm_map_region_walk(
	vm_map_t		   map,
	vm_map_offset_t		   va,
	vm_map_entry_t		   entry,
	vm_object_offset_t	   offset,
	vm_object_size_t	   range,
	vm_region_extended_info_t  extended,
	boolean_t		   look_for_pages,
	mach_msg_type_number_t count);

<span class="enscript-type">static</span> kern_return_t	vm_map_wire_nested(
	vm_map_t		   map,
	vm_map_offset_t		   start,
	vm_map_offset_t		   end,
	vm_prot_t		   caller_prot,
	boolean_t		   user_wire,
	pmap_t			   map_pmap, 
	vm_map_offset_t		   pmap_addr,
	ppnum_t			   *physpage_p);

<span class="enscript-type">static</span> kern_return_t	vm_map_unwire_nested(
	vm_map_t		   map,
	vm_map_offset_t		   start,
	vm_map_offset_t		   end,
	boolean_t		   user_wire,
	pmap_t			   map_pmap,
	vm_map_offset_t		   pmap_addr);

<span class="enscript-type">static</span> kern_return_t	vm_map_overwrite_submap_recurse(
	vm_map_t		   dst_map,
	vm_map_offset_t		   dst_addr,
	vm_map_size_t		   dst_size);

<span class="enscript-type">static</span> kern_return_t	vm_map_copy_overwrite_nested(
	vm_map_t		   dst_map,
	vm_map_offset_t		   dst_addr,
	vm_map_copy_t		   copy,
	boolean_t		   interruptible,
	pmap_t			   pmap,
	boolean_t		   discard_on_success);

<span class="enscript-type">static</span> kern_return_t	vm_map_remap_extract(
	vm_map_t		map,
	vm_map_offset_t		addr,
	vm_map_size_t		size,
	boolean_t		copy,
	<span class="enscript-type">struct</span> vm_map_header 	*map_header,
	vm_prot_t		*cur_protection,
	vm_prot_t		*max_protection,
	vm_inherit_t		inheritance,
	boolean_t		pageable);

<span class="enscript-type">static</span> kern_return_t	vm_map_remap_range_allocate(
	vm_map_t		map,
	vm_map_address_t	*address,
	vm_map_size_t		size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	vm_map_entry_t		*map_entry);

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		vm_map_region_look_for_page(
	vm_map_t		   map,
	vm_map_offset_t            va,
	vm_object_t		   object,
	vm_object_offset_t	   offset,
	<span class="enscript-type">int</span>                        max_refcnt,
	<span class="enscript-type">int</span>                        depth,
	vm_region_extended_info_t  extended,
	mach_msg_type_number_t count);

<span class="enscript-type">static</span> <span class="enscript-type">int</span>		vm_map_region_count_obj_refs(
	vm_map_entry_t    	   entry,
	vm_object_t       	   object);


<span class="enscript-type">static</span> kern_return_t	vm_map_willneed(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end);

<span class="enscript-type">static</span> kern_return_t	vm_map_reuse_pages(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end);

<span class="enscript-type">static</span> kern_return_t	vm_map_reusable_pages(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end);

<span class="enscript-type">static</span> kern_return_t	vm_map_can_reuse(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end);

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">MACH_ASSERT</span>
<span class="enscript-type">static</span> kern_return_t	vm_map_pageout(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end);
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* MACH_ASSERT */</span>

<span class="enscript-comment">/*
 * Macros to copy a vm_map_entry. We must be careful to correctly
 * manage the wired page count. vm_map_entry_copy() creates a new
 * map entry to the same memory - the wired count in the new entry
 * must be set to zero. vm_map_entry_copy_full() creates a new
 * entry that is identical to the old entry.  This preserves the
 * wire count; it's used for map splitting and zone changing in
 * vm_map_copyout.
 */</span>

#<span class="enscript-reference">define</span> <span class="enscript-function-name">vm_map_entry_copy</span>(NEW,OLD)	\
MACRO_BEGIN				\
boolean_t _vmec_reserved = (NEW)-&gt;from_reserved_zone;	\
	*(NEW) = *(OLD);                \
	(NEW)-&gt;is_shared = FALSE;	\
	(NEW)-&gt;needs_wakeup = FALSE;    \
	(NEW)-&gt;in_transition = FALSE;   \
	(NEW)-&gt;wired_count = 0;         \
	(NEW)-&gt;user_wired_count = 0;    \
	(NEW)-&gt;permanent = FALSE;	\
	(NEW)-&gt;used_for_jit = FALSE;	\
	(NEW)-&gt;from_reserved_zone = _vmec_reserved;	\
	(NEW)-&gt;iokit_acct = FALSE;	\
	(NEW)-&gt;vme_resilient_codesign = FALSE; \
	(NEW)-&gt;vme_resilient_media = FALSE;	\
MACRO_END

#<span class="enscript-reference">define</span> <span class="enscript-function-name">vm_map_entry_copy_full</span>(NEW,OLD)			\
MACRO_BEGIN						\
boolean_t _vmecf_reserved = (NEW)-&gt;from_reserved_zone;	\
(*(NEW) = *(OLD));					\
(NEW)-&gt;from_reserved_zone = _vmecf_reserved;			\
MACRO_END

<span class="enscript-comment">/*
 *	Decide if we want to allow processes to execute from their data or stack areas.
 *	override_nx() returns true if we do.  Data/stack execution can be enabled independently 
 *	for 32 and 64 bit processes.  Set the VM_ABI_32 or VM_ABI_64 flags in allow_data_exec
 *	or allow_stack_exec to enable data execution for that type of data area for that particular
 *	ABI (or both by or'ing the flags together).  These are initialized in the architecture
 *	specific pmap files since the default behavior varies according to architecture.  The 
 *	main reason it varies is because of the need to provide binary compatibility with old 
 *	applications that were written before these restrictions came into being.  In the old 
 *	days, an app could execute anything it could read, but this has slowly been tightened 
 *	up over time.  The default behavior is:
 *
 *	32-bit PPC apps		may execute from both stack and data areas
 *	32-bit Intel apps	may exeucte from data areas but not stack
 *	64-bit PPC/Intel apps	may not execute from either data or stack
 *
 *	An application on any architecture may override these defaults by explicitly
 *	adding PROT_EXEC permission to the page in question with the mprotect(2) 
 *	system call.  This code here just determines what happens when an app tries to
 * 	execute from a page that lacks execute permission.
 *
 *	Note that allow_data_exec or allow_stack_exec may also be modified by sysctl to change the
 *	default behavior for both 32 and 64 bit apps on a system-wide basis. Furthermore,
 *	a Mach-O header flag bit (MH_NO_HEAP_EXECUTION) can be used to forcibly disallow
 *	execution from data areas for a particular binary even if the arch normally permits it. As
 *	a final wrinkle, a posix_spawn attribute flag can be used to negate this opt-in header bit
 *	to support some complicated use cases, notably browsers with out-of-process plugins that
 *	are not all NX-safe.
 */</span>

<span class="enscript-type">extern</span> <span class="enscript-type">int</span> allow_data_exec, allow_stack_exec;

<span class="enscript-type">int</span>
<span class="enscript-function-name">override_nx</span>(vm_map_t map, uint32_t user_tag) <span class="enscript-comment">/* map unused on arm */</span>
{
	<span class="enscript-type">int</span> current_abi;

	<span class="enscript-keyword">if</span> (map-&gt;pmap == kernel_pmap) <span class="enscript-keyword">return</span> FALSE;

	<span class="enscript-comment">/*
	 * Determine if the app is running in 32 or 64 bit mode.
	 */</span>

	<span class="enscript-keyword">if</span> (vm_map_is_64bit(map))
		current_abi = VM_ABI_64;
	<span class="enscript-keyword">else</span>
		current_abi = VM_ABI_32;

	<span class="enscript-comment">/*
	 * Determine if we should allow the execution based on whether it's a 
	 * stack or data area and the current architecture.
	 */</span>

	<span class="enscript-keyword">if</span> (user_tag == VM_MEMORY_STACK)
		<span class="enscript-keyword">return</span> allow_stack_exec &amp; current_abi;

	<span class="enscript-keyword">return</span> (allow_data_exec &amp; current_abi) &amp;&amp; (map-&gt;map_disallow_data_exec == FALSE);
}


<span class="enscript-comment">/*
 *	Virtual memory maps provide for the mapping, protection,
 *	and sharing of virtual memory objects.  In addition,
 *	this module provides for an efficient virtual copy of
 *	memory from one map to another.
 *
 *	Synchronization is required prior to most operations.
 *
 *	Maps consist of an ordered doubly-linked list of simple
 *	entries; a single hint is used to speed up lookups.
 *
 *	Sharing maps have been deleted from this version of Mach.
 *	All shared objects are now mapped directly into the respective
 *	maps.  This requires a change in the copy on write strategy;
 *	the asymmetric (delayed) strategy is used for shared temporary
 *	objects instead of the symmetric (shadow) strategy.  All maps
 *	are now &quot;top level&quot; maps (either task map, kernel map or submap
 *	of the kernel map).  
 *
 *	Since portions of maps are specified by start/end addreses,
 *	which may not align with existing map entries, all
 *	routines merely &quot;clip&quot; entries to these start/end values.
 *	[That is, an entry is split into two, bordering at a
 *	start or end value.]  Note that these clippings may not
 *	always be necessary (as the two resulting entries are then
 *	not changed); however, the clipping is done for convenience.
 *	No attempt is currently made to &quot;glue back together&quot; two
 *	abutting entries.
 *
 *	The symmetric (shadow) copy strategy implements virtual copy
 *	by copying VM object references from one map to
 *	another, and then marking both regions as copy-on-write.
 *	It is important to note that only one writeable reference
 *	to a VM object region exists in any map when this strategy
 *	is used -- this means that shadow object creation can be
 *	delayed until a write operation occurs.  The symmetric (delayed)
 *	strategy allows multiple maps to have writeable references to
 *	the same region of a vm object, and hence cannot delay creating
 *	its copy objects.  See vm_object_copy_quickly() in vm_object.c.
 *	Copying of permanent objects is completely different; see
 *	vm_object_copy_strategically() in vm_object.c.
 */</span>

<span class="enscript-type">static</span> zone_t	vm_map_zone;		<span class="enscript-comment">/* zone for vm_map structures */</span>
<span class="enscript-type">static</span> zone_t	vm_map_entry_zone;	<span class="enscript-comment">/* zone for vm_map_entry structures */</span>
<span class="enscript-type">static</span> zone_t	vm_map_entry_reserved_zone;	<span class="enscript-comment">/* zone with reserve for non-blocking
					 * allocations */</span>
<span class="enscript-type">static</span> zone_t	vm_map_copy_zone;	<span class="enscript-comment">/* zone for vm_map_copy structures */</span>
zone_t		vm_map_holes_zone;	<span class="enscript-comment">/* zone for vm map holes (vm_map_links) structures */</span>


<span class="enscript-comment">/*
 *	Placeholder object for submap operations.  This object is dropped
 *	into the range by a call to vm_map_find, and removed when
 *	vm_map_submap creates the submap.
 */</span>

vm_object_t	vm_submap_object;

<span class="enscript-type">static</span> <span class="enscript-type">void</span>		*map_data;
<span class="enscript-type">static</span> vm_size_t	map_data_size;
<span class="enscript-type">static</span> <span class="enscript-type">void</span>		*kentry_data;
<span class="enscript-type">static</span> vm_size_t	kentry_data_size;
<span class="enscript-type">static</span> <span class="enscript-type">void</span>		*map_holes_data;
<span class="enscript-type">static</span> vm_size_t	map_holes_data_size;

#<span class="enscript-reference">define</span>         <span class="enscript-variable-name">NO_COALESCE_LIMIT</span>  ((1024 * 128) - 1)

<span class="enscript-comment">/* Skip acquiring locks if we're in the midst of a kernel core dump */</span>
<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> not_in_kdp = 1;

<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> vm_map_set_cache_attr_count = 0;

kern_return_t
<span class="enscript-function-name">vm_map_set_cache_attr</span>(
	vm_map_t	map,
	vm_map_offset_t	va)
{
	vm_map_entry_t	map_entry;
	vm_object_t	object;
	kern_return_t	kr = KERN_SUCCESS;

	vm_map_lock_read(map);

	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, va, &amp;map_entry) ||
	    map_entry-&gt;is_sub_map) {
		<span class="enscript-comment">/*
		 * that memory is not properly mapped
		 */</span>
		kr = KERN_INVALID_ARGUMENT;
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
	}
	object = VME_OBJECT(map_entry);

	<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
		<span class="enscript-comment">/*
		 * there should be a VM object here at this point
		 */</span>
		kr = KERN_INVALID_ARGUMENT;
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
	}
	vm_object_lock(object);
	object-&gt;set_cache_attr = TRUE;
	vm_object_unlock(object);

	vm_map_set_cache_attr_count++;
<span class="enscript-reference">done</span>:
	vm_map_unlock_read(map);

	<span class="enscript-keyword">return</span> kr;
}


#<span class="enscript-reference">if</span> <span class="enscript-variable-name">CONFIG_CODE_DECRYPTION</span>
<span class="enscript-comment">/*
 * vm_map_apple_protected:
 * This remaps the requested part of the object with an object backed by 
 * the decrypting pager.
 * crypt_info contains entry points and session data for the crypt module.
 * The crypt_info block will be copied by vm_map_apple_protected. The data structures
 * referenced in crypt_info must remain valid until crypt_info-&gt;crypt_end() is called.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_apple_protected</span>(
	vm_map_t		map,
	vm_map_offset_t		start,
	vm_map_offset_t		end,
	vm_object_offset_t	crypto_backing_offset,
	<span class="enscript-type">struct</span> pager_crypt_info *crypt_info)
{
	boolean_t	map_locked;
	kern_return_t	kr;
	vm_map_entry_t	map_entry;
	<span class="enscript-type">struct</span> vm_map_entry tmp_entry;
	memory_object_t	unprotected_mem_obj;
	vm_object_t	protected_object;
	vm_map_offset_t	map_addr;
	vm_map_offset_t	start_aligned, end_aligned;
	vm_object_offset_t	crypto_start, crypto_end;
	<span class="enscript-type">int</span>		vm_flags;

	map_locked = FALSE;
	unprotected_mem_obj = MEMORY_OBJECT_NULL;

	start_aligned = vm_map_trunc_page(start, PAGE_MASK_64);
	end_aligned = vm_map_round_page(end, PAGE_MASK_64);
	start_aligned = vm_map_trunc_page(start_aligned, VM_MAP_PAGE_MASK(map));
	end_aligned = vm_map_round_page(end_aligned, VM_MAP_PAGE_MASK(map));

	assert(start_aligned == start);
	assert(end_aligned == end);

	map_addr = start_aligned;
	<span class="enscript-keyword">for</span> (map_addr = start_aligned;
	     map_addr &lt; end;
	     map_addr = tmp_entry.vme_end) {
		vm_map_lock(map);
		map_locked = TRUE;

		<span class="enscript-comment">/* lookup the protected VM object */</span>
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map,
					 map_addr,
					 &amp;map_entry) ||
		    map_entry-&gt;is_sub_map ||
		    VME_OBJECT(map_entry) == VM_OBJECT_NULL ||
		    !(map_entry-&gt;protection &amp; VM_PROT_EXECUTE)) {
			<span class="enscript-comment">/* that memory is not properly mapped */</span>
			kr = KERN_INVALID_ARGUMENT;
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
		}

		<span class="enscript-comment">/* get the protected object to be decrypted */</span>
		protected_object = VME_OBJECT(map_entry);
		<span class="enscript-keyword">if</span> (protected_object == VM_OBJECT_NULL) {
			<span class="enscript-comment">/* there should be a VM object here at this point */</span>
			kr = KERN_INVALID_ARGUMENT;
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
		}
		<span class="enscript-comment">/* ensure protected object stays alive while map is unlocked */</span>
		vm_object_reference(protected_object);

		<span class="enscript-comment">/* limit the map entry to the area we want to cover */</span>
		vm_map_clip_start(map, map_entry, start_aligned);
		vm_map_clip_end(map, map_entry, end_aligned);

		tmp_entry = *map_entry;
		map_entry = VM_MAP_ENTRY_NULL; <span class="enscript-comment">/* not valid after unlocking map */</span>
		vm_map_unlock(map);
		map_locked = FALSE;

		<span class="enscript-comment">/*
		 * This map entry might be only partially encrypted
		 * (if not fully &quot;page-aligned&quot;).
		 */</span>
		crypto_start = 0;
		crypto_end = tmp_entry.vme_end - tmp_entry.vme_start;
		<span class="enscript-keyword">if</span> (tmp_entry.vme_start &lt; start) {
			<span class="enscript-keyword">if</span> (tmp_entry.vme_start != start_aligned) {
				kr = KERN_INVALID_ADDRESS;
			}
			crypto_start += (start - tmp_entry.vme_start);
		}
		<span class="enscript-keyword">if</span> (tmp_entry.vme_end &gt; end) {
			<span class="enscript-keyword">if</span> (tmp_entry.vme_end != end_aligned) {
				kr = KERN_INVALID_ADDRESS;
			}
			crypto_end -= (tmp_entry.vme_end - end);
		}

		<span class="enscript-comment">/*
		 * This &quot;extra backing offset&quot; is needed to get the decryption
		 * routine to use the right key.  It adjusts for the possibly
		 * relative offset of an interposed &quot;4K&quot; pager...
		 */</span>
		<span class="enscript-keyword">if</span> (crypto_backing_offset == (vm_object_offset_t) -1) {
			crypto_backing_offset = VME_OFFSET(&amp;tmp_entry);
		}

		<span class="enscript-comment">/*
		 * Lookup (and create if necessary) the protected memory object
		 * matching that VM object.
		 * If successful, this also grabs a reference on the memory object,
		 * to guarantee that it doesn't go away before we get a chance to map
		 * it.
		 */</span>
		unprotected_mem_obj = apple_protect_pager_setup(
			protected_object,
			VME_OFFSET(&amp;tmp_entry),
			crypto_backing_offset,
			crypt_info,
			crypto_start,
			crypto_end);

		<span class="enscript-comment">/* release extra ref on protected object */</span>
		vm_object_deallocate(protected_object);

		<span class="enscript-keyword">if</span> (unprotected_mem_obj == NULL) {
			kr = KERN_FAILURE;
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
		}

		vm_flags = VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE;

		<span class="enscript-comment">/* map this memory object in place of the current one */</span>
		map_addr = tmp_entry.vme_start;
		kr = vm_map_enter_mem_object(map,
					     &amp;map_addr,
					     (tmp_entry.vme_end -
					      tmp_entry.vme_start),
					     (mach_vm_offset_t) 0,
					     vm_flags,
					     (ipc_port_t) unprotected_mem_obj,
					     0,
					     TRUE,
					     tmp_entry.protection,
					     tmp_entry.max_protection,
					     tmp_entry.inheritance);
		assert(kr == KERN_SUCCESS);
		assert(map_addr == tmp_entry.vme_start);

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">VM_MAP_DEBUG_APPLE_PROTECT</span>
		printf(<span class="enscript-string">&quot;APPLE_PROTECT: map %p [0x%llx:0x%llx] pager %p: &quot;</span>
		       <span class="enscript-string">&quot;backing:[object:%p,offset:0x%llx,&quot;</span>
		       <span class="enscript-string">&quot;crypto_backing_offset:0x%llx,&quot;</span>
		       <span class="enscript-string">&quot;crypto_start:0x%llx,crypto_end:0x%llx]\n&quot;</span>,
		       map,
		       (uint64_t) map_addr,
		       (uint64_t) (map_addr + (tmp_entry.vme_end -
					       tmp_entry.vme_start)),
		       unprotected_mem_obj,
		       protected_object,
		       VME_OFFSET(&amp;tmp_entry),
		       crypto_backing_offset,
		       crypto_start,
		       crypto_end);
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* VM_MAP_DEBUG_APPLE_PROTECT */</span>
		       
		<span class="enscript-comment">/*
		 * Release the reference obtained by
		 * apple_protect_pager_setup().
		 * The mapping (if it succeeded) is now holding a reference on
		 * the memory object.
		 */</span>
		memory_object_deallocate(unprotected_mem_obj);
		unprotected_mem_obj = MEMORY_OBJECT_NULL;

		<span class="enscript-comment">/* continue with next map entry */</span>
		crypto_backing_offset += (tmp_entry.vme_end -
					  tmp_entry.vme_start);
		crypto_backing_offset -= crypto_start;
	}
	kr = KERN_SUCCESS;

<span class="enscript-reference">done</span>:
	<span class="enscript-keyword">if</span> (map_locked) {
		vm_map_unlock(map);
	}
	<span class="enscript-keyword">return</span> kr;
}
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* CONFIG_CODE_DECRYPTION */</span>


lck_grp_t		vm_map_lck_grp;
lck_grp_attr_t	vm_map_lck_grp_attr;
lck_attr_t		vm_map_lck_attr;
lck_attr_t		vm_map_lck_rw_attr;


<span class="enscript-comment">/*
 *	vm_map_init:
 *
 *	Initialize the vm_map module.  Must be called before
 *	any other vm_map routines.
 *
 *	Map and entry structures are allocated from zones -- we must
 *	initialize those zones.
 *
 *	There are three zones of interest:
 *
 *	vm_map_zone:		used to allocate maps.
 *	vm_map_entry_zone:	used to allocate map entries.
 *	vm_map_entry_reserved_zone:	fallback zone for kernel map entries
 *
 *	The kernel allocates map entries from a special zone that is initially
 *	&quot;crammed&quot; with memory.  It would be difficult (perhaps impossible) for
 *	the kernel to allocate more memory to a entry zone when it became
 *	empty since the very act of allocating memory implies the creation
 *	of a new entry.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_init</span>(
	<span class="enscript-type">void</span>)
{
	vm_size_t entry_zone_alloc_size;
	<span class="enscript-type">const</span> <span class="enscript-type">char</span> *mez_name = <span class="enscript-string">&quot;VM map entries&quot;</span>;

	vm_map_zone = zinit((vm_map_size_t) <span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> _vm_map), 40*1024,
			    PAGE_SIZE, <span class="enscript-string">&quot;maps&quot;</span>);
	zone_change(vm_map_zone, Z_NOENCRYPT, TRUE);
#<span class="enscript-reference">if</span>	<span class="enscript-reference">defined</span>(<span class="enscript-variable-name">__LP64__</span>)
	entry_zone_alloc_size = PAGE_SIZE * 5;
#<span class="enscript-reference">else</span>
	entry_zone_alloc_size = PAGE_SIZE * 6;
#<span class="enscript-reference">endif</span>
	vm_map_entry_zone = zinit((vm_map_size_t) <span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> vm_map_entry),
				  1024*1024, entry_zone_alloc_size,
				  mez_name);
	zone_change(vm_map_entry_zone, Z_NOENCRYPT, TRUE);
	zone_change(vm_map_entry_zone, Z_NOCALLOUT, TRUE);
	zone_change(vm_map_entry_zone, Z_GZALLOC_EXEMPT, TRUE);

	vm_map_entry_reserved_zone = zinit((vm_map_size_t) <span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> vm_map_entry),
				   kentry_data_size * 64, kentry_data_size,
				   <span class="enscript-string">&quot;Reserved VM map entries&quot;</span>);
	zone_change(vm_map_entry_reserved_zone, Z_NOENCRYPT, TRUE);

	vm_map_copy_zone = zinit((vm_map_size_t) <span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> vm_map_copy),
				 16*1024, PAGE_SIZE, <span class="enscript-string">&quot;VM map copies&quot;</span>);
	zone_change(vm_map_copy_zone, Z_NOENCRYPT, TRUE);

	vm_map_holes_zone = zinit((vm_map_size_t) <span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> vm_map_links),
				 16*1024, PAGE_SIZE, <span class="enscript-string">&quot;VM map holes&quot;</span>);
	zone_change(vm_map_holes_zone, Z_NOENCRYPT, TRUE);

	<span class="enscript-comment">/*
	 *	Cram the map and kentry zones with initial data.
	 *	Set reserved_zone non-collectible to aid zone_gc().
	 */</span>
	zone_change(vm_map_zone, Z_COLLECT, FALSE);

	zone_change(vm_map_entry_reserved_zone, Z_COLLECT, FALSE);
	zone_change(vm_map_entry_reserved_zone, Z_EXPAND, FALSE);
	zone_change(vm_map_entry_reserved_zone, Z_FOREIGN, TRUE);
	zone_change(vm_map_entry_reserved_zone, Z_NOCALLOUT, TRUE);
	zone_change(vm_map_entry_reserved_zone, Z_CALLERACCT, FALSE); <span class="enscript-comment">/* don't charge caller */</span>
	zone_change(vm_map_copy_zone, Z_CALLERACCT, FALSE); <span class="enscript-comment">/* don't charge caller */</span>
	zone_change(vm_map_entry_reserved_zone, Z_GZALLOC_EXEMPT, TRUE);

	zone_change(vm_map_holes_zone, Z_COLLECT, TRUE);
	zone_change(vm_map_holes_zone, Z_EXPAND, TRUE);
	zone_change(vm_map_holes_zone, Z_FOREIGN, TRUE);
	zone_change(vm_map_holes_zone, Z_NOCALLOUT, TRUE);
	zone_change(vm_map_holes_zone, Z_CALLERACCT, TRUE);
	zone_change(vm_map_holes_zone, Z_GZALLOC_EXEMPT, TRUE);

	<span class="enscript-comment">/* 
	 * Add the stolen memory to zones, adjust zone size and stolen counts.
	 */</span>
	zcram(vm_map_zone, (vm_offset_t)map_data, map_data_size);
	zcram(vm_map_entry_reserved_zone, (vm_offset_t)kentry_data, kentry_data_size);
	zcram(vm_map_holes_zone, (vm_offset_t)map_holes_data, map_holes_data_size);
	VM_PAGE_MOVE_STOLEN(atop_64(map_data_size) + atop_64(kentry_data_size) + atop_64(map_holes_data_size));

	lck_grp_attr_setdefault(&amp;vm_map_lck_grp_attr);
	lck_grp_init(&amp;vm_map_lck_grp, <span class="enscript-string">&quot;vm_map&quot;</span>, &amp;vm_map_lck_grp_attr);
	lck_attr_setdefault(&amp;vm_map_lck_attr);	

	lck_attr_setdefault(&amp;vm_map_lck_rw_attr);
	lck_attr_cleardebug(&amp;vm_map_lck_rw_attr);

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">CONFIG_FREEZE</span>
	default_freezer_init();
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* CONFIG_FREEZE */</span>
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_steal_memory</span>(
	<span class="enscript-type">void</span>)
{
	uint32_t kentry_initial_pages;

	map_data_size = round_page(10 * <span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> _vm_map));
	map_data = pmap_steal_memory(map_data_size);

	<span class="enscript-comment">/*
	 * kentry_initial_pages corresponds to the number of kernel map entries
	 * required during bootstrap until the asynchronous replenishment
	 * scheme is activated and/or entries are available from the general
	 * map entry pool.
	 */</span>
#<span class="enscript-reference">if</span>	<span class="enscript-reference">defined</span>(<span class="enscript-variable-name">__LP64__</span>)
	kentry_initial_pages = 10;
#<span class="enscript-reference">else</span>
	kentry_initial_pages = 6;
#<span class="enscript-reference">endif</span>

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">CONFIG_GZALLOC</span>
	<span class="enscript-comment">/* If using the guard allocator, reserve more memory for the kernel
	 * reserved map entry pool.
	*/</span>
	<span class="enscript-keyword">if</span> (gzalloc_enabled())
		kentry_initial_pages *= 1024;
#<span class="enscript-reference">endif</span>

	kentry_data_size = kentry_initial_pages * PAGE_SIZE;
	kentry_data = pmap_steal_memory(kentry_data_size);

	map_holes_data_size = kentry_data_size;
	map_holes_data = pmap_steal_memory(map_holes_data_size);
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_kernel_reserved_entry_init</span>(<span class="enscript-type">void</span>) {
	zone_prio_refill_configure(vm_map_entry_reserved_zone, (6*PAGE_SIZE)/<span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> vm_map_entry));
	zone_prio_refill_configure(vm_map_holes_zone, (6*PAGE_SIZE)/<span class="enscript-keyword">sizeof</span>(<span class="enscript-type">struct</span> vm_map_links));
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_disable_hole_optimization</span>(vm_map_t map)
{
	vm_map_entry_t	head_entry, hole_entry, next_hole_entry;

	<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {

		head_entry = hole_entry = (vm_map_entry_t) map-&gt;holes_list;

		<span class="enscript-keyword">while</span> (hole_entry != NULL) {

			next_hole_entry = hole_entry-&gt;vme_next;

			hole_entry-&gt;vme_next = NULL;
			hole_entry-&gt;vme_prev = NULL;
			zfree(vm_map_holes_zone, hole_entry);

			<span class="enscript-keyword">if</span> (next_hole_entry == head_entry) {
				hole_entry = NULL;
			} <span class="enscript-keyword">else</span> {
				hole_entry = next_hole_entry;
			}
		}

		map-&gt;holes_list = NULL;
		map-&gt;holelistenabled = FALSE;

		map-&gt;first_free = vm_map_first_entry(map);
		SAVE_HINT_HOLE_WRITE(map, NULL);
	}
}

boolean_t
<span class="enscript-function-name">vm_kernel_map_is_kernel</span>(vm_map_t map) {
	<span class="enscript-keyword">return</span> (map-&gt;pmap == kernel_pmap);
}

<span class="enscript-comment">/*
 *	vm_map_create:
 *
 *	Creates and returns a new empty VM map with
 *	the given physical map structure, and having
 *	the given lower and upper address bounds.
 */</span>

boolean_t vm_map_supports_hole_optimization = TRUE;

vm_map_t
<span class="enscript-function-name">vm_map_create</span>(
	pmap_t			pmap,
	vm_map_offset_t	min,
	vm_map_offset_t	max,
	boolean_t		pageable)
{
	<span class="enscript-type">static</span> <span class="enscript-type">int</span>		color_seed = 0;
	<span class="enscript-type">register</span> vm_map_t	result;
	<span class="enscript-type">struct</span> vm_map_links	*hole_entry = NULL;

	result = (vm_map_t) zalloc(vm_map_zone);
	<span class="enscript-keyword">if</span> (result == VM_MAP_NULL)
		panic(<span class="enscript-string">&quot;vm_map_create&quot;</span>);

	vm_map_first_entry(result) = vm_map_to_entry(result);
	vm_map_last_entry(result)  = vm_map_to_entry(result);
	result-&gt;hdr.nentries = 0;
	result-&gt;hdr.entries_pageable = pageable;

	vm_map_store_init( &amp;(result-&gt;hdr) );
	
	result-&gt;hdr.page_shift = PAGE_SHIFT;

	result-&gt;size = 0;
	result-&gt;user_wire_limit = MACH_VM_MAX_ADDRESS;	<span class="enscript-comment">/* default limit is unlimited */</span>
	result-&gt;user_wire_size  = 0;
	result-&gt;ref_count = 1;
#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">TASK_SWAPPER</span>
	result-&gt;res_count = 1;
	result-&gt;sw_state = MAP_SW_IN;
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* TASK_SWAPPER */</span>
	result-&gt;pmap = pmap;
	result-&gt;min_offset = min;
	result-&gt;max_offset = max;
	result-&gt;wiring_required = FALSE;
	result-&gt;no_zero_fill = FALSE;
	result-&gt;mapped_in_other_pmaps = FALSE;
	result-&gt;wait_for_space = FALSE;
	result-&gt;switch_protect = FALSE;
	result-&gt;disable_vmentry_reuse = FALSE;
	result-&gt;map_disallow_data_exec = FALSE;
	result-&gt;highest_entry_end = 0;
	result-&gt;first_free = vm_map_to_entry(result);
	result-&gt;hint = vm_map_to_entry(result);
	result-&gt;color_rr = (color_seed++) &amp; vm_color_mask;
 	result-&gt;jit_entry_exists = FALSE;

	<span class="enscript-keyword">if</span> (vm_map_supports_hole_optimization &amp;&amp; pmap != kernel_pmap) {
		hole_entry = zalloc(vm_map_holes_zone);

		hole_entry-&gt;start = min;
		hole_entry-&gt;end = (max &gt; (vm_map_offset_t)MACH_VM_MAX_ADDRESS) ? max : (vm_map_offset_t)MACH_VM_MAX_ADDRESS;
		result-&gt;holes_list = result-&gt;hole_hint = hole_entry;
		hole_entry-&gt;prev = hole_entry-&gt;next = (vm_map_entry_t) hole_entry;
		result-&gt;holelistenabled = TRUE;

	} <span class="enscript-keyword">else</span> {

		result-&gt;holelistenabled = FALSE;
	}

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">CONFIG_FREEZE</span>
	result-&gt;default_freezer_handle = NULL;
#<span class="enscript-reference">endif</span>
	vm_map_lock_init(result);
	lck_mtx_init_ext(&amp;result-&gt;s_lock, &amp;result-&gt;s_lock_ext, &amp;vm_map_lck_grp, &amp;vm_map_lck_attr);
	
	<span class="enscript-keyword">return</span>(result);
}

<span class="enscript-comment">/*
 *	vm_map_entry_create:	[ internal use only ]
 *
 *	Allocates a VM map entry for insertion in the
 *	given map (or map copy).  No fields are filled.
 */</span>
#<span class="enscript-reference">define</span>	<span class="enscript-function-name">vm_map_entry_create</span>(map, map_locked)	_vm_map_entry_create(&amp;(map)-&gt;hdr, map_locked)

#<span class="enscript-reference">define</span>	<span class="enscript-function-name">vm_map_copy_entry_create</span>(copy, map_locked)					\
	_vm_map_entry_create(&amp;(copy)-&gt;cpy_hdr, map_locked)
<span class="enscript-type">unsigned</span> reserved_zalloc_count, nonreserved_zalloc_count;

<span class="enscript-type">static</span> vm_map_entry_t
<span class="enscript-function-name">_vm_map_entry_create</span>(
	<span class="enscript-type">struct</span> vm_map_header	*map_header, boolean_t __unused map_locked)
{
	zone_t	zone;
	vm_map_entry_t	entry;

	zone = vm_map_entry_zone;

	assert(map_header-&gt;entries_pageable ? !map_locked : TRUE);

	<span class="enscript-keyword">if</span> (map_header-&gt;entries_pageable) {
		entry = (vm_map_entry_t) zalloc(zone);
	}
	<span class="enscript-keyword">else</span> {
		entry = (vm_map_entry_t) zalloc_canblock(zone, FALSE);

		<span class="enscript-keyword">if</span> (entry == VM_MAP_ENTRY_NULL) {
			zone = vm_map_entry_reserved_zone;
			entry = (vm_map_entry_t) zalloc(zone);
			OSAddAtomic(1, &amp;reserved_zalloc_count);
		} <span class="enscript-keyword">else</span>
			OSAddAtomic(1, &amp;nonreserved_zalloc_count);
	}

	<span class="enscript-keyword">if</span> (entry == VM_MAP_ENTRY_NULL)
		panic(<span class="enscript-string">&quot;vm_map_entry_create&quot;</span>);
	entry-&gt;from_reserved_zone = (zone == vm_map_entry_reserved_zone);

	vm_map_store_update( (vm_map_t) NULL, entry, VM_MAP_ENTRY_CREATE);
#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">MAP_ENTRY_CREATION_DEBUG</span>
	entry-&gt;vme_creation_maphdr = map_header;
	fastbacktrace(&amp;entry-&gt;vme_creation_bt[0],
		      (<span class="enscript-keyword">sizeof</span>(entry-&gt;vme_creation_bt)/<span class="enscript-keyword">sizeof</span>(uintptr_t)));
#<span class="enscript-reference">endif</span>
	<span class="enscript-keyword">return</span>(entry);
}

<span class="enscript-comment">/*
 *	vm_map_entry_dispose:	[ internal use only ]
 *
 *	Inverse of vm_map_entry_create.
 *
 * 	write map lock held so no need to
 *	do anything special to insure correctness
 * 	of the stores
 */</span>
#<span class="enscript-reference">define</span>	<span class="enscript-function-name">vm_map_entry_dispose</span>(map, entry)			\
	_vm_map_entry_dispose(&amp;(map)-&gt;hdr, (entry))

#<span class="enscript-reference">define</span>	<span class="enscript-function-name">vm_map_copy_entry_dispose</span>(map, entry) \
	_vm_map_entry_dispose(&amp;(copy)-&gt;cpy_hdr, (entry))

<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">_vm_map_entry_dispose</span>(
	<span class="enscript-type">register</span> <span class="enscript-type">struct</span> vm_map_header	*map_header,
	<span class="enscript-type">register</span> vm_map_entry_t		entry)
{
	<span class="enscript-type">register</span> zone_t		zone;

	<span class="enscript-keyword">if</span> (map_header-&gt;entries_pageable || !(entry-&gt;from_reserved_zone))
		zone = vm_map_entry_zone;
	<span class="enscript-keyword">else</span>
		zone = vm_map_entry_reserved_zone;

	<span class="enscript-keyword">if</span> (!map_header-&gt;entries_pageable) {
		<span class="enscript-keyword">if</span> (zone == vm_map_entry_zone)
			OSAddAtomic(-1, &amp;nonreserved_zalloc_count);
		<span class="enscript-keyword">else</span>
			OSAddAtomic(-1, &amp;reserved_zalloc_count);
	}

	zfree(zone, entry);
}

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">MACH_ASSERT</span>
<span class="enscript-type">static</span> boolean_t first_free_check = FALSE;
boolean_t
<span class="enscript-function-name">first_free_is_valid</span>(
	vm_map_t	map)
{
	<span class="enscript-keyword">if</span> (!first_free_check)
		<span class="enscript-keyword">return</span> TRUE;
	
	<span class="enscript-keyword">return</span>( first_free_is_valid_store( map ));	
}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* MACH_ASSERT */</span>


#<span class="enscript-reference">define</span> <span class="enscript-function-name">vm_map_copy_entry_link</span>(copy, after_where, entry)		\
	_vm_map_store_entry_link(&amp;(copy)-&gt;cpy_hdr, after_where, (entry))

#<span class="enscript-reference">define</span> <span class="enscript-function-name">vm_map_copy_entry_unlink</span>(copy, entry)				\
	_vm_map_store_entry_unlink(&amp;(copy)-&gt;cpy_hdr, (entry))

#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">MACH_ASSERT</span> &amp;&amp; <span class="enscript-variable-name">TASK_SWAPPER</span>
<span class="enscript-comment">/*
 *	vm_map_res_reference:
 *
 *	Adds another valid residence count to the given map.
 *
 *	Map is locked so this function can be called from
 *	vm_map_swapin.
 *
 */</span>
<span class="enscript-type">void</span> <span class="enscript-function-name">vm_map_res_reference</span>(<span class="enscript-type">register</span> vm_map_t map)
{
	<span class="enscript-comment">/* assert map is locked */</span>
	assert(map-&gt;res_count &gt;= 0);
	assert(map-&gt;ref_count &gt;= map-&gt;res_count);
	<span class="enscript-keyword">if</span> (map-&gt;res_count == 0) {
		lck_mtx_unlock(&amp;map-&gt;s_lock);
		vm_map_lock(map);
		vm_map_swapin(map);
		lck_mtx_lock(&amp;map-&gt;s_lock);
		++map-&gt;res_count;
		vm_map_unlock(map);
	} <span class="enscript-keyword">else</span>
		++map-&gt;res_count;
}

<span class="enscript-comment">/*
 *	vm_map_reference_swap:
 *
 *	Adds valid reference and residence counts to the given map.
 *
 *	The map may not be in memory (i.e. zero residence count).
 *
 */</span>
<span class="enscript-type">void</span> <span class="enscript-function-name">vm_map_reference_swap</span>(<span class="enscript-type">register</span> vm_map_t map)
{
	assert(map != VM_MAP_NULL);
	lck_mtx_lock(&amp;map-&gt;s_lock);
	assert(map-&gt;res_count &gt;= 0);
	assert(map-&gt;ref_count &gt;= map-&gt;res_count);
	map-&gt;ref_count++;
	vm_map_res_reference(map);
	lck_mtx_unlock(&amp;map-&gt;s_lock);
}

<span class="enscript-comment">/*
 *	vm_map_res_deallocate:
 *
 *	Decrement residence count on a map; possibly causing swapout.
 *
 *	The map must be in memory (i.e. non-zero residence count).
 *
 *	The map is locked, so this function is callable from vm_map_deallocate.
 *
 */</span>
<span class="enscript-type">void</span> <span class="enscript-function-name">vm_map_res_deallocate</span>(<span class="enscript-type">register</span> vm_map_t map)
{
	assert(map-&gt;res_count &gt; 0);
	<span class="enscript-keyword">if</span> (--map-&gt;res_count == 0) {
		lck_mtx_unlock(&amp;map-&gt;s_lock);
		vm_map_lock(map);
		vm_map_swapout(map);
		vm_map_unlock(map);
		lck_mtx_lock(&amp;map-&gt;s_lock);
	}
	assert(map-&gt;ref_count &gt;= map-&gt;res_count);
}
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* MACH_ASSERT &amp;&amp; TASK_SWAPPER */</span>

<span class="enscript-comment">/*
 *	vm_map_destroy:
 *
 *	Actually destroy a map.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_destroy</span>(
	vm_map_t	map,
	<span class="enscript-type">int</span>		flags)
{	
	vm_map_lock(map);

	<span class="enscript-comment">/* final cleanup: no need to unnest shared region */</span>
	flags |= VM_MAP_REMOVE_NO_UNNESTING;

	<span class="enscript-comment">/* clean up regular map entries */</span>
	(<span class="enscript-type">void</span>) vm_map_delete(map, map-&gt;min_offset, map-&gt;max_offset,
			     flags, VM_MAP_NULL);
	<span class="enscript-comment">/* clean up leftover special mappings (commpage, etc...) */</span>
	(<span class="enscript-type">void</span>) vm_map_delete(map, 0x0, 0xFFFFFFFFFFFFF000ULL,
			     flags, VM_MAP_NULL);

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">CONFIG_FREEZE</span>
	<span class="enscript-keyword">if</span> (map-&gt;default_freezer_handle) {
		default_freezer_handle_deallocate(map-&gt;default_freezer_handle);
		map-&gt;default_freezer_handle = NULL;
	}
#<span class="enscript-reference">endif</span>
	vm_map_disable_hole_optimization(map);
	vm_map_unlock(map);

	assert(map-&gt;hdr.nentries == 0);
	
	<span class="enscript-keyword">if</span>(map-&gt;pmap)
		pmap_destroy(map-&gt;pmap);

	zfree(vm_map_zone, map);
}

#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">TASK_SWAPPER</span>
<span class="enscript-comment">/*
 * vm_map_swapin/vm_map_swapout
 *
 * Swap a map in and out, either referencing or releasing its resources.  
 * These functions are internal use only; however, they must be exported
 * because they may be called from macros, which are exported.
 *
 * In the case of swapout, there could be races on the residence count, 
 * so if the residence count is up, we return, assuming that a 
 * vm_map_deallocate() call in the near future will bring us back.
 *
 * Locking:
 *	-- We use the map write lock for synchronization among races.
 *	-- The map write lock, and not the simple s_lock, protects the
 *	   swap state of the map.
 *	-- If a map entry is a share map, then we hold both locks, in
 *	   hierarchical order.
 *
 * Synchronization Notes:
 *	1) If a vm_map_swapin() call happens while swapout in progress, it
 *	will block on the map lock and proceed when swapout is through.
 *	2) A vm_map_reference() call at this time is illegal, and will
 *	cause a panic.  vm_map_reference() is only allowed on resident
 *	maps, since it refuses to block.
 *	3) A vm_map_swapin() call during a swapin will block, and 
 *	proceeed when the first swapin is done, turning into a nop.
 *	This is the reason the res_count is not incremented until
 *	after the swapin is complete.
 *	4) There is a timing hole after the checks of the res_count, before
 *	the map lock is taken, during which a swapin may get the lock
 *	before a swapout about to happen.  If this happens, the swapin
 *	will detect the state and increment the reference count, causing
 *	the swapout to be a nop, thereby delaying it until a later 
 *	vm_map_deallocate.  If the swapout gets the lock first, then 
 *	the swapin will simply block until the swapout is done, and 
 *	then proceed.
 *
 * Because vm_map_swapin() is potentially an expensive operation, it
 * should be used with caution.
 *
 * Invariants:
 *	1) A map with a residence count of zero is either swapped, or
 *	   being swapped.
 *	2) A map with a non-zero residence count is either resident,
 *	   or being swapped in.
 */</span>

<span class="enscript-type">int</span> vm_map_swap_enable = 1;

<span class="enscript-type">void</span> <span class="enscript-function-name">vm_map_swapin</span> (vm_map_t map)
{
	<span class="enscript-type">register</span> vm_map_entry_t entry;

	<span class="enscript-keyword">if</span> (!vm_map_swap_enable)	<span class="enscript-comment">/* debug */</span>
		<span class="enscript-keyword">return</span>;

	<span class="enscript-comment">/*
	 * Map is locked
	 * First deal with various races.
	 */</span>
	<span class="enscript-keyword">if</span> (map-&gt;sw_state == MAP_SW_IN)
		<span class="enscript-comment">/* 
		 * we raced with swapout and won.  Returning will incr.
		 * the res_count, turning the swapout into a nop.
		 */</span>
		<span class="enscript-keyword">return</span>;

	<span class="enscript-comment">/*
	 * The residence count must be zero.  If we raced with another
	 * swapin, the state would have been IN; if we raced with a
	 * swapout (after another competing swapin), we must have lost
	 * the race to get here (see above comment), in which case
	 * res_count is still 0.
	 */</span>
	assert(map-&gt;res_count == 0);

	<span class="enscript-comment">/*
	 * There are no intermediate states of a map going out or
	 * coming in, since the map is locked during the transition.
	 */</span>
	assert(map-&gt;sw_state == MAP_SW_OUT);

	<span class="enscript-comment">/*
	 * We now operate upon each map entry.  If the entry is a sub- 
	 * or share-map, we call vm_map_res_reference upon it.
	 * If the entry is an object, we call vm_object_res_reference
	 * (this may iterate through the shadow chain).
	 * Note that we hold the map locked the entire time,
	 * even if we get back here via a recursive call in
	 * vm_map_res_reference.
	 */</span>
	entry = vm_map_first_entry(map);

	<span class="enscript-keyword">while</span> (entry != vm_map_to_entry(map)) {
		<span class="enscript-keyword">if</span> (VME_OBJECT(entry) != VM_OBJECT_NULL) {
			<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
				vm_map_t lmap = VME_SUBMAP(entry);
				lck_mtx_lock(&amp;lmap-&gt;s_lock);
				vm_map_res_reference(lmap);
				lck_mtx_unlock(&amp;lmap-&gt;s_lock);
			} <span class="enscript-keyword">else</span> {
				vm_object_t object = VME_OBEJCT(entry);
				vm_object_lock(object);
				<span class="enscript-comment">/*
				 * This call may iterate through the
				 * shadow chain.
				 */</span>
				vm_object_res_reference(object);
				vm_object_unlock(object);
			}
		}
		entry = entry-&gt;vme_next;
	}
	assert(map-&gt;sw_state == MAP_SW_OUT);
	map-&gt;sw_state = MAP_SW_IN;
}

<span class="enscript-type">void</span> <span class="enscript-function-name">vm_map_swapout</span>(vm_map_t map)
{
	<span class="enscript-type">register</span> vm_map_entry_t entry;
	
	<span class="enscript-comment">/*
	 * Map is locked
	 * First deal with various races.
	 * If we raced with a swapin and lost, the residence count
	 * will have been incremented to 1, and we simply return.
	 */</span>
	lck_mtx_lock(&amp;map-&gt;s_lock);
	<span class="enscript-keyword">if</span> (map-&gt;res_count != 0) {
		lck_mtx_unlock(&amp;map-&gt;s_lock);
		<span class="enscript-keyword">return</span>;
	}
	lck_mtx_unlock(&amp;map-&gt;s_lock);

	<span class="enscript-comment">/*
	 * There are no intermediate states of a map going out or
	 * coming in, since the map is locked during the transition.
	 */</span>
	assert(map-&gt;sw_state == MAP_SW_IN);

	<span class="enscript-keyword">if</span> (!vm_map_swap_enable)
		<span class="enscript-keyword">return</span>;

	<span class="enscript-comment">/*
	 * We now operate upon each map entry.  If the entry is a sub- 
	 * or share-map, we call vm_map_res_deallocate upon it.
	 * If the entry is an object, we call vm_object_res_deallocate
	 * (this may iterate through the shadow chain).
	 * Note that we hold the map locked the entire time,
	 * even if we get back here via a recursive call in
	 * vm_map_res_deallocate.
	 */</span>
	entry = vm_map_first_entry(map);

	<span class="enscript-keyword">while</span> (entry != vm_map_to_entry(map)) {
		<span class="enscript-keyword">if</span> (VME_OBJECT(entry) != VM_OBJECT_NULL) {
			<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
				vm_map_t lmap = VME_SUBMAP(entry);
				lck_mtx_lock(&amp;lmap-&gt;s_lock);
				vm_map_res_deallocate(lmap);
				lck_mtx_unlock(&amp;lmap-&gt;s_lock);
			} <span class="enscript-keyword">else</span> {
				vm_object_t object = VME_OBJECT(entry);
				vm_object_lock(object);
				<span class="enscript-comment">/*
				 * This call may take a long time, 
				 * since it could actively push 
				 * out pages (if we implement it 
				 * that way).
				 */</span>
				vm_object_res_deallocate(object);
				vm_object_unlock(object);
			}
		}
		entry = entry-&gt;vme_next;
	}
	assert(map-&gt;sw_state == MAP_SW_IN);
	map-&gt;sw_state = MAP_SW_OUT;
}

#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* TASK_SWAPPER */</span>

<span class="enscript-comment">/*
 *	vm_map_lookup_entry:	[ internal use only ]
 *
 *	Calls into the vm map store layer to find the map 
 *	entry containing (or immediately preceding) the 
 *	specified address in the given map; the entry is returned
 *	in the &quot;entry&quot; parameter.  The boolean
 *	result indicates whether the address is
 *	actually contained in the map.
 */</span>
boolean_t
<span class="enscript-function-name">vm_map_lookup_entry</span>(
	<span class="enscript-type">register</span> vm_map_t		map,
	<span class="enscript-type">register</span> vm_map_offset_t	address,
	vm_map_entry_t		*entry)		<span class="enscript-comment">/* OUT */</span>
{
	<span class="enscript-keyword">return</span> ( vm_map_store_lookup_entry( map, address, entry ));
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_find_space
 *	Purpose:
 *		Allocate a range in the specified virtual address map,
 *		returning the entry allocated for that range.
 *		Used by kmem_alloc, etc.
 *
 *		The map must be NOT be locked. It will be returned locked
 *		on KERN_SUCCESS, unlocked on failure.
 *
 *		If an entry is allocated, the object/offset fields
 *		are initialized to zero.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_find_space</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	vm_map_offset_t		*address,	<span class="enscript-comment">/* OUT */</span>
	vm_map_size_t		size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	vm_map_entry_t		*o_entry)	<span class="enscript-comment">/* OUT */</span>
{
	vm_map_entry_t			entry, new_entry;
	<span class="enscript-type">register</span> vm_map_offset_t	start;
	<span class="enscript-type">register</span> vm_map_offset_t	end;
	vm_map_entry_t			hole_entry;

	<span class="enscript-keyword">if</span> (size == 0) {
		*address = 0;
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}

	<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_GUARD_AFTER) {
		<span class="enscript-comment">/* account for the back guard page in the size */</span>
		size += VM_MAP_PAGE_SIZE(map);
	}

	new_entry = vm_map_entry_create(map, FALSE);

	<span class="enscript-comment">/*
	 *	Look for the first possible address; if there's already
	 *	something at this address, we have to start after it.
	 */</span>

	vm_map_lock(map);

	<span class="enscript-keyword">if</span>( map-&gt;disable_vmentry_reuse == TRUE) {
		VM_MAP_HIGHEST_ENTRY(map, entry, start);
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			hole_entry = (vm_map_entry_t)map-&gt;holes_list;

			<span class="enscript-keyword">if</span> (hole_entry == NULL) {
				<span class="enscript-comment">/*
				 * No more space in the map?
				 */</span>
				vm_map_entry_dispose(map, new_entry);
				vm_map_unlock(map);
				<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
			}

			entry = hole_entry;
			start = entry-&gt;vme_start;
		} <span class="enscript-keyword">else</span> {
			assert(first_free_is_valid(map));
			<span class="enscript-keyword">if</span> ((entry = map-&gt;first_free) == vm_map_to_entry(map))
				start = map-&gt;min_offset;
			<span class="enscript-keyword">else</span>
				start = entry-&gt;vme_end;
		}
	}

	<span class="enscript-comment">/*
	 *	In any case, the &quot;entry&quot; always precedes
	 *	the proposed new region throughout the loop:
	 */</span>

	<span class="enscript-keyword">while</span> (TRUE) {
		<span class="enscript-type">register</span> vm_map_entry_t	next;

		<span class="enscript-comment">/*
		 *	Find the end of the proposed new region.
		 *	Be sure we didn't go beyond the end, or
		 *	wrap around the address.
		 */</span>

		<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_GUARD_BEFORE) {
			<span class="enscript-comment">/* reserve space for the front guard page */</span>
			start += VM_MAP_PAGE_SIZE(map);
		}
		end = ((start + mask) &amp; ~mask);
			
		<span class="enscript-keyword">if</span> (end &lt; start) {
			vm_map_entry_dispose(map, new_entry);
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
		}
		start = end;
		end += size;

		<span class="enscript-keyword">if</span> ((end &gt; map-&gt;max_offset) || (end &lt; start)) {
			vm_map_entry_dispose(map, new_entry);
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
		}

		next = entry-&gt;vme_next;

		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			<span class="enscript-keyword">if</span> (entry-&gt;vme_end &gt;= end)
				<span class="enscript-keyword">break</span>;
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/*
			 *	If there are no more entries, we must win.
			 *
			 *	OR
			 *
			 *	If there is another entry, it must be
			 *	after the end of the potential new region.
			 */</span>

			<span class="enscript-keyword">if</span> (next == vm_map_to_entry(map))
				<span class="enscript-keyword">break</span>;

			<span class="enscript-keyword">if</span> (next-&gt;vme_start &gt;= end)
				<span class="enscript-keyword">break</span>;
		}

		<span class="enscript-comment">/*
		 *	Didn't fit -- move to the next entry.
		 */</span>

		entry = next;

		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			<span class="enscript-keyword">if</span> (entry == (vm_map_entry_t) map-&gt;holes_list) {
				<span class="enscript-comment">/*
				 * Wrapped around
				 */</span>
				vm_map_entry_dispose(map, new_entry);
				vm_map_unlock(map);
				<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
			}
			start = entry-&gt;vme_start;
		} <span class="enscript-keyword">else</span> {
			start = entry-&gt;vme_end;
		}
	}

	<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, entry-&gt;vme_start, &amp;entry)) {
			panic(<span class="enscript-string">&quot;Found an existing entry (%p) instead of potential hole at address: 0x%llx.\n&quot;</span>, entry, (<span class="enscript-type">unsigned</span> <span class="enscript-type">long</span> <span class="enscript-type">long</span>)entry-&gt;vme_start);
		}
	}

	<span class="enscript-comment">/*
	 *	At this point,
	 *		&quot;start&quot; and &quot;end&quot; should define the endpoints of the
	 *			available new range, and
	 *		&quot;entry&quot; should refer to the region before the new
	 *			range, and
	 *
	 *		the map should be locked.
	 */</span>

	<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_GUARD_BEFORE) {
		<span class="enscript-comment">/* go back for the front guard page */</span>
		start -= VM_MAP_PAGE_SIZE(map);
	}
	*address = start;

	assert(start &lt; end);
	new_entry-&gt;vme_start = start;
	new_entry-&gt;vme_end = end;
	assert(page_aligned(new_entry-&gt;vme_start));
	assert(page_aligned(new_entry-&gt;vme_end));
	assert(VM_MAP_PAGE_ALIGNED(new_entry-&gt;vme_start,
				   VM_MAP_PAGE_MASK(map)));
	assert(VM_MAP_PAGE_ALIGNED(new_entry-&gt;vme_end,
				   VM_MAP_PAGE_MASK(map)));

	new_entry-&gt;is_shared = FALSE;
	new_entry-&gt;is_sub_map = FALSE;
	new_entry-&gt;use_pmap = TRUE;
	VME_OBJECT_SET(new_entry, VM_OBJECT_NULL);
	VME_OFFSET_SET(new_entry, (vm_object_offset_t) 0);

	new_entry-&gt;needs_copy = FALSE;

	new_entry-&gt;inheritance = VM_INHERIT_DEFAULT;
	new_entry-&gt;protection = VM_PROT_DEFAULT;
	new_entry-&gt;max_protection = VM_PROT_ALL;
	new_entry-&gt;behavior = VM_BEHAVIOR_DEFAULT;
	new_entry-&gt;wired_count = 0;
	new_entry-&gt;user_wired_count = 0;

	new_entry-&gt;in_transition = FALSE;
	new_entry-&gt;needs_wakeup = FALSE;
	new_entry-&gt;no_cache = FALSE;
	new_entry-&gt;permanent = FALSE;
	new_entry-&gt;superpage_size = FALSE;
	<span class="enscript-keyword">if</span> (VM_MAP_PAGE_SHIFT(map) != PAGE_SHIFT) {
		new_entry-&gt;map_aligned = TRUE;
	} <span class="enscript-keyword">else</span> {
		new_entry-&gt;map_aligned = FALSE;
	}

	new_entry-&gt;used_for_jit = FALSE;
	new_entry-&gt;zero_wired_pages = FALSE;
	new_entry-&gt;iokit_acct = FALSE;
	new_entry-&gt;vme_resilient_codesign = FALSE;
	new_entry-&gt;vme_resilient_media = FALSE;

	<span class="enscript-type">int</span> alias;
	VM_GET_FLAGS_ALIAS(flags, alias);
	VME_ALIAS_SET(new_entry, alias);

	<span class="enscript-comment">/*
	 *	Insert the new entry into the list
	 */</span>

	vm_map_store_entry_link(map, entry, new_entry);

	map-&gt;size += size;

	<span class="enscript-comment">/*
	 *	Update the lookup hint
	 */</span>
	SAVE_HINT_MAP_WRITE(map, new_entry);

	*o_entry = new_entry;
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}

<span class="enscript-type">int</span> vm_map_pmap_enter_print = FALSE;
<span class="enscript-type">int</span> vm_map_pmap_enter_enable = FALSE;

<span class="enscript-comment">/*
 *	Routine:	vm_map_pmap_enter [internal only]
 *
 *	Description:
 *		Force pages from the specified object to be entered into
 *		the pmap at the specified address if they are present.
 *		As soon as a page not found in the object the scan ends.
 *
 *	Returns:
 *		Nothing.  
 *
 *	In/out conditions:
 *		The source map should not be locked on entry.
 */</span>
__unused <span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_pmap_enter</span>(
	vm_map_t		map,
	<span class="enscript-type">register</span> vm_map_offset_t 	addr,
	<span class="enscript-type">register</span> vm_map_offset_t	end_addr,
	<span class="enscript-type">register</span> vm_object_t 	object,
	vm_object_offset_t	offset,
	vm_prot_t		protection)
{
	<span class="enscript-type">int</span>			type_of_fault;
	kern_return_t		kr;

	<span class="enscript-keyword">if</span>(map-&gt;pmap == 0)
		<span class="enscript-keyword">return</span>;

	<span class="enscript-keyword">while</span> (addr &lt; end_addr) {
		<span class="enscript-type">register</span> vm_page_t	m;


		<span class="enscript-comment">/*
   		 * TODO:
		 * From vm_map_enter(), we come into this function without the map
		 * lock held or the object lock held.
		 * We haven't taken a reference on the object either.
		 * We should do a proper lookup on the map to make sure
		 * that things are sane before we go locking objects that
		 * could have been deallocated from under us.
		 */</span>

		vm_object_lock(object);

		m = vm_page_lookup(object, offset);
		<span class="enscript-comment">/*
		 * ENCRYPTED SWAP:
		 * The user should never see encrypted data, so do not
		 * enter an encrypted page in the page table.
		 */</span>
		<span class="enscript-keyword">if</span> (m == VM_PAGE_NULL || m-&gt;busy || m-&gt;encrypted ||
		    m-&gt;fictitious ||
		    (m-&gt;unusual &amp;&amp; ( m-&gt;error || m-&gt;restart || m-&gt;absent))) {
			vm_object_unlock(object);
			<span class="enscript-keyword">return</span>;
		}

		<span class="enscript-keyword">if</span> (vm_map_pmap_enter_print) {
			printf(<span class="enscript-string">&quot;vm_map_pmap_enter:&quot;</span>);
			printf(<span class="enscript-string">&quot;map: %p, addr: %llx, object: %p, offset: %llx\n&quot;</span>,
			       map, (<span class="enscript-type">unsigned</span> <span class="enscript-type">long</span> <span class="enscript-type">long</span>)addr, object, (<span class="enscript-type">unsigned</span> <span class="enscript-type">long</span> <span class="enscript-type">long</span>)offset);
		}
		type_of_fault = DBG_CACHE_HIT_FAULT;
		kr = vm_fault_enter(m, map-&gt;pmap, addr, protection, protection,
				    VM_PAGE_WIRED(m), FALSE, FALSE, FALSE,
				    0, <span class="enscript-comment">/* XXX need user tag / alias? */</span>
				    0, <span class="enscript-comment">/* alternate accounting? */</span>
				    NULL,
				    &amp;type_of_fault);

		vm_object_unlock(object);

		offset += PAGE_SIZE_64;
		addr += PAGE_SIZE;
	}
}

boolean_t <span class="enscript-function-name">vm_map_pmap_is_empty</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t end);
boolean_t <span class="enscript-function-name">vm_map_pmap_is_empty</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end)
{
#<span class="enscript-reference">ifdef</span> <span class="enscript-variable-name">MACHINE_PMAP_IS_EMPTY</span>
	<span class="enscript-keyword">return</span> pmap_is_empty(map-&gt;pmap, start, end);
#<span class="enscript-reference">else</span> 	<span class="enscript-comment">/* MACHINE_PMAP_IS_EMPTY */</span>
	vm_map_offset_t	offset;
	ppnum_t		phys_page;

	<span class="enscript-keyword">if</span> (map-&gt;pmap == NULL) {
		<span class="enscript-keyword">return</span> TRUE;
	}

	<span class="enscript-keyword">for</span> (offset = start;
	     offset &lt; end;
	     offset += PAGE_SIZE) {
		phys_page = pmap_find_phys(map-&gt;pmap, offset);
		<span class="enscript-keyword">if</span> (phys_page) {
			kprintf(<span class="enscript-string">&quot;vm_map_pmap_is_empty(%p,0x%llx,0x%llx): &quot;</span>
				<span class="enscript-string">&quot;page %d at 0x%llx\n&quot;</span>,
				map, (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)start, (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)end,
				phys_page, (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)offset);
			<span class="enscript-keyword">return</span> FALSE;
		}
	}
	<span class="enscript-keyword">return</span> TRUE;
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* MACHINE_PMAP_IS_EMPTY */</span>
}

#<span class="enscript-reference">define</span> <span class="enscript-variable-name">MAX_TRIES_TO_GET_RANDOM_ADDRESS</span>	1000
kern_return_t
<span class="enscript-function-name">vm_map_random_address_for_size</span>(
	vm_map_t	map,
	vm_map_offset_t	*address,
	vm_map_size_t	size)
{
	kern_return_t	kr = KERN_SUCCESS;
	<span class="enscript-type">int</span>		tries = 0;
	vm_map_offset_t	random_addr = 0;
	vm_map_offset_t hole_end;

	vm_map_entry_t	next_entry = VM_MAP_ENTRY_NULL;
	vm_map_entry_t	prev_entry = VM_MAP_ENTRY_NULL;
	vm_map_size_t	vm_hole_size = 0;
	vm_map_size_t	addr_space_size;

	addr_space_size = vm_map_max(map) - vm_map_min(map);

	assert(page_aligned(size));

	<span class="enscript-keyword">while</span> (tries &lt; MAX_TRIES_TO_GET_RANDOM_ADDRESS) {
		random_addr = ((vm_map_offset_t)random()) &lt;&lt; PAGE_SHIFT;
		random_addr = vm_map_trunc_page(
			vm_map_min(map) +(random_addr % addr_space_size),
			VM_MAP_PAGE_MASK(map));

		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, random_addr, &amp;prev_entry) == FALSE) {
			<span class="enscript-keyword">if</span> (prev_entry == vm_map_to_entry(map)) {
				next_entry = vm_map_first_entry(map);
			} <span class="enscript-keyword">else</span> {
				next_entry = prev_entry-&gt;vme_next;
			}
			<span class="enscript-keyword">if</span> (next_entry == vm_map_to_entry(map)) {
				hole_end = vm_map_max(map);
			} <span class="enscript-keyword">else</span> {
				hole_end = next_entry-&gt;vme_start;
			}
			vm_hole_size = hole_end - random_addr;
			<span class="enscript-keyword">if</span> (vm_hole_size &gt;= size) {
				*address = random_addr;
				<span class="enscript-keyword">break</span>;
			}
		}
		tries++;
	}

	<span class="enscript-keyword">if</span> (tries == MAX_TRIES_TO_GET_RANDOM_ADDRESS) {
		kr = KERN_NO_SPACE;
	}
	<span class="enscript-keyword">return</span> kr;
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_enter
 *
 *	Description:
 *		Allocate a range in the specified virtual address map.
 *		The resulting range will refer to memory defined by
 *		the given memory object and offset into that object.
 *
 *		Arguments are as defined in the vm_map call.
 */</span>
<span class="enscript-type">int</span> _map_enter_debug = 0;
<span class="enscript-type">static</span> <span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> vm_map_enter_restore_successes = 0;
<span class="enscript-type">static</span> <span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> vm_map_enter_restore_failures = 0;
kern_return_t
<span class="enscript-function-name">vm_map_enter</span>(
	vm_map_t		map,
	vm_map_offset_t		*address,	<span class="enscript-comment">/* IN/OUT */</span>
	vm_map_size_t		size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	vm_object_t		object,
	vm_object_offset_t	offset,
	boolean_t		needs_copy,
	vm_prot_t		cur_protection,
	vm_prot_t		max_protection,
	vm_inherit_t		inheritance)
{
	vm_map_entry_t		entry, new_entry;
	vm_map_offset_t		start, tmp_start, tmp_offset;
	vm_map_offset_t		end, tmp_end;
	vm_map_offset_t		tmp2_start, tmp2_end;
	vm_map_offset_t		step;
	kern_return_t		result = KERN_SUCCESS;
	vm_map_t		zap_old_map = VM_MAP_NULL;
	vm_map_t		zap_new_map = VM_MAP_NULL;
	boolean_t		map_locked = FALSE;
	boolean_t		pmap_empty = TRUE;
	boolean_t		new_mapping_established = FALSE;
	boolean_t		keep_map_locked = ((flags &amp; VM_FLAGS_KEEP_MAP_LOCKED) != 0);
	boolean_t		anywhere = ((flags &amp; VM_FLAGS_ANYWHERE) != 0);
	boolean_t		purgable = ((flags &amp; VM_FLAGS_PURGABLE) != 0);
	boolean_t		overwrite = ((flags &amp; VM_FLAGS_OVERWRITE) != 0);
	boolean_t		no_cache = ((flags &amp; VM_FLAGS_NO_CACHE) != 0);
	boolean_t		is_submap = ((flags &amp; VM_FLAGS_SUBMAP) != 0);
	boolean_t		permanent = ((flags &amp; VM_FLAGS_PERMANENT) != 0);
	boolean_t		entry_for_jit = ((flags &amp; VM_FLAGS_MAP_JIT) != 0);
	boolean_t		iokit_acct = ((flags &amp; VM_FLAGS_IOKIT_ACCT) != 0);
	boolean_t		resilient_codesign = ((flags &amp; VM_FLAGS_RESILIENT_CODESIGN) != 0);
	boolean_t		resilient_media = ((flags &amp; VM_FLAGS_RESILIENT_MEDIA) != 0);
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		superpage_size = ((flags &amp; VM_FLAGS_SUPERPAGE_MASK) &gt;&gt; VM_FLAGS_SUPERPAGE_SHIFT);
	vm_tag_t        	alias, user_alias;
	vm_map_offset_t		effective_min_offset, effective_max_offset;
	kern_return_t		kr;
	boolean_t		clear_map_aligned = FALSE;
	vm_map_entry_t		hole_entry;

	<span class="enscript-keyword">if</span> (superpage_size) {
		<span class="enscript-keyword">switch</span> (superpage_size) {
			<span class="enscript-comment">/*
			 * Note that the current implementation only supports
			 * a single size for superpages, SUPERPAGE_SIZE, per
			 * architecture. As soon as more sizes are supposed
			 * to be supported, SUPERPAGE_SIZE has to be replaced
			 * with a lookup of the size depending on superpage_size.
			 */</span>
#<span class="enscript-reference">ifdef</span> <span class="enscript-variable-name">__x86_64__</span>
			<span class="enscript-keyword">case</span> <span class="enscript-reference">SUPERPAGE_SIZE_ANY</span>:
				<span class="enscript-comment">/* handle it like 2 MB and round up to page size */</span>
				size = (size + 2*1024*1024 - 1) &amp; ~(2*1024*1024 - 1);
			<span class="enscript-keyword">case</span> <span class="enscript-reference">SUPERPAGE_SIZE_2MB</span>:
				<span class="enscript-keyword">break</span>;
#<span class="enscript-reference">endif</span>
			<span class="enscript-reference">default</span>:
				<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}
		mask = SUPERPAGE_SIZE-1;
		<span class="enscript-keyword">if</span> (size &amp; (SUPERPAGE_SIZE-1))
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		inheritance = VM_INHERIT_NONE;	<span class="enscript-comment">/* fork() children won't inherit superpages */</span>
	}



	<span class="enscript-keyword">if</span> (resilient_codesign || resilient_media) {
		<span class="enscript-keyword">if</span> ((cur_protection &amp; (VM_PROT_WRITE | VM_PROT_EXECUTE)) ||
		    (max_protection &amp; (VM_PROT_WRITE | VM_PROT_EXECUTE))) {
			<span class="enscript-keyword">return</span> KERN_PROTECTION_FAILURE;
		}
	}

	<span class="enscript-keyword">if</span> (is_submap) {
		<span class="enscript-keyword">if</span> (purgable) {
			<span class="enscript-comment">/* submaps can not be purgeable */</span>
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
			<span class="enscript-comment">/* submaps can not be created lazily */</span>
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}
	}
	<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_ALREADY) {
		<span class="enscript-comment">/*
		 * VM_FLAGS_ALREADY says that it's OK if the same mapping
		 * is already present.  For it to be meaningul, the requested
		 * mapping has to be at a fixed address (!VM_FLAGS_ANYWHERE) and
		 * we shouldn't try and remove what was mapped there first
		 * (!VM_FLAGS_OVERWRITE).
		 */</span>
		<span class="enscript-keyword">if</span> ((flags &amp; VM_FLAGS_ANYWHERE) ||
		    (flags &amp; VM_FLAGS_OVERWRITE)) {
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}
	}

	effective_min_offset = map-&gt;min_offset;

	<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_BEYOND_MAX) {
		<span class="enscript-comment">/*
		 * Allow an insertion beyond the map's max offset.
		 */</span>
		<span class="enscript-keyword">if</span> (vm_map_is_64bit(map))
			effective_max_offset = 0xFFFFFFFFFFFFF000ULL;
		<span class="enscript-keyword">else</span>
			effective_max_offset = 0x00000000FFFFF000ULL;
	} <span class="enscript-keyword">else</span> {
		effective_max_offset = map-&gt;max_offset;
	}

	<span class="enscript-keyword">if</span> (size == 0 ||
	    (offset &amp; PAGE_MASK_64) != 0) {
		*address = 0;
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}

	VM_GET_FLAGS_ALIAS(flags, alias);
	<span class="enscript-keyword">if</span> (map-&gt;pmap == kernel_pmap) {
		user_alias = VM_KERN_MEMORY_NONE;
	} <span class="enscript-keyword">else</span> {
		user_alias = alias;
	}

#<span class="enscript-reference">define</span>	<span class="enscript-function-name">RETURN</span>(value)	{ result = value; goto BailOut; }

	assert(page_aligned(*address));
	assert(page_aligned(size));

	<span class="enscript-keyword">if</span> (!VM_MAP_PAGE_ALIGNED(size, VM_MAP_PAGE_MASK(map))) {
		<span class="enscript-comment">/*
		 * In most cases, the caller rounds the size up to the
		 * map's page size.
		 * If we get a size that is explicitly not map-aligned here,
		 * we'll have to respect the caller's wish and mark the
		 * mapping as &quot;not map-aligned&quot; to avoid tripping the
		 * map alignment checks later.
		 */</span>
		clear_map_aligned = TRUE;
	}
	<span class="enscript-keyword">if</span> (!anywhere &amp;&amp; 
	    !VM_MAP_PAGE_ALIGNED(*address, VM_MAP_PAGE_MASK(map))) {
		<span class="enscript-comment">/*
		 * We've been asked to map at a fixed address and that
		 * address is not aligned to the map's specific alignment.
		 * The caller should know what it's doing (i.e. most likely
		 * mapping some fragmented copy map, transferring memory from
		 * a VM map with a different alignment), so clear map_aligned
		 * for this new VM map entry and proceed.
		 */</span>
		clear_map_aligned = TRUE;
	}

	<span class="enscript-comment">/*
	 * Only zero-fill objects are allowed to be purgable.
	 * LP64todo - limit purgable objects to 32-bits for now
	 */</span>
	<span class="enscript-keyword">if</span> (purgable &amp;&amp;
	    (offset != 0 ||
	     (object != VM_OBJECT_NULL &amp;&amp;
	      (object-&gt;vo_size != size ||
	       object-&gt;purgable == VM_PURGABLE_DENY))
	     || size &gt; ANON_MAX_SIZE)) <span class="enscript-comment">/* LP64todo: remove when dp capable */</span>
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;

	<span class="enscript-keyword">if</span> (!anywhere &amp;&amp; overwrite) {
		<span class="enscript-comment">/*
		 * Create a temporary VM map to hold the old mappings in the
		 * affected area while we create the new one.
		 * This avoids releasing the VM map lock in
		 * vm_map_entry_delete() and allows atomicity
		 * when we want to replace some mappings with a new one.
		 * It also allows us to restore the old VM mappings if the
		 * new mapping fails.
		 */</span>
		zap_old_map = vm_map_create(PMAP_NULL,
					    *address,
					    *address + size,
					    map-&gt;hdr.entries_pageable);
		vm_map_set_page_shift(zap_old_map, VM_MAP_PAGE_SHIFT(map));
		vm_map_disable_hole_optimization(zap_old_map);
	}

<span class="enscript-reference">StartAgain</span>: ;

	start = *address;

	<span class="enscript-keyword">if</span> (anywhere) {
		vm_map_lock(map);
		map_locked = TRUE;
		
		<span class="enscript-keyword">if</span> (entry_for_jit) {
			<span class="enscript-keyword">if</span> (map-&gt;jit_entry_exists) {
				result = KERN_INVALID_ARGUMENT;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">BailOut</span>;
			}
			<span class="enscript-comment">/*
			 * Get a random start address.
			 */</span>
			result = vm_map_random_address_for_size(map, address, size);
			<span class="enscript-keyword">if</span> (result != KERN_SUCCESS) {
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">BailOut</span>;
			}
			start = *address;
		}


		<span class="enscript-comment">/*
		 *	Calculate the first possible address.
		 */</span>

		<span class="enscript-keyword">if</span> (start &lt; effective_min_offset)
			start = effective_min_offset;
		<span class="enscript-keyword">if</span> (start &gt; effective_max_offset)
			RETURN(KERN_NO_SPACE);

		<span class="enscript-comment">/*
		 *	Look for the first possible address;
		 *	if there's already something at this
		 *	address, we have to start after it.
		 */</span>

		<span class="enscript-keyword">if</span>( map-&gt;disable_vmentry_reuse == TRUE) {
			VM_MAP_HIGHEST_ENTRY(map, entry, start);
		} <span class="enscript-keyword">else</span> {

			<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
				hole_entry = (vm_map_entry_t)map-&gt;holes_list;

				<span class="enscript-keyword">if</span> (hole_entry == NULL) {
					<span class="enscript-comment">/*
					 * No more space in the map?
					 */</span>
					result = KERN_NO_SPACE;
					<span class="enscript-keyword">goto</span> <span class="enscript-reference">BailOut</span>;
				} <span class="enscript-keyword">else</span> {

					boolean_t found_hole = FALSE;

					<span class="enscript-keyword">do</span> {
						<span class="enscript-keyword">if</span> (hole_entry-&gt;vme_start &gt;= start) {
							start = hole_entry-&gt;vme_start;
							found_hole = TRUE;
							<span class="enscript-keyword">break</span>;
						}

						<span class="enscript-keyword">if</span> (hole_entry-&gt;vme_end &gt; start) {
							found_hole = TRUE;
							<span class="enscript-keyword">break</span>;
						}
						hole_entry = hole_entry-&gt;vme_next;

					} <span class="enscript-keyword">while</span> (hole_entry != (vm_map_entry_t) map-&gt;holes_list);

					<span class="enscript-keyword">if</span> (found_hole == FALSE) {
						result = KERN_NO_SPACE;
						<span class="enscript-keyword">goto</span> <span class="enscript-reference">BailOut</span>;
					}

					entry = hole_entry;

					<span class="enscript-keyword">if</span> (start == 0)
						start += PAGE_SIZE_64;
				}
			} <span class="enscript-keyword">else</span> {
				assert(first_free_is_valid(map));

				entry = map-&gt;first_free;

				<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(map)) {
					entry = NULL;
				} <span class="enscript-keyword">else</span> {
				       <span class="enscript-keyword">if</span> (entry-&gt;vme_next == vm_map_to_entry(map)){
					       <span class="enscript-comment">/*
						* Hole at the end of the map.
						*/</span>
						entry = NULL;
				       } <span class="enscript-keyword">else</span> {
						<span class="enscript-keyword">if</span> (start &lt; (entry-&gt;vme_next)-&gt;vme_start ) {
							start = entry-&gt;vme_end;
							start = vm_map_round_page(start,
										  VM_MAP_PAGE_MASK(map));
						} <span class="enscript-keyword">else</span> {
							<span class="enscript-comment">/*
							 * Need to do a lookup.
							 */</span>
							entry = NULL;
						}
				       }
				}

				<span class="enscript-keyword">if</span> (entry == NULL) {
					vm_map_entry_t	tmp_entry;
					<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;tmp_entry)) {
						assert(!entry_for_jit);
						start = tmp_entry-&gt;vme_end;
						start = vm_map_round_page(start,
									  VM_MAP_PAGE_MASK(map));
					}
					entry = tmp_entry;
				}
			}
		}

		<span class="enscript-comment">/*
		 *	In any case, the &quot;entry&quot; always precedes
		 *	the proposed new region throughout the
		 *	loop:
		 */</span>

		<span class="enscript-keyword">while</span> (TRUE) {
			<span class="enscript-type">register</span> vm_map_entry_t	next;

			<span class="enscript-comment">/*
			 *	Find the end of the proposed new region.
			 *	Be sure we didn't go beyond the end, or
			 *	wrap around the address.
			 */</span>

			end = ((start + mask) &amp; ~mask);
			end = vm_map_round_page(end,
						VM_MAP_PAGE_MASK(map));
			<span class="enscript-keyword">if</span> (end &lt; start)
				RETURN(KERN_NO_SPACE);
			start = end;
			assert(VM_MAP_PAGE_ALIGNED(start,
						   VM_MAP_PAGE_MASK(map)));
			end += size;

			<span class="enscript-keyword">if</span> ((end &gt; effective_max_offset) || (end &lt; start)) {
				<span class="enscript-keyword">if</span> (map-&gt;wait_for_space) {
					assert(!keep_map_locked);
					<span class="enscript-keyword">if</span> (size &lt;= (effective_max_offset -
						     effective_min_offset)) {
						assert_wait((event_t)map,
							    THREAD_ABORTSAFE);
						vm_map_unlock(map);
						map_locked = FALSE;
						thread_block(THREAD_CONTINUE_NULL);
						<span class="enscript-keyword">goto</span> <span class="enscript-reference">StartAgain</span>;
					}
				}
				RETURN(KERN_NO_SPACE);
			}

			next = entry-&gt;vme_next;

			<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
				<span class="enscript-keyword">if</span> (entry-&gt;vme_end &gt;= end)
					<span class="enscript-keyword">break</span>;
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/*
				 *	If there are no more entries, we must win.
				 *
				 *	OR
				 *
				 *	If there is another entry, it must be
				 *	after the end of the potential new region.
				 */</span>

				<span class="enscript-keyword">if</span> (next == vm_map_to_entry(map))
					<span class="enscript-keyword">break</span>;

				<span class="enscript-keyword">if</span> (next-&gt;vme_start &gt;= end)
					<span class="enscript-keyword">break</span>;
			}

			<span class="enscript-comment">/*
			 *	Didn't fit -- move to the next entry.
			 */</span>

			entry = next;

			<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
				<span class="enscript-keyword">if</span> (entry == (vm_map_entry_t) map-&gt;holes_list) {
					<span class="enscript-comment">/*
					 * Wrapped around
					 */</span>
					result = KERN_NO_SPACE;
					<span class="enscript-keyword">goto</span> <span class="enscript-reference">BailOut</span>;
				}
				start = entry-&gt;vme_start;
			} <span class="enscript-keyword">else</span> {
				start = entry-&gt;vme_end;
			}

			start = vm_map_round_page(start,
						  VM_MAP_PAGE_MASK(map));
		}

		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, entry-&gt;vme_start, &amp;entry)) {
				panic(<span class="enscript-string">&quot;Found an existing entry (%p) instead of potential hole at address: 0x%llx.\n&quot;</span>, entry, (<span class="enscript-type">unsigned</span> <span class="enscript-type">long</span> <span class="enscript-type">long</span>)entry-&gt;vme_start);
			}
		}

		*address = start;
		assert(VM_MAP_PAGE_ALIGNED(*address,
					   VM_MAP_PAGE_MASK(map)));
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-comment">/*
		 *	Verify that:
		 *		the address doesn't itself violate
		 *		the mask requirement.
		 */</span>

		vm_map_lock(map);
		map_locked = TRUE;
		<span class="enscript-keyword">if</span> ((start &amp; mask) != 0)
			RETURN(KERN_NO_SPACE);

		<span class="enscript-comment">/*
		 *	...	the address is within bounds
		 */</span>

		end = start + size;

		<span class="enscript-keyword">if</span> ((start &lt; effective_min_offset) ||
		    (end &gt; effective_max_offset) ||
		    (start &gt;= end)) {
			RETURN(KERN_INVALID_ADDRESS);
		}

		<span class="enscript-keyword">if</span> (overwrite &amp;&amp; zap_old_map != VM_MAP_NULL) {
			<span class="enscript-comment">/*
			 * Fixed mapping and &quot;overwrite&quot; flag: attempt to
			 * remove all existing mappings in the specified
			 * address range, saving them in our &quot;zap_old_map&quot;.
			 */</span>
			(<span class="enscript-type">void</span>) vm_map_delete(map, start, end,
					     (VM_MAP_REMOVE_SAVE_ENTRIES |
					      VM_MAP_REMOVE_NO_MAP_ALIGN),
					     zap_old_map);
		}

		<span class="enscript-comment">/*
		 *	...	the starting address isn't allocated
		 */</span>

		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;entry)) {
			<span class="enscript-keyword">if</span> (! (flags &amp; VM_FLAGS_ALREADY)) {
				RETURN(KERN_NO_SPACE);
			}
			<span class="enscript-comment">/*
			 * Check if what's already there is what we want.
			 */</span>
			tmp_start = start;
			tmp_offset = offset;
			<span class="enscript-keyword">if</span> (entry-&gt;vme_start &lt; start) {
				tmp_start -= start - entry-&gt;vme_start;
				tmp_offset -= start - entry-&gt;vme_start;
				
			}
			<span class="enscript-keyword">for</span> (; entry-&gt;vme_start &lt; end;
			     entry = entry-&gt;vme_next) {
				<span class="enscript-comment">/*
				 * Check if the mapping's attributes
				 * match the existing map entry.
				 */</span>
				<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(map) ||
				    entry-&gt;vme_start != tmp_start ||
				    entry-&gt;is_sub_map != is_submap ||
				    VME_OFFSET(entry) != tmp_offset ||
				    entry-&gt;needs_copy != needs_copy ||
				    entry-&gt;protection != cur_protection ||
				    entry-&gt;max_protection != max_protection ||
				    entry-&gt;inheritance != inheritance ||
				    entry-&gt;iokit_acct != iokit_acct ||
				    VME_ALIAS(entry) != alias) {
					<span class="enscript-comment">/* not the same mapping ! */</span>
					RETURN(KERN_NO_SPACE);
				}
				<span class="enscript-comment">/*
				 * Check if the same object is being mapped.
				 */</span>
				<span class="enscript-keyword">if</span> (is_submap) {
					<span class="enscript-keyword">if</span> (VME_SUBMAP(entry) !=
					    (vm_map_t) object) {
						<span class="enscript-comment">/* not the same submap */</span>
						RETURN(KERN_NO_SPACE);
					}
				} <span class="enscript-keyword">else</span> {
					<span class="enscript-keyword">if</span> (VME_OBJECT(entry) != object) {
						<span class="enscript-comment">/* not the same VM object... */</span>
						vm_object_t obj2;

						obj2 = VME_OBJECT(entry);
						<span class="enscript-keyword">if</span> ((obj2 == VM_OBJECT_NULL ||
						     obj2-&gt;internal) &amp;&amp;
						    (object == VM_OBJECT_NULL ||
						     object-&gt;internal)) {
							<span class="enscript-comment">/*
							 * ... but both are
							 * anonymous memory,
							 * so equivalent.
							 */</span>
						} <span class="enscript-keyword">else</span> {
							RETURN(KERN_NO_SPACE);
						}
					}
				}

				tmp_offset += entry-&gt;vme_end - entry-&gt;vme_start;
				tmp_start += entry-&gt;vme_end - entry-&gt;vme_start;
				<span class="enscript-keyword">if</span> (entry-&gt;vme_end &gt;= end) {
					<span class="enscript-comment">/* reached the end of our mapping */</span>
					<span class="enscript-keyword">break</span>;
				}
			}
			<span class="enscript-comment">/* it all matches:  let's use what's already there ! */</span>
			RETURN(KERN_MEMORY_PRESENT);
		}

		<span class="enscript-comment">/*
		 *	...	the next region doesn't overlap the
		 *		end point.
		 */</span>

		<span class="enscript-keyword">if</span> ((entry-&gt;vme_next != vm_map_to_entry(map)) &amp;&amp;
		    (entry-&gt;vme_next-&gt;vme_start &lt; end))
			RETURN(KERN_NO_SPACE);
	}

	<span class="enscript-comment">/*
	 *	At this point,
	 *		&quot;start&quot; and &quot;end&quot; should define the endpoints of the
	 *			available new range, and
	 *		&quot;entry&quot; should refer to the region before the new
	 *			range, and
	 *
	 *		the map should be locked.
	 */</span>

	<span class="enscript-comment">/*
	 *	See whether we can avoid creating a new entry (and object) by
	 *	extending one of our neighbors.  [So far, we only attempt to
	 *	extend from below.]  Note that we can never extend/join
	 *	purgable objects because they need to remain distinct
	 *	entities in order to implement their &quot;volatile object&quot;
	 *	semantics.
	 */</span>

	<span class="enscript-keyword">if</span> (purgable || entry_for_jit) {
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {

			object = vm_object_allocate(size);
			object-&gt;copy_strategy = MEMORY_OBJECT_COPY_NONE;
			object-&gt;true_share = TRUE;
			<span class="enscript-keyword">if</span> (purgable) {
				task_t owner;
				object-&gt;purgable = VM_PURGABLE_NONVOLATILE;
				<span class="enscript-keyword">if</span> (map-&gt;pmap == kernel_pmap) {
					<span class="enscript-comment">/*
					 * Purgeable mappings made in a kernel
					 * map are &quot;owned&quot; by the kernel itself
					 * rather than the current user task
					 * because they're likely to be used by
					 * more than this user task (see
					 * execargs_purgeable_allocate(), for
					 * example).
					 */</span>
					owner = kernel_task;
				} <span class="enscript-keyword">else</span> {
					owner = current_task();
				}
				assert(object-&gt;vo_purgeable_owner == NULL);
				assert(object-&gt;resident_page_count == 0);
				assert(object-&gt;wired_page_count == 0);
				vm_object_lock(object);
				vm_purgeable_nonvolatile_enqueue(object, owner);
				vm_object_unlock(object);
			}
			offset = (vm_object_offset_t)0;
		}
	} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> ((is_submap == FALSE) &amp;&amp;
		   (object == VM_OBJECT_NULL) &amp;&amp;
		   (entry != vm_map_to_entry(map)) &amp;&amp;
		   (entry-&gt;vme_end == start) &amp;&amp;
		   (!entry-&gt;is_shared) &amp;&amp;
		   (!entry-&gt;is_sub_map) &amp;&amp;
		   (!entry-&gt;in_transition) &amp;&amp;
		   (!entry-&gt;needs_wakeup) &amp;&amp;
		   (entry-&gt;behavior == VM_BEHAVIOR_DEFAULT) &amp;&amp;
		   (entry-&gt;protection == cur_protection) &amp;&amp;
		   (entry-&gt;max_protection == max_protection) &amp;&amp;
		   (entry-&gt;inheritance == inheritance) &amp;&amp;
		   ((user_alias == VM_MEMORY_REALLOC) ||
		    (VME_ALIAS(entry) == alias)) &amp;&amp;
		   (entry-&gt;no_cache == no_cache) &amp;&amp;
		   (entry-&gt;permanent == permanent) &amp;&amp;
		   (!entry-&gt;superpage_size &amp;&amp; !superpage_size) &amp;&amp;
		   <span class="enscript-comment">/*
		    * No coalescing if not map-aligned, to avoid propagating
		    * that condition any further than needed:
		    */</span>
		   (!entry-&gt;map_aligned || !clear_map_aligned) &amp;&amp;
		   (!entry-&gt;zero_wired_pages) &amp;&amp;
		   (!entry-&gt;used_for_jit &amp;&amp; !entry_for_jit) &amp;&amp;
		   (entry-&gt;iokit_acct == iokit_acct) &amp;&amp;
		   (!entry-&gt;vme_resilient_codesign) &amp;&amp;
		   (!entry-&gt;vme_resilient_media) &amp;&amp;

		   ((entry-&gt;vme_end - entry-&gt;vme_start) + size &lt;=
		    (user_alias == VM_MEMORY_REALLOC ?
		     ANON_CHUNK_SIZE :
		     NO_COALESCE_LIMIT)) &amp;&amp;

		   (entry-&gt;wired_count == 0)) { <span class="enscript-comment">/* implies user_wired_count == 0 */</span>
		<span class="enscript-keyword">if</span> (vm_object_coalesce(VME_OBJECT(entry),
				       VM_OBJECT_NULL,
				       VME_OFFSET(entry),
				       (vm_object_offset_t) 0,
				       (vm_map_size_t)(entry-&gt;vme_end - entry-&gt;vme_start),
				       (vm_map_size_t)(end - entry-&gt;vme_end))) {

			<span class="enscript-comment">/*
			 *	Coalesced the two objects - can extend
			 *	the previous map entry to include the
			 *	new range.
			 */</span>
			map-&gt;size += (end - entry-&gt;vme_end);
			assert(entry-&gt;vme_start &lt; end);
			assert(VM_MAP_PAGE_ALIGNED(end,
						   VM_MAP_PAGE_MASK(map)));
			<span class="enscript-keyword">if</span> (__improbable(vm_debug_events))
				DTRACE_VM5(map_entry_extend, vm_map_t, map, vm_map_entry_t, entry, vm_address_t, entry-&gt;vme_start, vm_address_t, entry-&gt;vme_end, vm_address_t, end);
			entry-&gt;vme_end = end;
			<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
				vm_map_store_update_first_free(map, entry, TRUE);
			} <span class="enscript-keyword">else</span> {
				vm_map_store_update_first_free(map, map-&gt;first_free, TRUE);
			}
			new_mapping_established = TRUE;
			RETURN(KERN_SUCCESS);
		}
	}

	step = superpage_size ? SUPERPAGE_SIZE : (end - start);
	new_entry = NULL;

	<span class="enscript-keyword">for</span> (tmp2_start = start; tmp2_start&lt;end; tmp2_start += step) {
		tmp2_end = tmp2_start + step;
		<span class="enscript-comment">/*
		 *	Create a new entry
		 *	LP64todo - for now, we can only allocate 4GB internal objects
		 *	because the default pager can't page bigger ones.  Remove this
		 *	when it can.
		 *
		 * XXX FBDP
		 * The reserved &quot;page zero&quot; in each process's address space can
		 * be arbitrarily large.  Splitting it into separate 4GB objects and
		 * therefore different VM map entries serves no purpose and just
		 * slows down operations on the VM map, so let's not split the
		 * allocation into 4GB chunks if the max protection is NONE.  That
		 * memory should never be accessible, so it will never get to the
		 * default pager.
		 */</span>
		tmp_start = tmp2_start;
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL &amp;&amp;
		    size &gt; (vm_map_size_t)ANON_CHUNK_SIZE &amp;&amp;
		    max_protection != VM_PROT_NONE &amp;&amp;
		    superpage_size == 0) 
			tmp_end = tmp_start + (vm_map_size_t)ANON_CHUNK_SIZE;
		<span class="enscript-keyword">else</span>
			tmp_end = tmp2_end;
		<span class="enscript-keyword">do</span> {
			new_entry = vm_map_entry_insert(map, entry, tmp_start, tmp_end,
							object,	offset, needs_copy,
							FALSE, FALSE,
							cur_protection, max_protection,
							VM_BEHAVIOR_DEFAULT,
							(entry_for_jit)? VM_INHERIT_NONE: inheritance, 
							0, no_cache,
							permanent,
							superpage_size,
							clear_map_aligned,
							is_submap);

			assert((object != kernel_object) || (VM_KERN_MEMORY_NONE != alias));
			VME_ALIAS_SET(new_entry, alias);

			<span class="enscript-keyword">if</span> (entry_for_jit){
				<span class="enscript-keyword">if</span> (!(map-&gt;jit_entry_exists)){
					new_entry-&gt;used_for_jit = TRUE;
					map-&gt;jit_entry_exists = TRUE;
				}
			}

			<span class="enscript-keyword">if</span> (resilient_codesign &amp;&amp;
			    ! ((cur_protection | max_protection) &amp;
			       (VM_PROT_WRITE | VM_PROT_EXECUTE))) {
				new_entry-&gt;vme_resilient_codesign = TRUE;
			}

			<span class="enscript-keyword">if</span> (resilient_media &amp;&amp;
			    ! ((cur_protection | max_protection) &amp;
			       (VM_PROT_WRITE | VM_PROT_EXECUTE))) {
				new_entry-&gt;vme_resilient_media = TRUE;
			}

			assert(!new_entry-&gt;iokit_acct);
			<span class="enscript-keyword">if</span> (!is_submap &amp;&amp;
			    object != VM_OBJECT_NULL &amp;&amp;
			    object-&gt;purgable != VM_PURGABLE_DENY) {
				assert(new_entry-&gt;use_pmap);
				assert(!new_entry-&gt;iokit_acct);
				<span class="enscript-comment">/*
				 * Turn off pmap accounting since
				 * purgeable objects have their
				 * own ledgers.
				 */</span>
				new_entry-&gt;use_pmap = FALSE;
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (!is_submap &amp;&amp;
				   iokit_acct) {
				<span class="enscript-comment">/* alternate accounting */</span>
				assert(!new_entry-&gt;iokit_acct);
				assert(new_entry-&gt;use_pmap);
				new_entry-&gt;iokit_acct = TRUE;
				new_entry-&gt;use_pmap = FALSE;
				vm_map_iokit_mapped_region(
					map,
					(new_entry-&gt;vme_end -
					 new_entry-&gt;vme_start));
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (!is_submap) {
				assert(!new_entry-&gt;iokit_acct);
				assert(new_entry-&gt;use_pmap);
			}

			<span class="enscript-keyword">if</span> (is_submap) {
				vm_map_t	submap;
				boolean_t	submap_is_64bit;
				boolean_t	use_pmap;

				assert(new_entry-&gt;is_sub_map);
				assert(!new_entry-&gt;use_pmap);
				assert(!new_entry-&gt;iokit_acct);
				submap = (vm_map_t) object;
				submap_is_64bit = vm_map_is_64bit(submap);
				use_pmap = (user_alias == VM_MEMORY_SHARED_PMAP);
#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
				<span class="enscript-keyword">if</span> (use_pmap &amp;&amp; submap-&gt;pmap == NULL) {
					ledger_t ledger = map-&gt;pmap-&gt;ledger;
					<span class="enscript-comment">/* we need a sub pmap to nest... */</span>
					submap-&gt;pmap = pmap_create(ledger, 0,
					    submap_is_64bit);
					<span class="enscript-keyword">if</span> (submap-&gt;pmap == NULL) {
						<span class="enscript-comment">/* let's proceed without nesting... */</span>
					}
				}
				<span class="enscript-keyword">if</span> (use_pmap &amp;&amp; submap-&gt;pmap != NULL) {
					kr = pmap_nest(map-&gt;pmap,
						       submap-&gt;pmap,
						       tmp_start,
						       tmp_start,
						       tmp_end - tmp_start);
					<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
						printf(<span class="enscript-string">&quot;vm_map_enter: &quot;</span>
						       <span class="enscript-string">&quot;pmap_nest(0x%llx,0x%llx) &quot;</span>
						       <span class="enscript-string">&quot;error 0x%x\n&quot;</span>,
						       (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)tmp_start,
						       (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)tmp_end,
						       kr);
					} <span class="enscript-keyword">else</span> {
						<span class="enscript-comment">/* we're now nested ! */</span>
						new_entry-&gt;use_pmap = TRUE;
						pmap_empty = FALSE;
					}
				}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* NO_NESTED_PMAP */</span>
			}
			entry = new_entry;

			<span class="enscript-keyword">if</span> (superpage_size) {
				vm_page_t pages, m;
				vm_object_t sp_object;

				VME_OFFSET_SET(entry, 0);

				<span class="enscript-comment">/* allocate one superpage */</span>
				kr = cpm_allocate(SUPERPAGE_SIZE, &amp;pages, 0, SUPERPAGE_NBASEPAGES-1, TRUE, 0);
				<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
					<span class="enscript-comment">/* deallocate whole range... */</span>
					new_mapping_established = TRUE;
					<span class="enscript-comment">/* ... but only up to &quot;tmp_end&quot; */</span>
					size -= end - tmp_end;
					RETURN(kr);
				}

				<span class="enscript-comment">/* create one vm_object per superpage */</span>
				sp_object = vm_object_allocate((vm_map_size_t)(entry-&gt;vme_end - entry-&gt;vme_start));
				sp_object-&gt;phys_contiguous = TRUE;
				sp_object-&gt;vo_shadow_offset = (vm_object_offset_t)pages-&gt;phys_page*PAGE_SIZE;
				VME_OBJECT_SET(entry, sp_object);
				assert(entry-&gt;use_pmap);

				<span class="enscript-comment">/* enter the base pages into the object */</span>
				vm_object_lock(sp_object);
				<span class="enscript-keyword">for</span> (offset = 0; offset &lt; SUPERPAGE_SIZE; offset += PAGE_SIZE) {
					m = pages;
					pmap_zero_page(m-&gt;phys_page);
					pages = NEXT_PAGE(m);
					*(NEXT_PAGE_PTR(m)) = VM_PAGE_NULL;
					vm_page_insert_wired(m, sp_object, offset, VM_KERN_MEMORY_OSFMK);
				}
				vm_object_unlock(sp_object);
			}
		} <span class="enscript-keyword">while</span> (tmp_end != tmp2_end &amp;&amp; 
			 (tmp_start = tmp_end) &amp;&amp;
			 (tmp_end = (tmp2_end - tmp_end &gt; (vm_map_size_t)ANON_CHUNK_SIZE) ? 
			  tmp_end + (vm_map_size_t)ANON_CHUNK_SIZE : tmp2_end));
	}

	new_mapping_established = TRUE;

<span class="enscript-reference">BailOut</span>:
	assert(map_locked == TRUE);

	<span class="enscript-keyword">if</span> (result == KERN_SUCCESS) {
		vm_prot_t pager_prot;
		memory_object_t pager;

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">DEBUG</span>
		<span class="enscript-keyword">if</span> (pmap_empty &amp;&amp;
		    !(flags &amp; VM_FLAGS_NO_PMAP_CHECK)) {
			assert(vm_map_pmap_is_empty(map,
						    *address,
						    *address+size));
		}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* DEBUG */</span>

		<span class="enscript-comment">/*
		 * For &quot;named&quot; VM objects, let the pager know that the
		 * memory object is being mapped.  Some pagers need to keep
		 * track of this, to know when they can reclaim the memory
		 * object, for example.
		 * VM calls memory_object_map() for each mapping (specifying
		 * the protection of each mapping) and calls
		 * memory_object_last_unmap() when all the mappings are gone.
		 */</span>
		pager_prot = max_protection;
		<span class="enscript-keyword">if</span> (needs_copy) {
			<span class="enscript-comment">/*
			 * Copy-On-Write mapping: won't modify
			 * the memory object.
			 */</span>
			pager_prot &amp;= ~VM_PROT_WRITE;
		}
		<span class="enscript-keyword">if</span> (!is_submap &amp;&amp;
		    object != VM_OBJECT_NULL &amp;&amp;
		    object-&gt;named &amp;&amp;
		    object-&gt;pager != MEMORY_OBJECT_NULL) {
			vm_object_lock(object);
			pager = object-&gt;pager;
			<span class="enscript-keyword">if</span> (object-&gt;named &amp;&amp;
			    pager != MEMORY_OBJECT_NULL) {
				assert(object-&gt;pager_ready);
				vm_object_mapping_wait(object, THREAD_UNINT);
				vm_object_mapping_begin(object);
				vm_object_unlock(object);

				kr = memory_object_map(pager, pager_prot);
				assert(kr == KERN_SUCCESS);

				vm_object_lock(object);
				vm_object_mapping_end(object);
			}
			vm_object_unlock(object);
		}
	}

	assert(map_locked == TRUE);

	<span class="enscript-keyword">if</span> (!keep_map_locked) {
		vm_map_unlock(map);
		map_locked = FALSE;
	}

	<span class="enscript-comment">/*
	 * We can't hold the map lock if we enter this block.
	 */</span>

	<span class="enscript-keyword">if</span> (result == KERN_SUCCESS) {

		<span class="enscript-comment">/*	Wire down the new entry if the user
		 *	requested all new map entries be wired.
		 */</span>
		<span class="enscript-keyword">if</span> ((map-&gt;wiring_required)||(superpage_size)) {
			assert(!keep_map_locked);
			pmap_empty = FALSE; <span class="enscript-comment">/* pmap won't be empty */</span>
			kr = vm_map_wire(map, start, end,
					     new_entry-&gt;protection | VM_PROT_MEMORY_TAG_MAKE(VM_KERN_MEMORY_MLOCK),
					     TRUE);
			result = kr;
		}

	}

	<span class="enscript-keyword">if</span> (result != KERN_SUCCESS) {
		<span class="enscript-keyword">if</span> (new_mapping_established) {
			<span class="enscript-comment">/*
			 * We have to get rid of the new mappings since we
			 * won't make them available to the user.
			 * Try and do that atomically, to minimize the risk
			 * that someone else create new mappings that range.
			 */</span>
			zap_new_map = vm_map_create(PMAP_NULL,
						    *address,
						    *address + size,
						    map-&gt;hdr.entries_pageable);
			vm_map_set_page_shift(zap_new_map,
					      VM_MAP_PAGE_SHIFT(map));
			vm_map_disable_hole_optimization(zap_new_map);

			<span class="enscript-keyword">if</span> (!map_locked) {
				vm_map_lock(map);
				map_locked = TRUE;
			}
			(<span class="enscript-type">void</span>) vm_map_delete(map, *address, *address+size,
					     (VM_MAP_REMOVE_SAVE_ENTRIES |
					      VM_MAP_REMOVE_NO_MAP_ALIGN),
					     zap_new_map);
		}
		<span class="enscript-keyword">if</span> (zap_old_map != VM_MAP_NULL &amp;&amp;
		    zap_old_map-&gt;hdr.nentries != 0) {
			vm_map_entry_t	entry1, entry2;

			<span class="enscript-comment">/*
			 * The new mapping failed.  Attempt to restore
			 * the old mappings, saved in the &quot;zap_old_map&quot;.
			 */</span>
			<span class="enscript-keyword">if</span> (!map_locked) {
				vm_map_lock(map);
				map_locked = TRUE;
			}

			<span class="enscript-comment">/* first check if the coast is still clear */</span>
			start = vm_map_first_entry(zap_old_map)-&gt;vme_start;
			end = vm_map_last_entry(zap_old_map)-&gt;vme_end;
			<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;entry1) ||
			    vm_map_lookup_entry(map, end, &amp;entry2) ||
			    entry1 != entry2) {
				<span class="enscript-comment">/*
				 * Part of that range has already been
				 * re-mapped:  we can't restore the old
				 * mappings...
				 */</span>
				vm_map_enter_restore_failures++;
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/*
				 * Transfer the saved map entries from
				 * &quot;zap_old_map&quot; to the original &quot;map&quot;,
				 * inserting them all after &quot;entry1&quot;.
				 */</span>
				<span class="enscript-keyword">for</span> (entry2 = vm_map_first_entry(zap_old_map);
				     entry2 != vm_map_to_entry(zap_old_map);
				     entry2 = vm_map_first_entry(zap_old_map)) {
					vm_map_size_t entry_size;

					entry_size = (entry2-&gt;vme_end -
						      entry2-&gt;vme_start);
					vm_map_store_entry_unlink(zap_old_map,
							    entry2);
					zap_old_map-&gt;size -= entry_size;
					vm_map_store_entry_link(map, entry1, entry2);
					map-&gt;size += entry_size;
					entry1 = entry2;
				}
				<span class="enscript-keyword">if</span> (map-&gt;wiring_required) {
					<span class="enscript-comment">/*
					 * XXX TODO: we should rewire the
					 * old pages here...
					 */</span>
				}
				vm_map_enter_restore_successes++;
			}
		}
	}

	<span class="enscript-comment">/*
	 * The caller is responsible for releasing the lock if it requested to
	 * keep the map locked.
	 */</span>
	<span class="enscript-keyword">if</span> (map_locked &amp;&amp; !keep_map_locked) {
		vm_map_unlock(map);
	}

	<span class="enscript-comment">/*
	 * Get rid of the &quot;zap_maps&quot; and all the map entries that
	 * they may still contain.
	 */</span>
	<span class="enscript-keyword">if</span> (zap_old_map != VM_MAP_NULL) {
		vm_map_destroy(zap_old_map, VM_MAP_REMOVE_NO_PMAP_CLEANUP);
		zap_old_map = VM_MAP_NULL;
	}
	<span class="enscript-keyword">if</span> (zap_new_map != VM_MAP_NULL) {
		vm_map_destroy(zap_new_map, VM_MAP_REMOVE_NO_PMAP_CLEANUP);
		zap_new_map = VM_MAP_NULL;
	}

	<span class="enscript-keyword">return</span> result;

#<span class="enscript-reference">undef</span>	<span class="enscript-variable-name">RETURN</span>
}


<span class="enscript-comment">/*
 * Counters for the prefault optimization.
 */</span>
int64_t vm_prefault_nb_pages = 0;
int64_t vm_prefault_nb_bailout = 0;

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_enter_mem_object_helper</span>(
	vm_map_t		target_map,
	vm_map_offset_t		*address,
	vm_map_size_t		initial_size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	ipc_port_t		port,
	vm_object_offset_t	offset,
	boolean_t		copy,
	vm_prot_t		cur_protection,
	vm_prot_t		max_protection,
	vm_inherit_t		inheritance,
	upl_page_list_ptr_t	page_list,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		page_list_count)
{
	vm_map_address_t	map_addr;
	vm_map_size_t		map_size;
	vm_object_t		object;
	vm_object_size_t	size;
	kern_return_t		result;
	boolean_t		mask_cur_protection, mask_max_protection;
	boolean_t		try_prefault = (page_list_count != 0);
	vm_map_offset_t		offset_in_mapping = 0;

	mask_cur_protection = cur_protection &amp; VM_PROT_IS_MASK;
	mask_max_protection = max_protection &amp; VM_PROT_IS_MASK;
	cur_protection &amp;= ~VM_PROT_IS_MASK;
	max_protection &amp;= ~VM_PROT_IS_MASK;

	<span class="enscript-comment">/*
	 * Check arguments for validity
	 */</span>
	<span class="enscript-keyword">if</span> ((target_map == VM_MAP_NULL) ||
	    (cur_protection &amp; ~VM_PROT_ALL) ||
	    (max_protection &amp; ~VM_PROT_ALL) ||
	    (inheritance &gt; VM_INHERIT_LAST_VALID) ||
	    (try_prefault &amp;&amp; (copy || !page_list)) ||
	    initial_size == 0) {
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}
	
	{
		map_addr = vm_map_trunc_page(*address,
					     VM_MAP_PAGE_MASK(target_map));
		map_size = vm_map_round_page(initial_size,
					     VM_MAP_PAGE_MASK(target_map));
	}
	size = vm_object_round_page(initial_size);

	<span class="enscript-comment">/*
	 * Find the vm object (if any) corresponding to this port.
	 */</span>
	<span class="enscript-keyword">if</span> (!IP_VALID(port)) {
		object = VM_OBJECT_NULL;
		offset = 0;
		copy = FALSE;
	} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (ip_kotype(port) == IKOT_NAMED_ENTRY) {
		vm_named_entry_t	named_entry;

		named_entry = (vm_named_entry_t) port-&gt;ip_kobject;

		<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR |
			     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
			offset += named_entry-&gt;data_offset;
		}
		
		<span class="enscript-comment">/* a few checks to make sure user is obeying rules */</span>
		<span class="enscript-keyword">if</span> (size == 0) {
			<span class="enscript-keyword">if</span> (offset &gt;= named_entry-&gt;size)
				<span class="enscript-keyword">return</span> KERN_INVALID_RIGHT;
			size = named_entry-&gt;size - offset;
		}
		<span class="enscript-keyword">if</span> (mask_max_protection) {
			max_protection &amp;= named_entry-&gt;protection;
		}
		<span class="enscript-keyword">if</span> (mask_cur_protection) {
			cur_protection &amp;= named_entry-&gt;protection;
		}
		<span class="enscript-keyword">if</span> ((named_entry-&gt;protection &amp; max_protection) !=
		    max_protection)
			<span class="enscript-keyword">return</span> KERN_INVALID_RIGHT;
		<span class="enscript-keyword">if</span> ((named_entry-&gt;protection &amp; cur_protection) !=
		    cur_protection)
			<span class="enscript-keyword">return</span> KERN_INVALID_RIGHT;
		<span class="enscript-keyword">if</span> (offset + size &lt; offset) {
			<span class="enscript-comment">/* overflow */</span>
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}
		<span class="enscript-keyword">if</span> (named_entry-&gt;size &lt; (offset + initial_size)) {
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}

		<span class="enscript-keyword">if</span> (named_entry-&gt;is_copy) {
			<span class="enscript-comment">/* for a vm_map_copy, we can only map it whole */</span>
			<span class="enscript-keyword">if</span> ((size != named_entry-&gt;size) &amp;&amp;
			    (vm_map_round_page(size,
					       VM_MAP_PAGE_MASK(target_map)) ==
			     named_entry-&gt;size)) {
				<span class="enscript-comment">/* XXX FBDP use the rounded size... */</span>
				size = vm_map_round_page(
					size,
					VM_MAP_PAGE_MASK(target_map));
			}
				
			<span class="enscript-keyword">if</span> (!(flags &amp; VM_FLAGS_ANYWHERE) &amp;&amp;
			    (offset != 0 ||
			     size != named_entry-&gt;size)) {
				<span class="enscript-comment">/*
				 * XXX for a mapping at a &quot;fixed&quot; address,
				 * we can't trim after mapping the whole
				 * memory entry, so reject a request for a
				 * partial mapping.
				 */</span>
				<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
			}
		}

		<span class="enscript-comment">/* the callers parameter offset is defined to be the */</span>
		<span class="enscript-comment">/* offset from beginning of named entry offset in object */</span>
		offset = offset + named_entry-&gt;offset;
		
		<span class="enscript-keyword">if</span> (! VM_MAP_PAGE_ALIGNED(size,
					  VM_MAP_PAGE_MASK(target_map))) {
			<span class="enscript-comment">/*
			 * Let's not map more than requested;
			 * vm_map_enter() will handle this &quot;not map-aligned&quot;
			 * case.
			 */</span>
			map_size = size;
		}

		named_entry_lock(named_entry);
		<span class="enscript-keyword">if</span> (named_entry-&gt;is_sub_map) {
			vm_map_t		submap;

			<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR |
				     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
				panic(<span class="enscript-string">&quot;VM_FLAGS_RETURN_DATA_ADDR not expected for submap.&quot;</span>);
			}

			submap = named_entry-&gt;backing.map;
			vm_map_lock(submap);
			vm_map_reference(submap);
			vm_map_unlock(submap);
			named_entry_unlock(named_entry);

			result = vm_map_enter(target_map,
					      &amp;map_addr,
					      map_size,
					      mask,
					      flags | VM_FLAGS_SUBMAP,
					      (vm_object_t) submap,
					      offset,
					      copy,
					      cur_protection,
					      max_protection,
					      inheritance);
			<span class="enscript-keyword">if</span> (result != KERN_SUCCESS) {
				vm_map_deallocate(submap);
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/*
				 * No need to lock &quot;submap&quot; just to check its
				 * &quot;mapped&quot; flag: that flag is never reset
				 * once it's been set and if we race, we'll
				 * just end up setting it twice, which is OK.
				 */</span>
				<span class="enscript-keyword">if</span> (submap-&gt;mapped_in_other_pmaps == FALSE &amp;&amp;
				    vm_map_pmap(submap) != PMAP_NULL &amp;&amp;
				    vm_map_pmap(submap) !=
				    vm_map_pmap(target_map)) {
					<span class="enscript-comment">/*
					 * This submap is being mapped in a map
					 * that uses a different pmap.
					 * Set its &quot;mapped_in_other_pmaps&quot; flag
					 * to indicate that we now need to 
					 * remove mappings from all pmaps rather
					 * than just the submap's pmap.
					 */</span>
					vm_map_lock(submap);
					submap-&gt;mapped_in_other_pmaps = TRUE;
					vm_map_unlock(submap);
				}
				*address = map_addr;
			}
			<span class="enscript-keyword">return</span> result;

		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (named_entry-&gt;is_pager) {
			<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	access;
			vm_prot_t	protections;
			<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	wimg_mode;

			protections = named_entry-&gt;protection &amp; VM_PROT_ALL;
			access = GET_MAP_MEM(named_entry-&gt;protection);

			<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR|
				     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
				panic(<span class="enscript-string">&quot;VM_FLAGS_RETURN_DATA_ADDR not expected for submap.&quot;</span>);
			}

			object = vm_object_enter(named_entry-&gt;backing.pager, 
						 named_entry-&gt;size, 
						 named_entry-&gt;internal, 
						 FALSE,
						 FALSE);
			<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
				named_entry_unlock(named_entry);
				<span class="enscript-keyword">return</span> KERN_INVALID_OBJECT;
			}

			<span class="enscript-comment">/* JMM - drop reference on pager here */</span>

			<span class="enscript-comment">/* create an extra ref for the named entry */</span>
			vm_object_lock(object);
			vm_object_reference_locked(object);
			named_entry-&gt;backing.object = object;
			named_entry-&gt;is_pager = FALSE;
			named_entry_unlock(named_entry);

			wimg_mode = object-&gt;wimg_bits;

			<span class="enscript-keyword">if</span> (access == MAP_MEM_IO) {
				wimg_mode = VM_WIMG_IO;
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (access == MAP_MEM_COPYBACK) {
				wimg_mode = VM_WIMG_USE_DEFAULT;
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (access == MAP_MEM_INNERWBACK) {
				wimg_mode = VM_WIMG_INNERWBACK;
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (access == MAP_MEM_WTHRU) {
				wimg_mode = VM_WIMG_WTHRU;
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (access == MAP_MEM_WCOMB) {
				wimg_mode = VM_WIMG_WCOMB;
			}

			<span class="enscript-comment">/* wait for object (if any) to be ready */</span>
			<span class="enscript-keyword">if</span> (!named_entry-&gt;internal) {
				<span class="enscript-keyword">while</span> (!object-&gt;pager_ready) {
					vm_object_wait(
						object,
						VM_OBJECT_EVENT_PAGER_READY,
						THREAD_UNINT);
					vm_object_lock(object);
				}
			}

			<span class="enscript-keyword">if</span> (object-&gt;wimg_bits != wimg_mode)
				vm_object_change_wimg_mode(object, wimg_mode);

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">VM_OBJECT_TRACKING_OP_TRUESHARE</span>
			<span class="enscript-keyword">if</span> (!object-&gt;true_share &amp;&amp;
			    vm_object_tracking_inited) {
				<span class="enscript-type">void</span> *bt[VM_OBJECT_TRACKING_BTDEPTH];
				<span class="enscript-type">int</span> num = 0;

				num = OSBacktrace(bt,
						  VM_OBJECT_TRACKING_BTDEPTH);
				btlog_add_entry(vm_object_tracking_btlog,
						object,
						VM_OBJECT_TRACKING_OP_TRUESHARE,
						bt,
						num);
			}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* VM_OBJECT_TRACKING_OP_TRUESHARE */</span>

			object-&gt;true_share = TRUE;

			<span class="enscript-keyword">if</span> (object-&gt;copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC)
				object-&gt;copy_strategy = MEMORY_OBJECT_COPY_DELAY;
			vm_object_unlock(object);

		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (named_entry-&gt;is_copy) {
			kern_return_t	kr;
			vm_map_copy_t	copy_map;
			vm_map_entry_t	copy_entry;
			vm_map_offset_t	copy_addr;

			<span class="enscript-keyword">if</span> (flags &amp; ~(VM_FLAGS_FIXED |
				      VM_FLAGS_ANYWHERE |
				      VM_FLAGS_OVERWRITE |
				      VM_FLAGS_RETURN_4K_DATA_ADDR |
				      VM_FLAGS_RETURN_DATA_ADDR)) {
				named_entry_unlock(named_entry);
				<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
			}

			<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR |
				     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
				offset_in_mapping = offset - vm_object_trunc_page(offset);
				<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_RETURN_4K_DATA_ADDR)
					offset_in_mapping &amp;= ~((<span class="enscript-type">signed</span>)(0xFFF));
				offset = vm_object_trunc_page(offset);
				map_size = vm_object_round_page(offset + offset_in_mapping + initial_size) - offset;
			}

			copy_map = named_entry-&gt;backing.copy;
			assert(copy_map-&gt;type == VM_MAP_COPY_ENTRY_LIST);
			<span class="enscript-keyword">if</span> (copy_map-&gt;type != VM_MAP_COPY_ENTRY_LIST) {
				<span class="enscript-comment">/* unsupported type; should not happen */</span>
				printf(<span class="enscript-string">&quot;vm_map_enter_mem_object: &quot;</span>
				       <span class="enscript-string">&quot;memory_entry-&gt;backing.copy &quot;</span>
				       <span class="enscript-string">&quot;unsupported type 0x%x\n&quot;</span>,
				       copy_map-&gt;type);
				named_entry_unlock(named_entry);
				<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
			}

			<span class="enscript-comment">/* reserve a contiguous range */</span>
			kr = vm_map_enter(target_map,
					  &amp;map_addr,
					  <span class="enscript-comment">/* map whole mem entry, trim later: */</span>
					  named_entry-&gt;size,
					  mask,
					  flags &amp; (VM_FLAGS_ANYWHERE |
						   VM_FLAGS_OVERWRITE |
						   VM_FLAGS_RETURN_4K_DATA_ADDR |
						   VM_FLAGS_RETURN_DATA_ADDR),
					  VM_OBJECT_NULL,
					  0,
					  FALSE, <span class="enscript-comment">/* copy */</span>
					  cur_protection,
					  max_protection,
					  inheritance);
			<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
				named_entry_unlock(named_entry);
				<span class="enscript-keyword">return</span> kr;
			}

			copy_addr = map_addr;

			<span class="enscript-keyword">for</span> (copy_entry = vm_map_copy_first_entry(copy_map);
			     copy_entry != vm_map_copy_to_entry(copy_map);
			     copy_entry = copy_entry-&gt;vme_next) {
				<span class="enscript-type">int</span>			remap_flags = 0;
				vm_map_t		copy_submap;
				vm_object_t		copy_object;
				vm_map_size_t		copy_size;
				vm_object_offset_t	copy_offset;

				copy_offset = VME_OFFSET(copy_entry);
				copy_size = (copy_entry-&gt;vme_end -
					     copy_entry-&gt;vme_start);

				<span class="enscript-comment">/* sanity check */</span>
				<span class="enscript-keyword">if</span> ((copy_addr + copy_size) &gt;
				    (map_addr +
				     named_entry-&gt;size <span class="enscript-comment">/* XXX full size */</span> )) {
					<span class="enscript-comment">/* over-mapping too much !? */</span>
					kr = KERN_INVALID_ARGUMENT;
					<span class="enscript-comment">/* abort */</span>
					<span class="enscript-keyword">break</span>;
				}

				<span class="enscript-comment">/* take a reference on the object */</span>
				<span class="enscript-keyword">if</span> (copy_entry-&gt;is_sub_map) {
					remap_flags |= VM_FLAGS_SUBMAP;
					copy_submap = VME_SUBMAP(copy_entry);
					vm_map_lock(copy_submap);
					vm_map_reference(copy_submap);
					vm_map_unlock(copy_submap);
					copy_object = (vm_object_t) copy_submap;
				} <span class="enscript-keyword">else</span> {
					copy_object = VME_OBJECT(copy_entry);
					vm_object_reference(copy_object);
				}

				<span class="enscript-comment">/* over-map the object into destination */</span>
				remap_flags |= flags;
				remap_flags |= VM_FLAGS_FIXED;
				remap_flags |= VM_FLAGS_OVERWRITE;
				remap_flags &amp;= ~VM_FLAGS_ANYWHERE;
				kr = vm_map_enter(target_map,
						  &amp;copy_addr,
						  copy_size,
						  (vm_map_offset_t) 0,
						  remap_flags,
						  copy_object,
						  copy_offset,
						  copy,
						  cur_protection,
						  max_protection,
						  inheritance);
				<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
					<span class="enscript-keyword">if</span> (copy_entry-&gt;is_sub_map) {
						vm_map_deallocate(copy_submap);
					} <span class="enscript-keyword">else</span> {
						vm_object_deallocate(copy_object);
					}
					<span class="enscript-comment">/* abort */</span>
					<span class="enscript-keyword">break</span>;
				}

				<span class="enscript-comment">/* next mapping */</span>
				copy_addr += copy_size;
			}
			
			<span class="enscript-keyword">if</span> (kr == KERN_SUCCESS) {
				<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR |
					     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
					*address = map_addr + offset_in_mapping;
				} <span class="enscript-keyword">else</span> {
					*address = map_addr;
				}

				<span class="enscript-keyword">if</span> (offset) {
					<span class="enscript-comment">/*
					 * Trim in front, from 0 to &quot;offset&quot;.
					 */</span>
					vm_map_remove(target_map,
						      map_addr,
						      map_addr + offset,
						      0);
					*address += offset;
				}
				<span class="enscript-keyword">if</span> (offset + map_size &lt; named_entry-&gt;size) {
					<span class="enscript-comment">/*
					 * Trim in back, from
					 * &quot;offset + map_size&quot; to
					 * &quot;named_entry-&gt;size&quot;.
					 */</span>
					vm_map_remove(target_map,
						      (map_addr +
						       offset + map_size),
						      (map_addr +
						       named_entry-&gt;size),
						      0);
				}
			}
			named_entry_unlock(named_entry);

			<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
				<span class="enscript-keyword">if</span> (! (flags &amp; VM_FLAGS_OVERWRITE)) {
					<span class="enscript-comment">/* deallocate the contiguous range */</span>
					(<span class="enscript-type">void</span>) vm_deallocate(target_map,
							     map_addr,
							     map_size);
				}
			}

			<span class="enscript-keyword">return</span> kr;
			
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/* This is the case where we are going to map */</span>
			<span class="enscript-comment">/* an already mapped object.  If the object is */</span>
			<span class="enscript-comment">/* not ready it is internal.  An external     */</span>
			<span class="enscript-comment">/* object cannot be mapped until it is ready  */</span>
			<span class="enscript-comment">/* we can therefore avoid the ready check     */</span>
			<span class="enscript-comment">/* in this case.  */</span>
			<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR |
				     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
				offset_in_mapping = offset - vm_object_trunc_page(offset);
				<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_RETURN_4K_DATA_ADDR)
					offset_in_mapping &amp;= ~((<span class="enscript-type">signed</span>)(0xFFF));
				offset = vm_object_trunc_page(offset);
				map_size = vm_object_round_page(offset + offset_in_mapping + initial_size) - offset;
			} 

			object = named_entry-&gt;backing.object;
			assert(object != VM_OBJECT_NULL);
			named_entry_unlock(named_entry);
			vm_object_reference(object);
		}
	} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (ip_kotype(port) == IKOT_MEMORY_OBJECT) {
		<span class="enscript-comment">/*
		 * JMM - This is temporary until we unify named entries
		 * and raw memory objects.
		 *
		 * Detected fake ip_kotype for a memory object.  In
		 * this case, the port isn't really a port at all, but
		 * instead is just a raw memory object.
		 */</span>
		<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR |
			     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
			panic(<span class="enscript-string">&quot;VM_FLAGS_RETURN_DATA_ADDR not expected for raw memory object.&quot;</span>);
		}

		object = vm_object_enter((memory_object_t)port,
					 size, FALSE, FALSE, FALSE);
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL)
			<span class="enscript-keyword">return</span> KERN_INVALID_OBJECT;

		<span class="enscript-comment">/* wait for object (if any) to be ready */</span>
		<span class="enscript-keyword">if</span> (object != VM_OBJECT_NULL) {
			<span class="enscript-keyword">if</span> (object == kernel_object) {
				printf(<span class="enscript-string">&quot;Warning: Attempt to map kernel object&quot;</span>
					<span class="enscript-string">&quot; by a non-private kernel entity\n&quot;</span>);
				<span class="enscript-keyword">return</span> KERN_INVALID_OBJECT;
			}
			<span class="enscript-keyword">if</span> (!object-&gt;pager_ready) {
				vm_object_lock(object);

				<span class="enscript-keyword">while</span> (!object-&gt;pager_ready) {
					vm_object_wait(object,
						       VM_OBJECT_EVENT_PAGER_READY,
						       THREAD_UNINT);
					vm_object_lock(object);
				}
				vm_object_unlock(object);
			}
		}
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-keyword">return</span> KERN_INVALID_OBJECT;
	}

	<span class="enscript-keyword">if</span> (object != VM_OBJECT_NULL &amp;&amp;
	    object-&gt;named &amp;&amp;
	    object-&gt;pager != MEMORY_OBJECT_NULL &amp;&amp;
	    object-&gt;copy_strategy != MEMORY_OBJECT_COPY_NONE) {
		memory_object_t pager;
		vm_prot_t	pager_prot;
		kern_return_t	kr;

		<span class="enscript-comment">/*
		 * For &quot;named&quot; VM objects, let the pager know that the
		 * memory object is being mapped.  Some pagers need to keep
		 * track of this, to know when they can reclaim the memory
		 * object, for example.
		 * VM calls memory_object_map() for each mapping (specifying
		 * the protection of each mapping) and calls
		 * memory_object_last_unmap() when all the mappings are gone.
		 */</span>
		pager_prot = max_protection;
		<span class="enscript-keyword">if</span> (copy) {
			<span class="enscript-comment">/*
			 * Copy-On-Write mapping: won't modify the
			 * memory object.
			 */</span>
			pager_prot &amp;= ~VM_PROT_WRITE;
		}
		vm_object_lock(object);
		pager = object-&gt;pager;
		<span class="enscript-keyword">if</span> (object-&gt;named &amp;&amp;
		    pager != MEMORY_OBJECT_NULL &amp;&amp;
		    object-&gt;copy_strategy != MEMORY_OBJECT_COPY_NONE) {
			assert(object-&gt;pager_ready);
			vm_object_mapping_wait(object, THREAD_UNINT);
			vm_object_mapping_begin(object);
			vm_object_unlock(object);

			kr = memory_object_map(pager, pager_prot);
			assert(kr == KERN_SUCCESS);

			vm_object_lock(object);
			vm_object_mapping_end(object);
		}
		vm_object_unlock(object);
	}

	<span class="enscript-comment">/*
	 *	Perform the copy if requested
	 */</span>

	<span class="enscript-keyword">if</span> (copy) {
		vm_object_t		new_object;
		vm_object_offset_t	new_offset;

		result = vm_object_copy_strategically(object, offset,
						      map_size,
						      &amp;new_object, &amp;new_offset,
						      &amp;copy);


		<span class="enscript-keyword">if</span> (result == KERN_MEMORY_RESTART_COPY) {
			boolean_t success;
			boolean_t src_needs_copy;

			<span class="enscript-comment">/*
			 * XXX
			 * We currently ignore src_needs_copy.
			 * This really is the issue of how to make
			 * MEMORY_OBJECT_COPY_SYMMETRIC safe for
			 * non-kernel users to use. Solution forthcoming.
			 * In the meantime, since we don't allow non-kernel
			 * memory managers to specify symmetric copy,
			 * we won't run into problems here.
			 */</span>
			new_object = object;
			new_offset = offset;
			success = vm_object_copy_quickly(&amp;new_object,
							 new_offset,
							 map_size,
							 &amp;src_needs_copy,
							 &amp;copy);
			assert(success);
			result = KERN_SUCCESS;
		}
		<span class="enscript-comment">/*
		 *	Throw away the reference to the
		 *	original object, as it won't be mapped.
		 */</span>

		vm_object_deallocate(object);

		<span class="enscript-keyword">if</span> (result != KERN_SUCCESS) {
			<span class="enscript-keyword">return</span> result;
		}

		object = new_object;
		offset = new_offset;
	}

	<span class="enscript-comment">/*
	 * If users want to try to prefault pages, the mapping and prefault
	 * needs to be atomic.
	 */</span>
	<span class="enscript-keyword">if</span> (try_prefault)
		flags |= VM_FLAGS_KEEP_MAP_LOCKED;

	{
		result = vm_map_enter(target_map,
				      &amp;map_addr, map_size,
				      (vm_map_offset_t)mask,
				      flags,
				      object, offset,
				      copy,
				      cur_protection, max_protection,
				      inheritance);
	}
	<span class="enscript-keyword">if</span> (result != KERN_SUCCESS)
		vm_object_deallocate(object);

	<span class="enscript-comment">/*
	 * Try to prefault, and do not forget to release the vm map lock.
	 */</span>
	<span class="enscript-keyword">if</span> (result == KERN_SUCCESS &amp;&amp; try_prefault) {
		mach_vm_address_t va = map_addr;
		kern_return_t kr = KERN_SUCCESS;
		<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> i = 0;

		<span class="enscript-keyword">for</span> (i = 0; i &lt; page_list_count; ++i) {
			<span class="enscript-keyword">if</span> (UPL_VALID_PAGE(page_list, i)) {
				<span class="enscript-comment">/*
				 * If this function call failed, we should stop
				 * trying to optimize, other calls are likely
				 * going to fail too.
				 *
				 * We are not gonna report an error for such
				 * failure though. That's an optimization, not
				 * something critical.
				 */</span>
				kr = pmap_enter_options(target_map-&gt;pmap,
				                        va, UPL_PHYS_PAGE(page_list, i),
				                        cur_protection, VM_PROT_NONE,
				                        0, TRUE, PMAP_OPTIONS_NOWAIT, NULL);
				<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
					OSIncrementAtomic64(&amp;vm_prefault_nb_bailout);
					<span class="enscript-keyword">break</span>;
				}
				OSIncrementAtomic64(&amp;vm_prefault_nb_pages);
			}

			<span class="enscript-comment">/* Next virtual address */</span>
			va += PAGE_SIZE;
		}
		vm_map_unlock(target_map);
	}

	<span class="enscript-keyword">if</span> (flags &amp; (VM_FLAGS_RETURN_DATA_ADDR |
		     VM_FLAGS_RETURN_4K_DATA_ADDR)) {
		*address = map_addr + offset_in_mapping;
	} <span class="enscript-keyword">else</span> {
		*address = map_addr;
	}
	<span class="enscript-keyword">return</span> result;
}

kern_return_t
<span class="enscript-function-name">vm_map_enter_mem_object</span>(
	vm_map_t		target_map,
	vm_map_offset_t		*address,
	vm_map_size_t		initial_size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	ipc_port_t		port,
	vm_object_offset_t	offset,
	boolean_t		copy,
	vm_prot_t		cur_protection,
	vm_prot_t		max_protection,
	vm_inherit_t		inheritance)
{
	<span class="enscript-keyword">return</span> vm_map_enter_mem_object_helper(target_map, address, initial_size, mask, flags,
	                                      port, offset, copy, cur_protection, max_protection,
	                                      inheritance, NULL, 0);
}

kern_return_t
<span class="enscript-function-name">vm_map_enter_mem_object_prefault</span>(
	vm_map_t		target_map,
	vm_map_offset_t		*address,
	vm_map_size_t		initial_size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	ipc_port_t		port,
	vm_object_offset_t	offset,
	vm_prot_t		cur_protection,
	vm_prot_t		max_protection,
	upl_page_list_ptr_t	page_list,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		page_list_count)
{
	<span class="enscript-keyword">return</span> vm_map_enter_mem_object_helper(target_map, address, initial_size, mask, flags,
	                                      port, offset, FALSE, cur_protection, max_protection,
	                                      VM_INHERIT_DEFAULT, page_list, page_list_count);
}


kern_return_t
<span class="enscript-function-name">vm_map_enter_mem_object_control</span>(
	vm_map_t		target_map,
	vm_map_offset_t		*address,
	vm_map_size_t		initial_size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	memory_object_control_t	control,
	vm_object_offset_t	offset,
	boolean_t		copy,
	vm_prot_t		cur_protection,
	vm_prot_t		max_protection,
	vm_inherit_t		inheritance)
{
	vm_map_address_t	map_addr;
	vm_map_size_t		map_size;
	vm_object_t		object;
	vm_object_size_t	size;
	kern_return_t		result;
	memory_object_t		pager;
	vm_prot_t		pager_prot;
	kern_return_t		kr;

	<span class="enscript-comment">/*
	 * Check arguments for validity
	 */</span>
	<span class="enscript-keyword">if</span> ((target_map == VM_MAP_NULL) ||
	    (cur_protection &amp; ~VM_PROT_ALL) ||
	    (max_protection &amp; ~VM_PROT_ALL) ||
	    (inheritance &gt; VM_INHERIT_LAST_VALID) ||
	    initial_size == 0) {
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}

	{
		map_addr = vm_map_trunc_page(*address,
					     VM_MAP_PAGE_MASK(target_map));
		map_size = vm_map_round_page(initial_size,
					     VM_MAP_PAGE_MASK(target_map));
	}
	size = vm_object_round_page(initial_size);

	object = memory_object_control_to_vm_object(control);

	<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL)
		<span class="enscript-keyword">return</span> KERN_INVALID_OBJECT;

	<span class="enscript-keyword">if</span> (object == kernel_object) {
		printf(<span class="enscript-string">&quot;Warning: Attempt to map kernel object&quot;</span>
		       <span class="enscript-string">&quot; by a non-private kernel entity\n&quot;</span>);
		<span class="enscript-keyword">return</span> KERN_INVALID_OBJECT;
	}

	vm_object_lock(object);
	object-&gt;ref_count++;
	vm_object_res_reference(object);

	<span class="enscript-comment">/*
	 * For &quot;named&quot; VM objects, let the pager know that the
	 * memory object is being mapped.  Some pagers need to keep
	 * track of this, to know when they can reclaim the memory
	 * object, for example.
	 * VM calls memory_object_map() for each mapping (specifying
	 * the protection of each mapping) and calls
	 * memory_object_last_unmap() when all the mappings are gone.
	 */</span>
	pager_prot = max_protection;
	<span class="enscript-keyword">if</span> (copy) {
		pager_prot &amp;= ~VM_PROT_WRITE;
	}
	pager = object-&gt;pager;
	<span class="enscript-keyword">if</span> (object-&gt;named &amp;&amp;
	    pager != MEMORY_OBJECT_NULL &amp;&amp;
	    object-&gt;copy_strategy != MEMORY_OBJECT_COPY_NONE) {
		assert(object-&gt;pager_ready);
		vm_object_mapping_wait(object, THREAD_UNINT);
		vm_object_mapping_begin(object);
		vm_object_unlock(object);

		kr = memory_object_map(pager, pager_prot);
		assert(kr == KERN_SUCCESS);

		vm_object_lock(object);
		vm_object_mapping_end(object);
	}
	vm_object_unlock(object);

	<span class="enscript-comment">/*
	 *	Perform the copy if requested
	 */</span>

	<span class="enscript-keyword">if</span> (copy) {
		vm_object_t		new_object;
		vm_object_offset_t	new_offset;

		result = vm_object_copy_strategically(object, offset, size,
						      &amp;new_object, &amp;new_offset,
						      &amp;copy);


		<span class="enscript-keyword">if</span> (result == KERN_MEMORY_RESTART_COPY) {
			boolean_t success;
			boolean_t src_needs_copy;

			<span class="enscript-comment">/*
			 * XXX
			 * We currently ignore src_needs_copy.
			 * This really is the issue of how to make
			 * MEMORY_OBJECT_COPY_SYMMETRIC safe for
			 * non-kernel users to use. Solution forthcoming.
			 * In the meantime, since we don't allow non-kernel
			 * memory managers to specify symmetric copy,
			 * we won't run into problems here.
			 */</span>
			new_object = object;
			new_offset = offset;
			success = vm_object_copy_quickly(&amp;new_object,
							 new_offset, size,
							 &amp;src_needs_copy,
							 &amp;copy);
			assert(success);
			result = KERN_SUCCESS;
		}
		<span class="enscript-comment">/*
		 *	Throw away the reference to the
		 *	original object, as it won't be mapped.
		 */</span>

		vm_object_deallocate(object);

		<span class="enscript-keyword">if</span> (result != KERN_SUCCESS) {
			<span class="enscript-keyword">return</span> result;
		}

		object = new_object;
		offset = new_offset;
	}

	{
		result = vm_map_enter(target_map,
				      &amp;map_addr, map_size,
				      (vm_map_offset_t)mask,
				      flags,
				      object, offset,
				      copy,
				      cur_protection, max_protection,
				      inheritance);
	}
	<span class="enscript-keyword">if</span> (result != KERN_SUCCESS)
		vm_object_deallocate(object);
	*address = map_addr;

	<span class="enscript-keyword">return</span> result;
}


#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">VM_CPM</span>

#<span class="enscript-reference">ifdef</span> <span class="enscript-variable-name">MACH_ASSERT</span>
<span class="enscript-type">extern</span> pmap_paddr_t	avail_start, avail_end;
#<span class="enscript-reference">endif</span>

<span class="enscript-comment">/*
 *	Allocate memory in the specified map, with the caveat that
 *	the memory is physically contiguous.  This call may fail
 *	if the system can't find sufficient contiguous memory.
 *	This call may cause or lead to heart-stopping amounts of
 *	paging activity.
 *
 *	Memory obtained from this call should be freed in the
 *	normal way, viz., via vm_deallocate.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_enter_cpm</span>(
	vm_map_t		map,
	vm_map_offset_t	*addr,
	vm_map_size_t		size,
	<span class="enscript-type">int</span>			flags)
{
	vm_object_t		cpm_obj;
	pmap_t			pmap;
	vm_page_t		m, pages;
	kern_return_t		kr;
	vm_map_offset_t		va, start, end, offset;
#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">MACH_ASSERT</span>
	vm_map_offset_t		prev_addr = 0;
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* MACH_ASSERT */</span>

	boolean_t		anywhere = ((VM_FLAGS_ANYWHERE &amp; flags) != 0);
	vm_tag_t tag;

	VM_GET_FLAGS_ALIAS(flags, tag);

	<span class="enscript-keyword">if</span> (size == 0) {
		*addr = 0;
		<span class="enscript-keyword">return</span> KERN_SUCCESS;
	}
	<span class="enscript-keyword">if</span> (anywhere)
		*addr = vm_map_min(map);
	<span class="enscript-keyword">else</span>
		*addr = vm_map_trunc_page(*addr,
					  VM_MAP_PAGE_MASK(map));
	size = vm_map_round_page(size,
				 VM_MAP_PAGE_MASK(map));

	<span class="enscript-comment">/*
	 * LP64todo - cpm_allocate should probably allow
	 * allocations of &gt;4GB, but not with the current
	 * algorithm, so just cast down the size for now.
	 */</span>
	<span class="enscript-keyword">if</span> (size &gt; VM_MAX_ADDRESS)
		<span class="enscript-keyword">return</span> KERN_RESOURCE_SHORTAGE;
	<span class="enscript-keyword">if</span> ((kr = cpm_allocate(CAST_DOWN(vm_size_t, size),
			       &amp;pages, 0, 0, TRUE, flags)) != KERN_SUCCESS)
		<span class="enscript-keyword">return</span> kr;

	cpm_obj = vm_object_allocate((vm_object_size_t)size);
	assert(cpm_obj != VM_OBJECT_NULL);
	assert(cpm_obj-&gt;internal);
	assert(cpm_obj-&gt;vo_size == (vm_object_size_t)size);
	assert(cpm_obj-&gt;can_persist == FALSE);
	assert(cpm_obj-&gt;pager_created == FALSE);
	assert(cpm_obj-&gt;pageout == FALSE);
	assert(cpm_obj-&gt;shadow == VM_OBJECT_NULL);

	<span class="enscript-comment">/*
	 *	Insert pages into object.
	 */</span>

	vm_object_lock(cpm_obj);
	<span class="enscript-keyword">for</span> (offset = 0; offset &lt; size; offset += PAGE_SIZE) {
		m = pages;
		pages = NEXT_PAGE(m);
		*(NEXT_PAGE_PTR(m)) = VM_PAGE_NULL;

		assert(!m-&gt;gobbled);
		assert(!m-&gt;wanted);
		assert(!m-&gt;pageout);
		assert(!m-&gt;tabled);
		assert(VM_PAGE_WIRED(m));
		<span class="enscript-comment">/*
		 * ENCRYPTED SWAP:
		 * &quot;m&quot; is not supposed to be pageable, so it
		 * should not be encrypted.  It wouldn't be safe
		 * to enter it in a new VM object while encrypted.
		 */</span>
		ASSERT_PAGE_DECRYPTED(m);
		assert(m-&gt;busy);
		assert(m-&gt;phys_page&gt;=(avail_start&gt;&gt;PAGE_SHIFT) &amp;&amp; m-&gt;phys_page&lt;=(avail_end&gt;&gt;PAGE_SHIFT));

		m-&gt;busy = FALSE;
		vm_page_insert(m, cpm_obj, offset);
	}
	assert(cpm_obj-&gt;resident_page_count == size / PAGE_SIZE);
	vm_object_unlock(cpm_obj);

	<span class="enscript-comment">/*
	 *	Hang onto a reference on the object in case a
	 *	multi-threaded application for some reason decides
	 *	to deallocate the portion of the address space into
	 *	which we will insert this object.
	 *
	 *	Unfortunately, we must insert the object now before
	 *	we can talk to the pmap module about which addresses
	 *	must be wired down.  Hence, the race with a multi-
	 *	threaded app.
	 */</span>
	vm_object_reference(cpm_obj);

	<span class="enscript-comment">/*
	 *	Insert object into map.
	 */</span>

	kr = vm_map_enter(
		map,
		addr,
		size,
		(vm_map_offset_t)0,
		flags,
		cpm_obj,
		(vm_object_offset_t)0,
		FALSE,
		VM_PROT_ALL,
		VM_PROT_ALL,
		VM_INHERIT_DEFAULT);

	<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
		<span class="enscript-comment">/*
		 *	A CPM object doesn't have can_persist set,
		 *	so all we have to do is deallocate it to
		 *	free up these pages.
		 */</span>
		assert(cpm_obj-&gt;pager_created == FALSE);
		assert(cpm_obj-&gt;can_persist == FALSE);
		assert(cpm_obj-&gt;pageout == FALSE);
		assert(cpm_obj-&gt;shadow == VM_OBJECT_NULL);
		vm_object_deallocate(cpm_obj); <span class="enscript-comment">/* kill acquired ref */</span>
		vm_object_deallocate(cpm_obj); <span class="enscript-comment">/* kill creation ref */</span>
	}

	<span class="enscript-comment">/*
	 *	Inform the physical mapping system that the
	 *	range of addresses may not fault, so that
	 *	page tables and such can be locked down as well.
	 */</span>
	start = *addr;
	end = start + size;
	pmap = vm_map_pmap(map);
	pmap_pageable(pmap, start, end, FALSE);

	<span class="enscript-comment">/*
	 *	Enter each page into the pmap, to avoid faults.
	 *	Note that this loop could be coded more efficiently,
	 *	if the need arose, rather than looking up each page
	 *	again.
	 */</span>
	<span class="enscript-keyword">for</span> (offset = 0, va = start; offset &lt; size;
	     va += PAGE_SIZE, offset += PAGE_SIZE) {
	        <span class="enscript-type">int</span> type_of_fault;

		vm_object_lock(cpm_obj);
		m = vm_page_lookup(cpm_obj, (vm_object_offset_t)offset);
		assert(m != VM_PAGE_NULL);

		vm_page_zero_fill(m);

		type_of_fault = DBG_ZERO_FILL_FAULT;

		vm_fault_enter(m, pmap, va, VM_PROT_ALL, VM_PROT_WRITE,
			       VM_PAGE_WIRED(m), FALSE, FALSE, FALSE, 0, NULL,
			       &amp;type_of_fault);

		vm_object_unlock(cpm_obj);
	}

#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">MACH_ASSERT</span>
	<span class="enscript-comment">/*
	 *	Verify ordering in address space.
	 */</span>
	<span class="enscript-keyword">for</span> (offset = 0; offset &lt; size; offset += PAGE_SIZE) {
		vm_object_lock(cpm_obj);
		m = vm_page_lookup(cpm_obj, (vm_object_offset_t)offset);
		vm_object_unlock(cpm_obj);
		<span class="enscript-keyword">if</span> (m == VM_PAGE_NULL)
			panic(<span class="enscript-string">&quot;vm_allocate_cpm:  obj %p off 0x%llx no page&quot;</span>,
			      cpm_obj, (uint64_t)offset);
		assert(m-&gt;tabled);
		assert(!m-&gt;busy);
		assert(!m-&gt;wanted);
		assert(!m-&gt;fictitious);
		assert(!m-&gt;private);
		assert(!m-&gt;absent);
		assert(!m-&gt;error);
		assert(!m-&gt;cleaning);
		assert(!m-&gt;laundry);
		assert(!m-&gt;precious);
		assert(!m-&gt;clustered);
		<span class="enscript-keyword">if</span> (offset != 0) {
			<span class="enscript-keyword">if</span> (m-&gt;phys_page != prev_addr + 1) {
				printf(<span class="enscript-string">&quot;start 0x%llx end 0x%llx va 0x%llx\n&quot;</span>,
				       (uint64_t)start, (uint64_t)end, (uint64_t)va);
				printf(<span class="enscript-string">&quot;obj %p off 0x%llx\n&quot;</span>, cpm_obj, (uint64_t)offset);
				printf(<span class="enscript-string">&quot;m %p prev_address 0x%llx\n&quot;</span>, m, (uint64_t)prev_addr);
				panic(<span class="enscript-string">&quot;vm_allocate_cpm:  pages not contig!&quot;</span>);
			}
		}
		prev_addr = m-&gt;phys_page;
	}
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* MACH_ASSERT */</span>

	vm_object_deallocate(cpm_obj); <span class="enscript-comment">/* kill extra ref */</span>

	<span class="enscript-keyword">return</span> kr;
}


#<span class="enscript-reference">else</span>	<span class="enscript-comment">/* VM_CPM */</span>

<span class="enscript-comment">/*
 *	Interface is defined in all cases, but unless the kernel
 *	is built explicitly for this option, the interface does
 *	nothing.
 */</span>

kern_return_t
<span class="enscript-function-name">vm_map_enter_cpm</span>(
	__unused vm_map_t	map,
	__unused vm_map_offset_t	*addr,
	__unused vm_map_size_t	size,
	__unused <span class="enscript-type">int</span>		flags)
{
	<span class="enscript-keyword">return</span> KERN_FAILURE;
}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* VM_CPM */</span>

<span class="enscript-comment">/* Not used without nested pmaps */</span>
#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
<span class="enscript-comment">/*
 * Clip and unnest a portion of a nested submap mapping.
 */</span>


<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_clip_unnest</span>(
	vm_map_t	map,
	vm_map_entry_t	entry,
	vm_map_offset_t	start_unnest,
	vm_map_offset_t	end_unnest)
{
	vm_map_offset_t old_start_unnest = start_unnest;
	vm_map_offset_t old_end_unnest = end_unnest;

	assert(entry-&gt;is_sub_map);
	assert(VME_SUBMAP(entry) != NULL);
	assert(entry-&gt;use_pmap);

	<span class="enscript-comment">/*
	 * Query the platform for the optimal unnest range.
	 * DRK: There's some duplication of effort here, since
	 * callers may have adjusted the range to some extent. This
	 * routine was introduced to support 1GiB subtree nesting
	 * for x86 platforms, which can also nest on 2MiB boundaries
	 * depending on size/alignment.
	 */</span>
	<span class="enscript-keyword">if</span> (pmap_adjust_unnest_parameters(map-&gt;pmap, &amp;start_unnest, &amp;end_unnest)) {
		log_unnest_badness(map, old_start_unnest, old_end_unnest);
	}

	<span class="enscript-keyword">if</span> (entry-&gt;vme_start &gt; start_unnest ||
	    entry-&gt;vme_end &lt; end_unnest) {
		panic(<span class="enscript-string">&quot;vm_map_clip_unnest(0x%llx,0x%llx): &quot;</span>
		      <span class="enscript-string">&quot;bad nested entry: start=0x%llx end=0x%llx\n&quot;</span>,
		      (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)start_unnest, (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)end_unnest,
		      (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)entry-&gt;vme_start, (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)entry-&gt;vme_end);
	}

	<span class="enscript-keyword">if</span> (start_unnest &gt; entry-&gt;vme_start) {
		_vm_map_clip_start(&amp;map-&gt;hdr,
				   entry,
				   start_unnest);
		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			vm_map_store_update_first_free(map, NULL, FALSE);
		} <span class="enscript-keyword">else</span> {
			vm_map_store_update_first_free(map, map-&gt;first_free, FALSE);
		}
	}
	<span class="enscript-keyword">if</span> (entry-&gt;vme_end &gt; end_unnest) {
		_vm_map_clip_end(&amp;map-&gt;hdr,
				 entry,
				 end_unnest);
		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			vm_map_store_update_first_free(map, NULL, FALSE);
		} <span class="enscript-keyword">else</span> {
			vm_map_store_update_first_free(map, map-&gt;first_free, FALSE);
		}
	}

	pmap_unnest(map-&gt;pmap,
		    entry-&gt;vme_start,
		    entry-&gt;vme_end - entry-&gt;vme_start);
	<span class="enscript-keyword">if</span> ((map-&gt;mapped_in_other_pmaps) &amp;&amp; (map-&gt;ref_count)) {
		<span class="enscript-comment">/* clean up parent map/maps */</span>
		vm_map_submap_pmap_clean(
			map, entry-&gt;vme_start,
			entry-&gt;vme_end,
			VME_SUBMAP(entry),
			VME_OFFSET(entry));
	}
	entry-&gt;use_pmap = FALSE;
	<span class="enscript-keyword">if</span> ((map-&gt;pmap != kernel_pmap) &amp;&amp;
	    (VME_ALIAS(entry) == VM_MEMORY_SHARED_PMAP)) {
		VME_ALIAS_SET(entry, VM_MEMORY_UNSHARED_PMAP);
	}
}
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* NO_NESTED_PMAP */</span>

<span class="enscript-comment">/*
 *	vm_map_clip_start:	[ internal use only ]
 *
 *	Asserts that the given entry begins at or after
 *	the specified address; if necessary,
 *	it splits the entry into two.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_clip_start</span>(
	vm_map_t	map,
	vm_map_entry_t	entry,
	vm_map_offset_t	startaddr)
{
#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
	<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map &amp;&amp;
	    entry-&gt;use_pmap &amp;&amp;
	    startaddr &gt;= entry-&gt;vme_start) {
		vm_map_offset_t	start_unnest, end_unnest;

		<span class="enscript-comment">/*
		 * Make sure &quot;startaddr&quot; is no longer in a nested range
		 * before we clip.  Unnest only the minimum range the platform
		 * can handle.
		 * vm_map_clip_unnest may perform additional adjustments to
		 * the unnest range.
		 */</span>
		start_unnest = startaddr &amp; ~(pmap_nesting_size_min - 1);
		end_unnest = start_unnest + pmap_nesting_size_min;
		vm_map_clip_unnest(map, entry, start_unnest, end_unnest);
	}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* NO_NESTED_PMAP */</span>
	<span class="enscript-keyword">if</span> (startaddr &gt; entry-&gt;vme_start) {
		<span class="enscript-keyword">if</span> (VME_OBJECT(entry) &amp;&amp;
		    !entry-&gt;is_sub_map &amp;&amp;
		    VME_OBJECT(entry)-&gt;phys_contiguous) {
			pmap_remove(map-&gt;pmap,
				    (addr64_t)(entry-&gt;vme_start),
				    (addr64_t)(entry-&gt;vme_end));
		}
		_vm_map_clip_start(&amp;map-&gt;hdr, entry, startaddr);
		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			vm_map_store_update_first_free(map, NULL, FALSE);
		} <span class="enscript-keyword">else</span> {
			vm_map_store_update_first_free(map, map-&gt;first_free, FALSE);
		}
	}
}


#<span class="enscript-reference">define</span> <span class="enscript-function-name">vm_map_copy_clip_start</span>(copy, entry, startaddr) \
	MACRO_BEGIN \
	<span class="enscript-keyword">if</span> ((startaddr) &gt; (entry)-&gt;vme_start) \
		_vm_map_clip_start(&amp;(copy)-&gt;cpy_hdr,(entry),(startaddr)); \
	MACRO_END

<span class="enscript-comment">/*
 *	This routine is called only when it is known that
 *	the entry must be split.
 */</span>
<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">_vm_map_clip_start</span>(
	<span class="enscript-type">register</span> <span class="enscript-type">struct</span> vm_map_header	*map_header,
	<span class="enscript-type">register</span> vm_map_entry_t		entry,
	<span class="enscript-type">register</span> vm_map_offset_t	start)
{
	<span class="enscript-type">register</span> vm_map_entry_t	new_entry;

	<span class="enscript-comment">/*
	 *	Split off the front portion --
	 *	note that we must insert the new
	 *	entry BEFORE this one, so that
	 *	this entry has the specified starting
	 *	address.
	 */</span>

	<span class="enscript-keyword">if</span> (entry-&gt;map_aligned) {
		assert(VM_MAP_PAGE_ALIGNED(start,
					   VM_MAP_HDR_PAGE_MASK(map_header)));
	}

	new_entry = _vm_map_entry_create(map_header, !map_header-&gt;entries_pageable);
	vm_map_entry_copy_full(new_entry, entry);

	new_entry-&gt;vme_end = start;
	assert(new_entry-&gt;vme_start &lt; new_entry-&gt;vme_end);
	VME_OFFSET_SET(entry, VME_OFFSET(entry) + (start - entry-&gt;vme_start));
	assert(start &lt; entry-&gt;vme_end);
	entry-&gt;vme_start = start;

	_vm_map_store_entry_link(map_header, entry-&gt;vme_prev, new_entry);

	<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map)
		vm_map_reference(VME_SUBMAP(new_entry));
	<span class="enscript-keyword">else</span>
		vm_object_reference(VME_OBJECT(new_entry));
}


<span class="enscript-comment">/*
 *	vm_map_clip_end:	[ internal use only ]
 *
 *	Asserts that the given entry ends at or before
 *	the specified address; if necessary,
 *	it splits the entry into two.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_clip_end</span>(
	vm_map_t	map,
	vm_map_entry_t	entry,
	vm_map_offset_t	endaddr)
{
	<span class="enscript-keyword">if</span> (endaddr &gt; entry-&gt;vme_end) {
		<span class="enscript-comment">/*
		 * Within the scope of this clipping, limit &quot;endaddr&quot; to
		 * the end of this map entry...
		 */</span>
		endaddr = entry-&gt;vme_end;
	}
#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
	<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map &amp;&amp; entry-&gt;use_pmap) {
		vm_map_offset_t	start_unnest, end_unnest;

		<span class="enscript-comment">/*
		 * Make sure the range between the start of this entry and
		 * the new &quot;endaddr&quot; is no longer nested before we clip.
		 * Unnest only the minimum range the platform can handle.
		 * vm_map_clip_unnest may perform additional adjustments to
		 * the unnest range.
		 */</span>
		start_unnest = entry-&gt;vme_start;
		end_unnest =
			(endaddr + pmap_nesting_size_min - 1) &amp;
			~(pmap_nesting_size_min - 1);
		vm_map_clip_unnest(map, entry, start_unnest, end_unnest);
	}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* NO_NESTED_PMAP */</span>
	<span class="enscript-keyword">if</span> (endaddr &lt; entry-&gt;vme_end) {
		<span class="enscript-keyword">if</span> (VME_OBJECT(entry) &amp;&amp;
		    !entry-&gt;is_sub_map &amp;&amp;
		    VME_OBJECT(entry)-&gt;phys_contiguous) {
			pmap_remove(map-&gt;pmap,
				    (addr64_t)(entry-&gt;vme_start),
				    (addr64_t)(entry-&gt;vme_end));
		}
		_vm_map_clip_end(&amp;map-&gt;hdr, entry, endaddr);
		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			vm_map_store_update_first_free(map, NULL, FALSE);
		} <span class="enscript-keyword">else</span> {
			vm_map_store_update_first_free(map, map-&gt;first_free, FALSE);
		}
	}
}


#<span class="enscript-reference">define</span> <span class="enscript-function-name">vm_map_copy_clip_end</span>(copy, entry, endaddr) \
	MACRO_BEGIN \
	<span class="enscript-keyword">if</span> ((endaddr) &lt; (entry)-&gt;vme_end) \
		_vm_map_clip_end(&amp;(copy)-&gt;cpy_hdr,(entry),(endaddr)); \
	MACRO_END

<span class="enscript-comment">/*
 *	This routine is called only when it is known that
 *	the entry must be split.
 */</span>
<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">_vm_map_clip_end</span>(
	<span class="enscript-type">register</span> <span class="enscript-type">struct</span> vm_map_header	*map_header,
	<span class="enscript-type">register</span> vm_map_entry_t		entry,
	<span class="enscript-type">register</span> vm_map_offset_t	end)
{
	<span class="enscript-type">register</span> vm_map_entry_t	new_entry;

	<span class="enscript-comment">/*
	 *	Create a new entry and insert it
	 *	AFTER the specified entry
	 */</span>

	<span class="enscript-keyword">if</span> (entry-&gt;map_aligned) {
		assert(VM_MAP_PAGE_ALIGNED(end,
					   VM_MAP_HDR_PAGE_MASK(map_header)));
	}

	new_entry = _vm_map_entry_create(map_header, !map_header-&gt;entries_pageable);
	vm_map_entry_copy_full(new_entry, entry);

	assert(entry-&gt;vme_start &lt; end);
	new_entry-&gt;vme_start = entry-&gt;vme_end = end;
	VME_OFFSET_SET(new_entry,
		       VME_OFFSET(new_entry) + (end - entry-&gt;vme_start));
	assert(new_entry-&gt;vme_start &lt; new_entry-&gt;vme_end);

	_vm_map_store_entry_link(map_header, entry, new_entry);

	<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map)
		vm_map_reference(VME_SUBMAP(new_entry));
	<span class="enscript-keyword">else</span>
		vm_object_reference(VME_OBJECT(new_entry));
}


<span class="enscript-comment">/*
 *	VM_MAP_RANGE_CHECK:	[ internal use only ]
 *
 *	Asserts that the starting and ending region
 *	addresses fall within the valid range of the map.
 */</span>
#<span class="enscript-reference">define</span>	<span class="enscript-function-name">VM_MAP_RANGE_CHECK</span>(map, start, end)	\
	MACRO_BEGIN				\
	<span class="enscript-keyword">if</span> (start &lt; vm_map_min(map))		\
		start = vm_map_min(map);	\
	<span class="enscript-keyword">if</span> (end &gt; vm_map_max(map))		\
		end = vm_map_max(map);		\
	<span class="enscript-keyword">if</span> (start &gt; end)			\
		start = end;			\
	MACRO_END

<span class="enscript-comment">/*
 *	vm_map_range_check:	[ internal use only ]
 *	
 *	Check that the region defined by the specified start and
 *	end addresses are wholly contained within a single map
 *	entry or set of adjacent map entries of the spacified map,
 *	i.e. the specified region contains no unmapped space.
 *	If any or all of the region is unmapped, FALSE is returned.
 *	Otherwise, TRUE is returned and if the output argument 'entry'
 *	is not NULL it points to the map entry containing the start
 *	of the region.
 *
 *	The map is locked for reading on entry and is left locked.
 */</span>
<span class="enscript-type">static</span> boolean_t
<span class="enscript-function-name">vm_map_range_check</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	vm_map_entry_t		*entry)
{
	vm_map_entry_t		cur;
	<span class="enscript-type">register</span> vm_map_offset_t	prev;

	<span class="enscript-comment">/*
	 * 	Basic sanity checks first
	 */</span>
	<span class="enscript-keyword">if</span> (start &lt; vm_map_min(map) || end &gt; vm_map_max(map) || start &gt; end)
		<span class="enscript-keyword">return</span> (FALSE);

	<span class="enscript-comment">/*
	 * 	Check first if the region starts within a valid
	 *	mapping for the map.
	 */</span>
	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;cur))
		<span class="enscript-keyword">return</span> (FALSE);

	<span class="enscript-comment">/*
	 *	Optimize for the case that the region is contained 
	 *	in a single map entry.
	 */</span>
	<span class="enscript-keyword">if</span> (entry != (vm_map_entry_t *) NULL)
		*entry = cur;
	<span class="enscript-keyword">if</span> (end &lt;= cur-&gt;vme_end)
		<span class="enscript-keyword">return</span> (TRUE);

	<span class="enscript-comment">/*
	 * 	If the region is not wholly contained within a
	 * 	single entry, walk the entries looking for holes.
	 */</span>
	prev = cur-&gt;vme_end;
	cur = cur-&gt;vme_next;
	<span class="enscript-keyword">while</span> ((cur != vm_map_to_entry(map)) &amp;&amp; (prev == cur-&gt;vme_start)) {
		<span class="enscript-keyword">if</span> (end &lt;= cur-&gt;vme_end)
			<span class="enscript-keyword">return</span> (TRUE);
		prev = cur-&gt;vme_end;
		cur = cur-&gt;vme_next;
	}
	<span class="enscript-keyword">return</span> (FALSE);
}

<span class="enscript-comment">/*
 *	vm_map_submap:		[ kernel use only ]
 *
 *	Mark the given range as handled by a subordinate map.
 *
 *	This range must have been created with vm_map_find using
 *	the vm_submap_object, and no other operations may have been
 *	performed on this range prior to calling vm_map_submap.
 *
 *	Only a limited number of operations can be performed
 *	within this rage after calling vm_map_submap:
 *		vm_fault
 *	[Don't try vm_map_copyin!]
 *
 *	To remove a submapping, one must first remove the
 *	range from the superior map, and then destroy the
 *	submap (if desired).  [Better yet, don't try it.]
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_submap</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end,
	vm_map_t	submap,
	vm_map_offset_t	offset,
#<span class="enscript-reference">ifdef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
	__unused
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* NO_NESTED_PMAP */</span>
	boolean_t	use_pmap)
{
	vm_map_entry_t		entry;
	<span class="enscript-type">register</span> kern_return_t	result = KERN_INVALID_ARGUMENT;
	<span class="enscript-type">register</span> vm_object_t	object;

	vm_map_lock(map);

	<span class="enscript-keyword">if</span> (! vm_map_lookup_entry(map, start, &amp;entry)) {
		entry = entry-&gt;vme_next;
	}

	<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(map) ||
	    entry-&gt;is_sub_map) {
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}

	vm_map_clip_start(map, entry, start);
	vm_map_clip_end(map, entry, end);

	<span class="enscript-keyword">if</span> ((entry-&gt;vme_start == start) &amp;&amp; (entry-&gt;vme_end == end) &amp;&amp;
	    (!entry-&gt;is_sub_map) &amp;&amp;
	    ((object = VME_OBJECT(entry)) == vm_submap_object) &amp;&amp;
	    (object-&gt;resident_page_count == 0) &amp;&amp;
	    (object-&gt;copy == VM_OBJECT_NULL) &amp;&amp;
	    (object-&gt;shadow == VM_OBJECT_NULL) &amp;&amp;
	    (!object-&gt;pager_created)) {
		VME_OFFSET_SET(entry, (vm_object_offset_t)offset);
		VME_OBJECT_SET(entry, VM_OBJECT_NULL);
		vm_object_deallocate(object);
		entry-&gt;is_sub_map = TRUE;
		entry-&gt;use_pmap = FALSE;
		VME_SUBMAP_SET(entry, submap);
		vm_map_reference(submap);
		<span class="enscript-keyword">if</span> (submap-&gt;mapped_in_other_pmaps == FALSE &amp;&amp;
		    vm_map_pmap(submap) != PMAP_NULL &amp;&amp;
		    vm_map_pmap(submap) != vm_map_pmap(map)) {
			<span class="enscript-comment">/*
			 * This submap is being mapped in a map
			 * that uses a different pmap.
			 * Set its &quot;mapped_in_other_pmaps&quot; flag
			 * to indicate that we now need to 
			 * remove mappings from all pmaps rather
			 * than just the submap's pmap.
			 */</span>
			submap-&gt;mapped_in_other_pmaps = TRUE;
		}

#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
		<span class="enscript-keyword">if</span> (use_pmap) {
			<span class="enscript-comment">/* nest if platform code will allow */</span>
			<span class="enscript-keyword">if</span>(submap-&gt;pmap == NULL) {
				ledger_t ledger = map-&gt;pmap-&gt;ledger;
				submap-&gt;pmap = pmap_create(ledger,
						(vm_map_size_t) 0, FALSE);
				<span class="enscript-keyword">if</span>(submap-&gt;pmap == PMAP_NULL) {
					vm_map_unlock(map);
					<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
				}
			}
			result = pmap_nest(map-&gt;pmap,
					   (VME_SUBMAP(entry))-&gt;pmap, 
					   (addr64_t)start,
					   (addr64_t)start,
					   (uint64_t)(end - start));
			<span class="enscript-keyword">if</span>(result)
				panic(<span class="enscript-string">&quot;vm_map_submap: pmap_nest failed, rc = %08X\n&quot;</span>, result);
			entry-&gt;use_pmap = TRUE;
		}
#<span class="enscript-reference">else</span>	<span class="enscript-comment">/* NO_NESTED_PMAP */</span>
		pmap_remove(map-&gt;pmap, (addr64_t)start, (addr64_t)end);
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* NO_NESTED_PMAP */</span>
		result = KERN_SUCCESS;
	}
	vm_map_unlock(map);

	<span class="enscript-keyword">return</span>(result);
}

<span class="enscript-comment">/*
 *	vm_map_protect:
 *
 *	Sets the protection of the specified address
 *	region in the target map.  If &quot;set_max&quot; is
 *	specified, the maximum protection is to be set;
 *	otherwise, only the current protection is affected.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_protect</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	<span class="enscript-type">register</span> vm_prot_t	new_prot,
	<span class="enscript-type">register</span> boolean_t	set_max)
{
	<span class="enscript-type">register</span> vm_map_entry_t		current;
	<span class="enscript-type">register</span> vm_map_offset_t	prev;
	vm_map_entry_t			entry;
	vm_prot_t			new_max;

	XPR(XPR_VM_MAP,
	    <span class="enscript-string">&quot;vm_map_protect, 0x%X start 0x%X end 0x%X, new 0x%X %d&quot;</span>,
	    map, start, end, new_prot, set_max);

	vm_map_lock(map);

	<span class="enscript-comment">/* LP64todo - remove this check when vm_map_commpage64()
	 * no longer has to stuff in a map_entry for the commpage
	 * above the map's max_offset.
	 */</span>
	<span class="enscript-keyword">if</span> (start &gt;= map-&gt;max_offset) {
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}

	<span class="enscript-keyword">while</span>(1) {
		<span class="enscript-comment">/*
		 * 	Lookup the entry.  If it doesn't start in a valid
		 *	entry, return an error.
		 */</span>
		<span class="enscript-keyword">if</span> (! vm_map_lookup_entry(map, start, &amp;entry)) {
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}

		<span class="enscript-keyword">if</span> (entry-&gt;superpage_size &amp;&amp; (start &amp; (SUPERPAGE_SIZE-1))) { <span class="enscript-comment">/* extend request to whole entry */</span>
			start = SUPERPAGE_ROUND_DOWN(start);
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-keyword">break</span>;
 	}
	<span class="enscript-keyword">if</span> (entry-&gt;superpage_size)
 		end = SUPERPAGE_ROUND_UP(end);

	<span class="enscript-comment">/*
	 *	Make a first pass to check for protection and address
	 *	violations.
	 */</span>

	current = entry;
	prev = current-&gt;vme_start;
	<span class="enscript-keyword">while</span> ((current != vm_map_to_entry(map)) &amp;&amp;
	       (current-&gt;vme_start &lt; end)) {

		<span class="enscript-comment">/*
		 * If there is a hole, return an error.
		 */</span>
		<span class="enscript-keyword">if</span> (current-&gt;vme_start != prev) {
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}

		new_max = current-&gt;max_protection;
		<span class="enscript-keyword">if</span>(new_prot &amp; VM_PROT_COPY) {
			new_max |= VM_PROT_WRITE;
			<span class="enscript-keyword">if</span> ((new_prot &amp; (new_max | VM_PROT_COPY)) != new_prot) {
				vm_map_unlock(map);
				<span class="enscript-keyword">return</span>(KERN_PROTECTION_FAILURE);
			}
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> ((new_prot &amp; new_max) != new_prot) {
				vm_map_unlock(map);
				<span class="enscript-keyword">return</span>(KERN_PROTECTION_FAILURE);
			}
		}


		prev = current-&gt;vme_end;
		current = current-&gt;vme_next;
	}
	<span class="enscript-keyword">if</span> (end &gt; prev) {
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}

	<span class="enscript-comment">/*
	 *	Go back and fix up protections.
	 *	Clip to start here if the range starts within
	 *	the entry.
	 */</span>

	current = entry;
	<span class="enscript-keyword">if</span> (current != vm_map_to_entry(map)) {
		<span class="enscript-comment">/* clip and unnest if necessary */</span>
		vm_map_clip_start(map, current, start);
	}

	<span class="enscript-keyword">while</span> ((current != vm_map_to_entry(map)) &amp;&amp;
	       (current-&gt;vme_start &lt; end)) {

		vm_prot_t	old_prot;

		vm_map_clip_end(map, current, end);

		<span class="enscript-keyword">if</span> (current-&gt;is_sub_map) {
			<span class="enscript-comment">/* clipping did unnest if needed */</span>
			assert(!current-&gt;use_pmap);
		}

		old_prot = current-&gt;protection;

		<span class="enscript-keyword">if</span>(new_prot &amp; VM_PROT_COPY) {
			<span class="enscript-comment">/* caller is asking specifically to copy the      */</span>
			<span class="enscript-comment">/* mapped data, this implies that max protection  */</span>
			<span class="enscript-comment">/* will include write.  Caller must be prepared   */</span>
			<span class="enscript-comment">/* for loss of shared memory communication in the */</span>
			<span class="enscript-comment">/* target area after taking this step */</span>

			<span class="enscript-keyword">if</span> (current-&gt;is_sub_map == FALSE &amp;&amp;
			    VME_OBJECT(current) == VM_OBJECT_NULL) {
				VME_OBJECT_SET(current, 
					       vm_object_allocate(
						       (vm_map_size_t)
						       (current-&gt;vme_end -
							current-&gt;vme_start)));
				VME_OFFSET_SET(current, 0);
				assert(current-&gt;use_pmap);
			}
			assert(current-&gt;wired_count == 0);
			current-&gt;needs_copy = TRUE;
			current-&gt;max_protection |= VM_PROT_WRITE;
		}

		<span class="enscript-keyword">if</span> (set_max)
			current-&gt;protection =
				(current-&gt;max_protection = 
				 new_prot &amp; ~VM_PROT_COPY) &amp;
				old_prot;
		<span class="enscript-keyword">else</span>
			current-&gt;protection = new_prot &amp; ~VM_PROT_COPY;

		<span class="enscript-comment">/*
		 *	Update physical map if necessary.
		 *	If the request is to turn off write protection, 
		 *	we won't do it for real (in pmap). This is because 
		 *	it would cause copy-on-write to fail.  We've already 
		 *	set, the new protection in the map, so if a 
		 *	write-protect fault occurred, it will be fixed up 
		 *	properly, COW or not.
		 */</span>
		<span class="enscript-keyword">if</span> (current-&gt;protection != old_prot) {
			<span class="enscript-comment">/* Look one level in we support nested pmaps */</span>
			<span class="enscript-comment">/* from mapped submaps which are direct entries */</span>
			<span class="enscript-comment">/* in our map */</span>

			vm_prot_t prot;

			prot = current-&gt;protection &amp; ~VM_PROT_WRITE;

			<span class="enscript-keyword">if</span> (override_nx(map, VME_ALIAS(current)) &amp;&amp; prot)
			        prot |= VM_PROT_EXECUTE;

			<span class="enscript-keyword">if</span> (current-&gt;is_sub_map &amp;&amp; current-&gt;use_pmap) {
				pmap_protect(VME_SUBMAP(current)-&gt;pmap, 
					     current-&gt;vme_start,
					     current-&gt;vme_end,
					     prot);
			} <span class="enscript-keyword">else</span> {
				pmap_protect(map-&gt;pmap,
					     current-&gt;vme_start,
					     current-&gt;vme_end,
					     prot);
			}
		}
		current = current-&gt;vme_next;
	}

	current = entry;
	<span class="enscript-keyword">while</span> ((current != vm_map_to_entry(map)) &amp;&amp;
	       (current-&gt;vme_start &lt;= end)) {
		vm_map_simplify_entry(map, current);
		current = current-&gt;vme_next;
	}

	vm_map_unlock(map);
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}

<span class="enscript-comment">/*
 *	vm_map_inherit:
 *
 *	Sets the inheritance of the specified address
 *	range in the target map.  Inheritance
 *	affects how the map will be shared with
 *	child maps at the time of vm_map_fork.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_inherit</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	<span class="enscript-type">register</span> vm_inherit_t	new_inheritance)
{
	<span class="enscript-type">register</span> vm_map_entry_t	entry;
	vm_map_entry_t	temp_entry;

	vm_map_lock(map);

	VM_MAP_RANGE_CHECK(map, start, end);

	<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;temp_entry)) {
		entry = temp_entry;
	}
	<span class="enscript-keyword">else</span> {
		temp_entry = temp_entry-&gt;vme_next;
		entry = temp_entry;
	}

	<span class="enscript-comment">/* first check entire range for submaps which can't support the */</span>
	<span class="enscript-comment">/* given inheritance. */</span>
	<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp; (entry-&gt;vme_start &lt; end)) {
		<span class="enscript-keyword">if</span>(entry-&gt;is_sub_map) {
			<span class="enscript-keyword">if</span>(new_inheritance == VM_INHERIT_COPY) {
				vm_map_unlock(map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);
			}
		}

		entry = entry-&gt;vme_next;
	}

	entry = temp_entry;
	<span class="enscript-keyword">if</span> (entry != vm_map_to_entry(map)) {
		<span class="enscript-comment">/* clip and unnest if necessary */</span>
		vm_map_clip_start(map, entry, start);
	}

	<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp; (entry-&gt;vme_start &lt; end)) {
		vm_map_clip_end(map, entry, end);
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			<span class="enscript-comment">/* clip did unnest if needed */</span>
			assert(!entry-&gt;use_pmap);
		}

		entry-&gt;inheritance = new_inheritance;

		entry = entry-&gt;vme_next;
	}

	vm_map_unlock(map);
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}

<span class="enscript-comment">/*
 * Update the accounting for the amount of wired memory in this map.  If the user has
 * exceeded the defined limits, then we fail.  Wiring on behalf of the kernel never fails.
 */</span>

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">add_wire_counts</span>(
	vm_map_t	map,
	vm_map_entry_t	entry, 
	boolean_t	user_wire)
{ 
	vm_map_size_t	size;

	<span class="enscript-keyword">if</span> (user_wire) {
		<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> total_wire_count =  vm_page_wire_count + vm_lopage_free_count;

		<span class="enscript-comment">/*
		 * We're wiring memory at the request of the user.  Check if this is the first time the user is wiring
		 * this map entry.
		 */</span>

		<span class="enscript-keyword">if</span> (entry-&gt;user_wired_count == 0) {
			size = entry-&gt;vme_end - entry-&gt;vme_start;
 
			<span class="enscript-comment">/*
			 * Since this is the first time the user is wiring this map entry, check to see if we're
			 * exceeding the user wire limits.  There is a per map limit which is the smaller of either
			 * the process's rlimit or the global vm_user_wire_limit which caps this value.  There is also
			 * a system-wide limit on the amount of memory all users can wire.  If the user is over either
			 * limit, then we fail.
			 */</span>

			<span class="enscript-keyword">if</span>(size + map-&gt;user_wire_size &gt; MIN(map-&gt;user_wire_limit, vm_user_wire_limit) ||
			   size + ptoa_64(total_wire_count) &gt; vm_global_user_wire_limit ||
		    	   size + ptoa_64(total_wire_count) &gt; max_mem - vm_global_no_user_wire_amount)
				<span class="enscript-keyword">return</span> KERN_RESOURCE_SHORTAGE;

			<span class="enscript-comment">/*
			 * The first time the user wires an entry, we also increment the wired_count and add this to
			 * the total that has been wired in the map.
			 */</span>

			<span class="enscript-keyword">if</span> (entry-&gt;wired_count &gt;= MAX_WIRE_COUNT)
				<span class="enscript-keyword">return</span> KERN_FAILURE;

			entry-&gt;wired_count++;
			map-&gt;user_wire_size += size;
		}

		<span class="enscript-keyword">if</span> (entry-&gt;user_wired_count &gt;= MAX_WIRE_COUNT)
			<span class="enscript-keyword">return</span> KERN_FAILURE;

		entry-&gt;user_wired_count++;

	} <span class="enscript-keyword">else</span> {

		<span class="enscript-comment">/*
		 * The kernel's wiring the memory.  Just bump the count and continue.
		 */</span>

		<span class="enscript-keyword">if</span> (entry-&gt;wired_count &gt;= MAX_WIRE_COUNT)
			panic(<span class="enscript-string">&quot;vm_map_wire: too many wirings&quot;</span>);

		entry-&gt;wired_count++;
	}

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 * Update the memory wiring accounting now that the given map entry is being unwired.
 */</span>

<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">subtract_wire_counts</span>(
	vm_map_t	map,
	vm_map_entry_t	entry, 
	boolean_t	user_wire)
{ 

	<span class="enscript-keyword">if</span> (user_wire) {

		<span class="enscript-comment">/*
		 * We're unwiring memory at the request of the user.  See if we're removing the last user wire reference.
		 */</span>

		<span class="enscript-keyword">if</span> (entry-&gt;user_wired_count == 1) {

			<span class="enscript-comment">/*
			 * We're removing the last user wire reference.  Decrement the wired_count and the total
			 * user wired memory for this map.
			 */</span>

			assert(entry-&gt;wired_count &gt;= 1);
			entry-&gt;wired_count--;
			map-&gt;user_wire_size -= entry-&gt;vme_end - entry-&gt;vme_start;
		}

		assert(entry-&gt;user_wired_count &gt;= 1);
		entry-&gt;user_wired_count--;

	} <span class="enscript-keyword">else</span> {

		<span class="enscript-comment">/*
		 * The kernel is unwiring the memory.   Just update the count.
		 */</span>

		assert(entry-&gt;wired_count &gt;= 1);
		entry-&gt;wired_count--;
	}
}

<span class="enscript-comment">/*
 *	vm_map_wire:
 *
 *	Sets the pageability of the specified address range in the
 *	target map as wired.  Regions specified as not pageable require
 *	locked-down physical memory and physical page maps.  The
 *	access_type variable indicates types of accesses that must not
 *	generate page faults.  This is checked against protection of
 *	memory being locked-down.
 *
 *	The map must not be locked, but a reference must remain to the
 *	map throughout the call.
 */</span>
<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_wire_nested</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	<span class="enscript-type">register</span> vm_prot_t	caller_prot,
	boolean_t		user_wire,
	pmap_t			map_pmap, 
	vm_map_offset_t		pmap_addr,
	ppnum_t			*physpage_p)
{
	<span class="enscript-type">register</span> vm_map_entry_t	entry;
	<span class="enscript-type">register</span> vm_prot_t	access_type;
	<span class="enscript-type">struct</span> vm_map_entry	*first_entry, tmp_entry;
	vm_map_t		real_map;
	<span class="enscript-type">register</span> vm_map_offset_t	s,e;
	kern_return_t		rc;
	boolean_t		need_wakeup;
	boolean_t		main_map = FALSE;
	wait_interrupt_t	interruptible_state;
	thread_t		cur_thread;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		last_timestamp;
	vm_map_size_t		size;
	boolean_t		wire_and_extract;

	access_type = (caller_prot &amp; VM_PROT_ALL);

	wire_and_extract = FALSE;
	<span class="enscript-keyword">if</span> (physpage_p != NULL) {
		<span class="enscript-comment">/*
		 * The caller wants the physical page number of the
		 * wired page.  We return only one physical page number
		 * so this works for only one page at a time.
		 */</span>
		<span class="enscript-keyword">if</span> ((end - start) != PAGE_SIZE) {
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}
		wire_and_extract = TRUE;
		*physpage_p = 0;
	}

	vm_map_lock(map);
	<span class="enscript-keyword">if</span>(map_pmap == NULL)
		main_map = TRUE;
	last_timestamp = map-&gt;timestamp;

	VM_MAP_RANGE_CHECK(map, start, end);
	assert(page_aligned(start));
	assert(page_aligned(end));
	assert(VM_MAP_PAGE_ALIGNED(start, VM_MAP_PAGE_MASK(map)));
	assert(VM_MAP_PAGE_ALIGNED(end, VM_MAP_PAGE_MASK(map)));
	<span class="enscript-keyword">if</span> (start == end) {
		<span class="enscript-comment">/* We wired what the caller asked for, zero pages */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> KERN_SUCCESS;
	}

	need_wakeup = FALSE;
	cur_thread = current_thread();

	s = start;
	rc = KERN_SUCCESS;

	<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, s, &amp;first_entry)) {
		entry = first_entry;
		<span class="enscript-comment">/*
		 * vm_map_clip_start will be done later.
		 * We don't want to unnest any nested submaps here !
		 */</span>
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-comment">/* Start address is not in map */</span>
		rc = KERN_INVALID_ADDRESS;
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
	}

	<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp; (s &lt; end)) {
		<span class="enscript-comment">/*
		 * At this point, we have wired from &quot;start&quot; to &quot;s&quot;.
		 * We still need to wire from &quot;s&quot; to &quot;end&quot;.
		 *
		 * &quot;entry&quot; hasn't been clipped, so it could start before &quot;s&quot;
		 * and/or end after &quot;end&quot;.
		 */</span>

		<span class="enscript-comment">/* &quot;e&quot; is how far we want to wire in this entry */</span>
		e = entry-&gt;vme_end;
		<span class="enscript-keyword">if</span> (e &gt; end)
			e = end;

		<span class="enscript-comment">/*
		 * If another thread is wiring/unwiring this entry then
		 * block after informing other thread to wake us up.
		 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;in_transition) {
			wait_result_t wait_result;

			<span class="enscript-comment">/*
			 * We have not clipped the entry.  Make sure that
			 * the start address is in range so that the lookup
			 * below will succeed.
			 * &quot;s&quot; is the current starting point: we've already
			 * wired from &quot;start&quot; to &quot;s&quot; and we still have
			 * to wire from &quot;s&quot; to &quot;end&quot;.
			 */</span>

			entry-&gt;needs_wakeup = TRUE;

			<span class="enscript-comment">/*
			 * wake up anybody waiting on entries that we have
			 * already wired.
			 */</span>
			<span class="enscript-keyword">if</span> (need_wakeup) {
				vm_map_entry_wakeup(map);
				need_wakeup = FALSE;
			}
			<span class="enscript-comment">/*
			 * User wiring is interruptible
			 */</span>
			wait_result = vm_map_entry_wait(map, 
							(user_wire) ? THREAD_ABORTSAFE :
							THREAD_UNINT);
			<span class="enscript-keyword">if</span> (user_wire &amp;&amp; wait_result ==	THREAD_INTERRUPTED) {
				<span class="enscript-comment">/*
				 * undo the wirings we have done so far
				 * We do not clear the needs_wakeup flag,
				 * because we cannot tell if we were the
				 * only one waiting.
				 */</span>
				rc = KERN_FAILURE;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
			}

			<span class="enscript-comment">/*
			 * Cannot avoid a lookup here. reset timestamp.
			 */</span>
			last_timestamp = map-&gt;timestamp;

			<span class="enscript-comment">/*
			 * The entry could have been clipped, look it up again.
			 * Worse that can happen is, it may not exist anymore.
			 */</span>
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, s, &amp;first_entry)) {
				<span class="enscript-comment">/*
				 * User: undo everything upto the previous
				 * entry.  let vm_map_unwire worry about
				 * checking the validity of the range.
				 */</span>
				rc = KERN_FAILURE;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
			}
			entry = first_entry;
			<span class="enscript-keyword">continue</span>;
		}
	
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			vm_map_offset_t	sub_start;
			vm_map_offset_t	sub_end;
			vm_map_offset_t	local_start;
			vm_map_offset_t	local_end;
			pmap_t		pmap;

			<span class="enscript-keyword">if</span> (wire_and_extract) {
				<span class="enscript-comment">/*
				 * Wiring would result in copy-on-write
				 * which would not be compatible with
				 * the sharing we have with the original
				 * provider of this memory.
				 */</span>
				rc = KERN_INVALID_ARGUMENT;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
			}

			vm_map_clip_start(map, entry, s);
			vm_map_clip_end(map, entry, end);

			sub_start = VME_OFFSET(entry);
			sub_end = entry-&gt;vme_end;
			sub_end += VME_OFFSET(entry) - entry-&gt;vme_start;
		
			local_end = entry-&gt;vme_end;
			<span class="enscript-keyword">if</span>(map_pmap == NULL) {
				vm_object_t		object;
				vm_object_offset_t	offset;
				vm_prot_t		prot;
				boolean_t		wired;
				vm_map_entry_t		local_entry;
				vm_map_version_t	 version;
				vm_map_t		lookup_map;

				<span class="enscript-keyword">if</span>(entry-&gt;use_pmap) {
					pmap = VME_SUBMAP(entry)-&gt;pmap;
					<span class="enscript-comment">/* ppc implementation requires that */</span>
					<span class="enscript-comment">/* submaps pmap address ranges line */</span>
					<span class="enscript-comment">/* up with parent map */</span>
#<span class="enscript-reference">ifdef</span> <span class="enscript-variable-name">notdef</span>
					pmap_addr = sub_start;
#<span class="enscript-reference">endif</span>
					pmap_addr = s;
				} <span class="enscript-keyword">else</span> {
					pmap = map-&gt;pmap;
					pmap_addr = s;
				}

				<span class="enscript-keyword">if</span> (entry-&gt;wired_count) {
					<span class="enscript-keyword">if</span> ((rc = add_wire_counts(map, entry, user_wire)) != KERN_SUCCESS)
						<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;

					<span class="enscript-comment">/*
					 * The map was not unlocked:
					 * no need to goto re-lookup.
					 * Just go directly to next entry.
					 */</span>
					entry = entry-&gt;vme_next;
					s = entry-&gt;vme_start;
					<span class="enscript-keyword">continue</span>;

				}

				<span class="enscript-comment">/* call vm_map_lookup_locked to */</span>
				<span class="enscript-comment">/* cause any needs copy to be   */</span>
				<span class="enscript-comment">/* evaluated */</span>
				local_start = entry-&gt;vme_start;
				lookup_map = map;
				vm_map_lock_write_to_read(map);
				<span class="enscript-keyword">if</span>(vm_map_lookup_locked(
					   &amp;lookup_map, local_start, 
					   access_type,
					   OBJECT_LOCK_EXCLUSIVE,
					   &amp;version, &amp;object,
					   &amp;offset, &amp;prot, &amp;wired,
					   NULL,
					   &amp;real_map)) {

					vm_map_unlock_read(lookup_map);
					vm_map_unwire(map, start,
						      s, user_wire);
					<span class="enscript-keyword">return</span>(KERN_FAILURE);
				}
				vm_object_unlock(object);
				<span class="enscript-keyword">if</span>(real_map != lookup_map)
					vm_map_unlock(real_map);
				vm_map_unlock_read(lookup_map);
				vm_map_lock(map);

				<span class="enscript-comment">/* we unlocked, so must re-lookup */</span>
				<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, 
							 local_start,
							 &amp;local_entry)) {
					rc = KERN_FAILURE;
					<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
				}

				<span class="enscript-comment">/*
				 * entry could have been &quot;simplified&quot;,
				 * so re-clip
				 */</span>
				entry = local_entry;
				assert(s == local_start);
				vm_map_clip_start(map, entry, s);
				vm_map_clip_end(map, entry, end);
				<span class="enscript-comment">/* re-compute &quot;e&quot; */</span>
				e = entry-&gt;vme_end;
				<span class="enscript-keyword">if</span> (e &gt; end)
					e = end;

				<span class="enscript-comment">/* did we have a change of type? */</span>
				<span class="enscript-keyword">if</span> (!entry-&gt;is_sub_map) {
					last_timestamp = map-&gt;timestamp;
					<span class="enscript-keyword">continue</span>;
				}
			} <span class="enscript-keyword">else</span> {
				local_start = entry-&gt;vme_start;
				pmap = map_pmap;
			}

			<span class="enscript-keyword">if</span> ((rc = add_wire_counts(map, entry, user_wire)) != KERN_SUCCESS)
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;

			entry-&gt;in_transition = TRUE;

			vm_map_unlock(map);
			rc = vm_map_wire_nested(VME_SUBMAP(entry), 
						sub_start, sub_end,
						caller_prot, 
						user_wire, pmap, pmap_addr,
						NULL);
			vm_map_lock(map);

			<span class="enscript-comment">/*
			 * Find the entry again.  It could have been clipped
			 * after we unlocked the map.
			 */</span>
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, local_start,
						 &amp;first_entry))
				panic(<span class="enscript-string">&quot;vm_map_wire: re-lookup failed&quot;</span>);
			entry = first_entry;

			assert(local_start == s);
			<span class="enscript-comment">/* re-compute &quot;e&quot; */</span>
			e = entry-&gt;vme_end;
			<span class="enscript-keyword">if</span> (e &gt; end)
				e = end;

			last_timestamp = map-&gt;timestamp;
			<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp;
			       (entry-&gt;vme_start &lt; e)) {
				assert(entry-&gt;in_transition);
				entry-&gt;in_transition = FALSE;
				<span class="enscript-keyword">if</span> (entry-&gt;needs_wakeup) {
					entry-&gt;needs_wakeup = FALSE;
					need_wakeup = TRUE;
				}
				<span class="enscript-keyword">if</span> (rc != KERN_SUCCESS) {<span class="enscript-comment">/* from vm_*_wire */</span>
					subtract_wire_counts(map, entry, user_wire);
				}
				entry = entry-&gt;vme_next;
			}
			<span class="enscript-keyword">if</span> (rc != KERN_SUCCESS) {	<span class="enscript-comment">/* from vm_*_wire */</span>
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
			}

			<span class="enscript-comment">/* no need to relookup again */</span>
			s = entry-&gt;vme_start;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-comment">/*
		 * If this entry is already wired then increment
		 * the appropriate wire reference count.
		 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;wired_count) {

			<span class="enscript-keyword">if</span> ((entry-&gt;protection &amp; access_type) != access_type) {
				<span class="enscript-comment">/* found a protection problem */</span>

				<span class="enscript-comment">/*
				 * XXX FBDP
				 * We should always return an error
				 * in this case but since we didn't
				 * enforce it before, let's do
				 * it only for the new &quot;wire_and_extract&quot;
				 * code path for now...
				 */</span>
				<span class="enscript-keyword">if</span> (wire_and_extract) {
					rc = KERN_PROTECTION_FAILURE;
					<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
				}
			}

			<span class="enscript-comment">/*
			 * entry is already wired down, get our reference
			 * after clipping to our range.
			 */</span>
			vm_map_clip_start(map, entry, s);
			vm_map_clip_end(map, entry, end);

			<span class="enscript-keyword">if</span> ((rc = add_wire_counts(map, entry, user_wire)) != KERN_SUCCESS)
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;

			<span class="enscript-keyword">if</span> (wire_and_extract) {
				vm_object_t		object;
				vm_object_offset_t	offset;
				vm_page_t		m;

				<span class="enscript-comment">/*
				 * We don't have to &quot;wire&quot; the page again
				 * bit we still have to &quot;extract&quot; its
				 * physical page number, after some sanity
				 * checks.
				 */</span>
				assert((entry-&gt;vme_end - entry-&gt;vme_start)
				       == PAGE_SIZE);
				assert(!entry-&gt;needs_copy);
				assert(!entry-&gt;is_sub_map);
				assert(VME_OBJECT(entry));
				<span class="enscript-keyword">if</span> (((entry-&gt;vme_end - entry-&gt;vme_start)
				     != PAGE_SIZE) ||
				    entry-&gt;needs_copy ||
				    entry-&gt;is_sub_map ||
				    VME_OBJECT(entry) == VM_OBJECT_NULL) {
					rc = KERN_INVALID_ARGUMENT;
					<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
				}

				object = VME_OBJECT(entry);
				offset = VME_OFFSET(entry);
				<span class="enscript-comment">/* need exclusive lock to update m-&gt;dirty */</span>
				<span class="enscript-keyword">if</span> (entry-&gt;protection &amp; VM_PROT_WRITE) {
					vm_object_lock(object);
				} <span class="enscript-keyword">else</span> {
					vm_object_lock_shared(object);
				}
				m = vm_page_lookup(object, offset);
				assert(m != VM_PAGE_NULL);
				assert(m-&gt;wire_count);
				<span class="enscript-keyword">if</span> (m != VM_PAGE_NULL &amp;&amp; m-&gt;wire_count) {
					*physpage_p = m-&gt;phys_page;
					<span class="enscript-keyword">if</span> (entry-&gt;protection &amp; VM_PROT_WRITE) {
						vm_object_lock_assert_exclusive(
							m-&gt;object);
						m-&gt;dirty = TRUE;
					}
				} <span class="enscript-keyword">else</span> {
					<span class="enscript-comment">/* not already wired !? */</span>
					*physpage_p = 0;
				}
				vm_object_unlock(object);
			}

			<span class="enscript-comment">/* map was not unlocked: no need to relookup */</span>
			entry = entry-&gt;vme_next;
			s = entry-&gt;vme_start;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-comment">/*
		 * Unwired entry or wire request transmitted via submap
		 */</span>


		<span class="enscript-comment">/*
		 * Perform actions of vm_map_lookup that need the write
		 * lock on the map: create a shadow object for a
		 * copy-on-write region, or an object for a zero-fill
		 * region.
		 */</span>
		size = entry-&gt;vme_end - entry-&gt;vme_start;
		<span class="enscript-comment">/*
		 * If wiring a copy-on-write page, we need to copy it now
		 * even if we're only (currently) requesting read access.
		 * This is aggressive, but once it's wired we can't move it.
		 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;needs_copy) {
			<span class="enscript-keyword">if</span> (wire_and_extract) {
				<span class="enscript-comment">/*
				 * We're supposed to share with the original
				 * provider so should not be &quot;needs_copy&quot;
				 */</span>
				rc = KERN_INVALID_ARGUMENT;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
			}

			VME_OBJECT_SHADOW(entry, size);
			entry-&gt;needs_copy = FALSE;
		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (VME_OBJECT(entry) == VM_OBJECT_NULL) {
			<span class="enscript-keyword">if</span> (wire_and_extract) {
				<span class="enscript-comment">/*
				 * We're supposed to share with the original
				 * provider so should already have an object.
				 */</span>
				rc = KERN_INVALID_ARGUMENT;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
			}
			VME_OBJECT_SET(entry, vm_object_allocate(size));
			VME_OFFSET_SET(entry, (vm_object_offset_t)0);
			assert(entry-&gt;use_pmap);
		}

		vm_map_clip_start(map, entry, s);
		vm_map_clip_end(map, entry, end);

		<span class="enscript-comment">/* re-compute &quot;e&quot; */</span>
		e = entry-&gt;vme_end;
		<span class="enscript-keyword">if</span> (e &gt; end)
			e = end;

		<span class="enscript-comment">/*
		 * Check for holes and protection mismatch.
		 * Holes: Next entry should be contiguous unless this
		 *	  is the end of the region.
		 * Protection: Access requested must be allowed, unless
		 *	wiring is by protection class
		 */</span>
		<span class="enscript-keyword">if</span> ((entry-&gt;vme_end &lt; end) &amp;&amp;
		    ((entry-&gt;vme_next == vm_map_to_entry(map)) ||
		     (entry-&gt;vme_next-&gt;vme_start &gt; entry-&gt;vme_end))) {
			<span class="enscript-comment">/* found a hole */</span>
			rc = KERN_INVALID_ADDRESS;
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
		}
		<span class="enscript-keyword">if</span> ((entry-&gt;protection &amp; access_type) != access_type) {
			<span class="enscript-comment">/* found a protection problem */</span>
			rc = KERN_PROTECTION_FAILURE;
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
		}

		assert(entry-&gt;wired_count == 0 &amp;&amp; entry-&gt;user_wired_count == 0);

		<span class="enscript-keyword">if</span> ((rc = add_wire_counts(map, entry, user_wire)) != KERN_SUCCESS)
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;

		entry-&gt;in_transition = TRUE;

		<span class="enscript-comment">/*
		 * This entry might get split once we unlock the map.
		 * In vm_fault_wire(), we need the current range as
		 * defined by this entry.  In order for this to work
		 * along with a simultaneous clip operation, we make a
		 * temporary copy of this entry and use that for the
		 * wiring.  Note that the underlying objects do not
		 * change during a clip.
		 */</span>
		tmp_entry = *entry;

		<span class="enscript-comment">/*
		 * The in_transition state guarentees that the entry
		 * (or entries for this range, if split occured) will be
		 * there when the map lock is acquired for the second time.
		 */</span>
		vm_map_unlock(map);

		<span class="enscript-keyword">if</span> (!user_wire &amp;&amp; cur_thread != THREAD_NULL)
			interruptible_state = thread_interrupt_level(THREAD_UNINT);
		<span class="enscript-keyword">else</span>
			interruptible_state = THREAD_UNINT;

		<span class="enscript-keyword">if</span>(map_pmap)
			rc = vm_fault_wire(map, 
					   &amp;tmp_entry, caller_prot, map_pmap, pmap_addr,
					   physpage_p);
		<span class="enscript-keyword">else</span>
			rc = vm_fault_wire(map, 
					   &amp;tmp_entry, caller_prot, map-&gt;pmap, 
					   tmp_entry.vme_start,
					   physpage_p);

		<span class="enscript-keyword">if</span> (!user_wire &amp;&amp; cur_thread != THREAD_NULL)
			thread_interrupt_level(interruptible_state);

		vm_map_lock(map);

		<span class="enscript-keyword">if</span> (last_timestamp+1 != map-&gt;timestamp) {
			<span class="enscript-comment">/*
			 * Find the entry again.  It could have been clipped
			 * after we unlocked the map.
			 */</span>
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, tmp_entry.vme_start,
						 &amp;first_entry))
				panic(<span class="enscript-string">&quot;vm_map_wire: re-lookup failed&quot;</span>);

			entry = first_entry;
		}

		last_timestamp = map-&gt;timestamp;

		<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp;
		       (entry-&gt;vme_start &lt; tmp_entry.vme_end)) {
			assert(entry-&gt;in_transition);
			entry-&gt;in_transition = FALSE;
			<span class="enscript-keyword">if</span> (entry-&gt;needs_wakeup) {
				entry-&gt;needs_wakeup = FALSE;
				need_wakeup = TRUE;
			}
			<span class="enscript-keyword">if</span> (rc != KERN_SUCCESS) {	<span class="enscript-comment">/* from vm_*_wire */</span>
				subtract_wire_counts(map, entry, user_wire);
			}
			entry = entry-&gt;vme_next;
		}

		<span class="enscript-keyword">if</span> (rc != KERN_SUCCESS) {		<span class="enscript-comment">/* from vm_*_wire */</span>
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
		}

		s = entry-&gt;vme_start;
	} <span class="enscript-comment">/* end while loop through map entries */</span>

<span class="enscript-reference">done</span>:
	<span class="enscript-keyword">if</span> (rc == KERN_SUCCESS) {
		<span class="enscript-comment">/* repair any damage we may have made to the VM map */</span>
		vm_map_simplify_range(map, start, end);
	}

	vm_map_unlock(map);

	<span class="enscript-comment">/*
	 * wake up anybody waiting on entries we wired.
	 */</span>
	<span class="enscript-keyword">if</span> (need_wakeup)
		vm_map_entry_wakeup(map);

	<span class="enscript-keyword">if</span> (rc != KERN_SUCCESS) {
		<span class="enscript-comment">/* undo what has been wired so far */</span>
		vm_map_unwire(map, start, s, user_wire);
		<span class="enscript-keyword">if</span> (physpage_p) {
			*physpage_p = 0;
		}
	}

	<span class="enscript-keyword">return</span> rc;

}

kern_return_t
<span class="enscript-function-name">vm_map_wire_external</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	<span class="enscript-type">register</span> vm_prot_t	caller_prot,
	boolean_t		user_wire)
{
	kern_return_t	kret;

	caller_prot &amp;= ~VM_PROT_MEMORY_TAG_MASK;
	caller_prot |= VM_PROT_MEMORY_TAG_MAKE(vm_tag_bt());
	kret = vm_map_wire_nested(map, start, end, caller_prot, 
				  user_wire, (pmap_t)NULL, 0, NULL);
	<span class="enscript-keyword">return</span> kret;
}

kern_return_t
<span class="enscript-function-name">vm_map_wire</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	<span class="enscript-type">register</span> vm_prot_t	caller_prot,
	boolean_t		user_wire)
{
	kern_return_t	kret;

	kret = vm_map_wire_nested(map, start, end, caller_prot, 
				  user_wire, (pmap_t)NULL, 0, NULL);
	<span class="enscript-keyword">return</span> kret;
}

kern_return_t
<span class="enscript-function-name">vm_map_wire_and_extract_external</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_prot_t	caller_prot,
	boolean_t	user_wire,
	ppnum_t		*physpage_p)
{
	kern_return_t	kret;

	caller_prot &amp;= ~VM_PROT_MEMORY_TAG_MASK;
	caller_prot |= VM_PROT_MEMORY_TAG_MAKE(vm_tag_bt());
	kret = vm_map_wire_nested(map,
				  start,
				  start+VM_MAP_PAGE_SIZE(map),
				  caller_prot, 
				  user_wire,
				  (pmap_t)NULL,
				  0,
				  physpage_p);
	<span class="enscript-keyword">if</span> (kret != KERN_SUCCESS &amp;&amp;
	    physpage_p != NULL) {
		*physpage_p = 0;
	}
	<span class="enscript-keyword">return</span> kret;
}

kern_return_t
<span class="enscript-function-name">vm_map_wire_and_extract</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_prot_t	caller_prot,
	boolean_t	user_wire,
	ppnum_t		*physpage_p)
{
	kern_return_t	kret;

	kret = vm_map_wire_nested(map,
				  start,
				  start+VM_MAP_PAGE_SIZE(map),
				  caller_prot, 
				  user_wire,
				  (pmap_t)NULL,
				  0,
				  physpage_p);
	<span class="enscript-keyword">if</span> (kret != KERN_SUCCESS &amp;&amp;
	    physpage_p != NULL) {
		*physpage_p = 0;
	}
	<span class="enscript-keyword">return</span> kret;
}

<span class="enscript-comment">/*
 *	vm_map_unwire:
 *
 *	Sets the pageability of the specified address range in the target
 *	as pageable.  Regions specified must have been wired previously.
 *
 *	The map must not be locked, but a reference must remain to the map
 *	throughout the call.
 *
 *	Kernel will panic on failures.  User unwire ignores holes and
 *	unwired and intransition entries to avoid losing memory by leaving
 *	it unwired.
 */</span>
<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_unwire_nested</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	boolean_t		user_wire,
	pmap_t			map_pmap,
	vm_map_offset_t		pmap_addr)
{
	<span class="enscript-type">register</span> vm_map_entry_t	entry;
	<span class="enscript-type">struct</span> vm_map_entry	*first_entry, tmp_entry;
	boolean_t		need_wakeup;
	boolean_t		main_map = FALSE;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		last_timestamp;

	vm_map_lock(map);
	<span class="enscript-keyword">if</span>(map_pmap == NULL)
		main_map = TRUE;
	last_timestamp = map-&gt;timestamp;

	VM_MAP_RANGE_CHECK(map, start, end);
	assert(page_aligned(start));
	assert(page_aligned(end));
	assert(VM_MAP_PAGE_ALIGNED(start, VM_MAP_PAGE_MASK(map)));
	assert(VM_MAP_PAGE_ALIGNED(end, VM_MAP_PAGE_MASK(map)));

	<span class="enscript-keyword">if</span> (start == end) {
		<span class="enscript-comment">/* We unwired what the caller asked for: zero pages */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> KERN_SUCCESS;
	}

	<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;first_entry)) {
		entry = first_entry;
		<span class="enscript-comment">/*
		 * vm_map_clip_start will be done later.
		 * We don't want to unnest any nested sub maps here !
		 */</span>
	}
	<span class="enscript-keyword">else</span> {
		<span class="enscript-keyword">if</span> (!user_wire) {
			panic(<span class="enscript-string">&quot;vm_map_unwire: start not found&quot;</span>);
		}
		<span class="enscript-comment">/*	Start address is not in map. */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}

	<span class="enscript-keyword">if</span> (entry-&gt;superpage_size) {
		<span class="enscript-comment">/* superpages are always wired */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}

	need_wakeup = FALSE;
	<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp; (entry-&gt;vme_start &lt; end)) {
		<span class="enscript-keyword">if</span> (entry-&gt;in_transition) {
			<span class="enscript-comment">/*
			 * 1)
			 * Another thread is wiring down this entry. Note
			 * that if it is not for the other thread we would
			 * be unwiring an unwired entry.  This is not
			 * permitted.  If we wait, we will be unwiring memory
			 * we did not wire.
			 *
			 * 2)
			 * Another thread is unwiring this entry.  We did not
			 * have a reference to it, because if we did, this
			 * entry will not be getting unwired now.
			 */</span>
			<span class="enscript-keyword">if</span> (!user_wire) {
				<span class="enscript-comment">/*
				 * XXX FBDP
				 * This could happen:  there could be some
				 * overlapping vslock/vsunlock operations
				 * going on.
				 * We should probably just wait and retry,
				 * but then we have to be careful that this
				 * entry could get &quot;simplified&quot; after 
				 * &quot;in_transition&quot; gets unset and before
				 * we re-lookup the entry, so we would
				 * have to re-clip the entry to avoid
				 * re-unwiring what we have already unwired...
				 * See vm_map_wire_nested().
				 *
				 * Or we could just ignore &quot;in_transition&quot;
				 * here and proceed to decement the wired
				 * count(s) on this entry.  That should be fine
				 * as long as &quot;wired_count&quot; doesn't drop all
				 * the way to 0 (and we should panic if THAT
				 * happens).
				 */</span>
				panic(<span class="enscript-string">&quot;vm_map_unwire: in_transition entry&quot;</span>);
			}

			entry = entry-&gt;vme_next;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			vm_map_offset_t	sub_start;
			vm_map_offset_t	sub_end;
			vm_map_offset_t	local_end;
			pmap_t		pmap;
		
			vm_map_clip_start(map, entry, start);
			vm_map_clip_end(map, entry, end);

			sub_start = VME_OFFSET(entry);
			sub_end = entry-&gt;vme_end - entry-&gt;vme_start;
			sub_end += VME_OFFSET(entry);
			local_end = entry-&gt;vme_end;
			<span class="enscript-keyword">if</span>(map_pmap == NULL) {
				<span class="enscript-keyword">if</span>(entry-&gt;use_pmap) {
					pmap = VME_SUBMAP(entry)-&gt;pmap;
					pmap_addr = sub_start;
				} <span class="enscript-keyword">else</span> {
					pmap = map-&gt;pmap;
					pmap_addr = start;
				}
				<span class="enscript-keyword">if</span> (entry-&gt;wired_count == 0 ||
				    (user_wire &amp;&amp; entry-&gt;user_wired_count == 0)) {
					<span class="enscript-keyword">if</span> (!user_wire)
						panic(<span class="enscript-string">&quot;vm_map_unwire: entry is unwired&quot;</span>);
					entry = entry-&gt;vme_next;
					<span class="enscript-keyword">continue</span>;
				}

				<span class="enscript-comment">/*
				 * Check for holes
				 * Holes: Next entry should be contiguous unless
				 * this is the end of the region.
				 */</span>
				<span class="enscript-keyword">if</span> (((entry-&gt;vme_end &lt; end) &amp;&amp; 
				     ((entry-&gt;vme_next == vm_map_to_entry(map)) ||
				      (entry-&gt;vme_next-&gt;vme_start 
				       &gt; entry-&gt;vme_end)))) {
					<span class="enscript-keyword">if</span> (!user_wire)
						panic(<span class="enscript-string">&quot;vm_map_unwire: non-contiguous region&quot;</span>);
<span class="enscript-comment">/*
					entry = entry-&gt;vme_next;
					continue;
*/</span>
				}

				subtract_wire_counts(map, entry, user_wire);

				<span class="enscript-keyword">if</span> (entry-&gt;wired_count != 0) {
					entry = entry-&gt;vme_next;
					<span class="enscript-keyword">continue</span>;
				}

				entry-&gt;in_transition = TRUE;
				tmp_entry = *entry;<span class="enscript-comment">/* see comment in vm_map_wire() */</span>

				<span class="enscript-comment">/*
				 * We can unlock the map now. The in_transition state
				 * guarantees existance of the entry.
				 */</span>
				vm_map_unlock(map);
				vm_map_unwire_nested(VME_SUBMAP(entry), 
						     sub_start, sub_end, user_wire, pmap, pmap_addr);
				vm_map_lock(map);

				<span class="enscript-keyword">if</span> (last_timestamp+1 != map-&gt;timestamp) {
					<span class="enscript-comment">/*
					 * Find the entry again.  It could have been 
					 * clipped or deleted after we unlocked the map.
					 */</span>
					<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, 
								 tmp_entry.vme_start,
								 &amp;first_entry)) {
						<span class="enscript-keyword">if</span> (!user_wire)
							panic(<span class="enscript-string">&quot;vm_map_unwire: re-lookup failed&quot;</span>);
						entry = first_entry-&gt;vme_next;
					} <span class="enscript-keyword">else</span>
						entry = first_entry;
				}
				last_timestamp = map-&gt;timestamp;

				<span class="enscript-comment">/*
				 * clear transition bit for all constituent entries
				 * that were in the original entry (saved in 
				 * tmp_entry).  Also check for waiters.
				 */</span>
				<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp;
				       (entry-&gt;vme_start &lt; tmp_entry.vme_end)) {
					assert(entry-&gt;in_transition);
					entry-&gt;in_transition = FALSE;
					<span class="enscript-keyword">if</span> (entry-&gt;needs_wakeup) {
						entry-&gt;needs_wakeup = FALSE;
						need_wakeup = TRUE;
					}
					entry = entry-&gt;vme_next;
				}
				<span class="enscript-keyword">continue</span>;
			} <span class="enscript-keyword">else</span> {
				vm_map_unlock(map);
				vm_map_unwire_nested(VME_SUBMAP(entry),
						     sub_start, sub_end, user_wire, map_pmap,
						     pmap_addr);
				vm_map_lock(map);

				<span class="enscript-keyword">if</span> (last_timestamp+1 != map-&gt;timestamp) {
					<span class="enscript-comment">/*
					 * Find the entry again.  It could have been 
					 * clipped or deleted after we unlocked the map.
					 */</span>
					<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, 
								 tmp_entry.vme_start,
								 &amp;first_entry)) {
						<span class="enscript-keyword">if</span> (!user_wire)
							panic(<span class="enscript-string">&quot;vm_map_unwire: re-lookup failed&quot;</span>);
						entry = first_entry-&gt;vme_next;
					} <span class="enscript-keyword">else</span>
						entry = first_entry;
				}
				last_timestamp = map-&gt;timestamp;
			}
		}


		<span class="enscript-keyword">if</span> ((entry-&gt;wired_count == 0) ||
		    (user_wire &amp;&amp; entry-&gt;user_wired_count == 0)) {
			<span class="enscript-keyword">if</span> (!user_wire)
				panic(<span class="enscript-string">&quot;vm_map_unwire: entry is unwired&quot;</span>);

			entry = entry-&gt;vme_next;
			<span class="enscript-keyword">continue</span>;
		}
	
		assert(entry-&gt;wired_count &gt; 0 &amp;&amp;
		       (!user_wire || entry-&gt;user_wired_count &gt; 0));

		vm_map_clip_start(map, entry, start);
		vm_map_clip_end(map, entry, end);

		<span class="enscript-comment">/*
		 * Check for holes
		 * Holes: Next entry should be contiguous unless
		 *	  this is the end of the region.
		 */</span>
		<span class="enscript-keyword">if</span> (((entry-&gt;vme_end &lt; end) &amp;&amp; 
		     ((entry-&gt;vme_next == vm_map_to_entry(map)) ||
		      (entry-&gt;vme_next-&gt;vme_start &gt; entry-&gt;vme_end)))) {

			<span class="enscript-keyword">if</span> (!user_wire)
				panic(<span class="enscript-string">&quot;vm_map_unwire: non-contiguous region&quot;</span>);
			entry = entry-&gt;vme_next;
			<span class="enscript-keyword">continue</span>;
		}

		subtract_wire_counts(map, entry, user_wire);

		<span class="enscript-keyword">if</span> (entry-&gt;wired_count != 0) {
			entry = entry-&gt;vme_next;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-keyword">if</span>(entry-&gt;zero_wired_pages) {
			entry-&gt;zero_wired_pages = FALSE;
		}

		entry-&gt;in_transition = TRUE;
		tmp_entry = *entry;	<span class="enscript-comment">/* see comment in vm_map_wire() */</span>

		<span class="enscript-comment">/*
		 * We can unlock the map now. The in_transition state
		 * guarantees existance of the entry.
		 */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">if</span>(map_pmap) {
			vm_fault_unwire(map, 
					&amp;tmp_entry, FALSE, map_pmap, pmap_addr);
		} <span class="enscript-keyword">else</span> {
			vm_fault_unwire(map, 
					&amp;tmp_entry, FALSE, map-&gt;pmap, 
					tmp_entry.vme_start);
		}
		vm_map_lock(map);

		<span class="enscript-keyword">if</span> (last_timestamp+1 != map-&gt;timestamp) {
			<span class="enscript-comment">/*
			 * Find the entry again.  It could have been clipped
			 * or deleted after we unlocked the map.
			 */</span>
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, tmp_entry.vme_start,
						 &amp;first_entry)) {
				<span class="enscript-keyword">if</span> (!user_wire)
					panic(<span class="enscript-string">&quot;vm_map_unwire: re-lookup failed&quot;</span>);
				entry = first_entry-&gt;vme_next;
			} <span class="enscript-keyword">else</span>
				entry = first_entry;
		}
		last_timestamp = map-&gt;timestamp;

		<span class="enscript-comment">/*
		 * clear transition bit for all constituent entries that
		 * were in the original entry (saved in tmp_entry).  Also
		 * check for waiters.
		 */</span>
		<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp;
		       (entry-&gt;vme_start &lt; tmp_entry.vme_end)) {
			assert(entry-&gt;in_transition);
			entry-&gt;in_transition = FALSE;
			<span class="enscript-keyword">if</span> (entry-&gt;needs_wakeup) {
				entry-&gt;needs_wakeup = FALSE;
				need_wakeup = TRUE;
			}
			entry = entry-&gt;vme_next;
		}
	}

	<span class="enscript-comment">/*
	 * We might have fragmented the address space when we wired this
	 * range of addresses.  Attempt to re-coalesce these VM map entries
	 * with their neighbors now that they're no longer wired.
	 * Under some circumstances, address space fragmentation can
	 * prevent VM object shadow chain collapsing, which can cause
	 * swap space leaks.
	 */</span>
	vm_map_simplify_range(map, start, end);

	vm_map_unlock(map);
	<span class="enscript-comment">/*
	 * wake up anybody waiting on entries that we have unwired.
	 */</span>
	<span class="enscript-keyword">if</span> (need_wakeup)
		vm_map_entry_wakeup(map);
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);

}

kern_return_t
<span class="enscript-function-name">vm_map_unwire</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	boolean_t		user_wire)
{
	<span class="enscript-keyword">return</span> vm_map_unwire_nested(map, start, end, 
				    user_wire, (pmap_t)NULL, 0);
}


<span class="enscript-comment">/*
 *	vm_map_entry_delete:	[ internal use only ]
 *
 *	Deallocate the given entry from the target map.
 */</span>		
<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_entry_delete</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_entry_t	entry)
{
	<span class="enscript-type">register</span> vm_map_offset_t	s, e;
	<span class="enscript-type">register</span> vm_object_t	object;
	<span class="enscript-type">register</span> vm_map_t	submap;

	s = entry-&gt;vme_start;
	e = entry-&gt;vme_end;
	assert(page_aligned(s));
	assert(page_aligned(e));
	<span class="enscript-keyword">if</span> (entry-&gt;map_aligned == TRUE) {
		assert(VM_MAP_PAGE_ALIGNED(s, VM_MAP_PAGE_MASK(map)));
		assert(VM_MAP_PAGE_ALIGNED(e, VM_MAP_PAGE_MASK(map)));
	}
	assert(entry-&gt;wired_count == 0);
	assert(entry-&gt;user_wired_count == 0);
	assert(!entry-&gt;permanent);

	<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
		object = NULL;
		submap = VME_SUBMAP(entry);
	} <span class="enscript-keyword">else</span> {
		submap = NULL;
		object = VME_OBJECT(entry);
	}

	vm_map_store_entry_unlink(map, entry);
	map-&gt;size -= e - s;

	vm_map_entry_dispose(map, entry);

	vm_map_unlock(map);
	<span class="enscript-comment">/*
	 *	Deallocate the object only after removing all
	 *	pmap entries pointing to its pages.
	 */</span>
	<span class="enscript-keyword">if</span> (submap)
		vm_map_deallocate(submap);
	<span class="enscript-keyword">else</span>
		vm_object_deallocate(object);

}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_submap_pmap_clean</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end,
	vm_map_t	sub_map,
	vm_map_offset_t	offset)
{
	vm_map_offset_t	submap_start;
	vm_map_offset_t	submap_end;
	vm_map_size_t	remove_size;
	vm_map_entry_t	entry;

	submap_end = offset + (end - start);
	submap_start = offset;

	vm_map_lock_read(sub_map);
	<span class="enscript-keyword">if</span>(vm_map_lookup_entry(sub_map, offset, &amp;entry)) {
		
		remove_size = (entry-&gt;vme_end - entry-&gt;vme_start);
		<span class="enscript-keyword">if</span>(offset &gt; entry-&gt;vme_start)
			remove_size -= offset - entry-&gt;vme_start;
		

		<span class="enscript-keyword">if</span>(submap_end &lt; entry-&gt;vme_end) {
			remove_size -=
				entry-&gt;vme_end - submap_end;
		}
		<span class="enscript-keyword">if</span>(entry-&gt;is_sub_map) {
			vm_map_submap_pmap_clean(
				sub_map,
				start,
				start + remove_size,
				VME_SUBMAP(entry),
				VME_OFFSET(entry));
		} <span class="enscript-keyword">else</span> {

			<span class="enscript-keyword">if</span>((map-&gt;mapped_in_other_pmaps) &amp;&amp; (map-&gt;ref_count)
			   &amp;&amp; (VME_OBJECT(entry) != NULL)) {
				vm_object_pmap_protect_options(
					VME_OBJECT(entry),
					(VME_OFFSET(entry) +
					 offset -
					 entry-&gt;vme_start),
					remove_size,
					PMAP_NULL,
					entry-&gt;vme_start,
					VM_PROT_NONE,
					PMAP_OPTIONS_REMOVE);
			} <span class="enscript-keyword">else</span> {
				pmap_remove(map-&gt;pmap, 
					    (addr64_t)start, 
					    (addr64_t)(start + remove_size));
			}
		}
	}

	entry = entry-&gt;vme_next;

	<span class="enscript-keyword">while</span>((entry != vm_map_to_entry(sub_map)) 
	      &amp;&amp; (entry-&gt;vme_start &lt; submap_end)) {
		remove_size = (entry-&gt;vme_end - entry-&gt;vme_start); 
		<span class="enscript-keyword">if</span>(submap_end &lt; entry-&gt;vme_end) {
			remove_size -= entry-&gt;vme_end - submap_end;
		}
		<span class="enscript-keyword">if</span>(entry-&gt;is_sub_map) {
			vm_map_submap_pmap_clean(
				sub_map,
				(start + entry-&gt;vme_start) - offset,
				((start + entry-&gt;vme_start) - offset) + remove_size,
				VME_SUBMAP(entry),
				VME_OFFSET(entry));
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span>((map-&gt;mapped_in_other_pmaps) &amp;&amp; (map-&gt;ref_count)
			   &amp;&amp; (VME_OBJECT(entry) != NULL)) {
				vm_object_pmap_protect_options(
					VME_OBJECT(entry),
					VME_OFFSET(entry),
					remove_size,
					PMAP_NULL,
					entry-&gt;vme_start,
					VM_PROT_NONE,
					PMAP_OPTIONS_REMOVE);
			} <span class="enscript-keyword">else</span> {
				pmap_remove(map-&gt;pmap, 
					    (addr64_t)((start + entry-&gt;vme_start) 
						       - offset),
					    (addr64_t)(((start + entry-&gt;vme_start) 
							- offset) + remove_size));
			}
		}
		entry = entry-&gt;vme_next;
	}
	vm_map_unlock_read(sub_map);
	<span class="enscript-keyword">return</span>;
}

<span class="enscript-comment">/*
 *	vm_map_delete:	[ internal use only ]
 *
 *	Deallocates the given address range from the target map.
 *	Removes all user wirings. Unwires one kernel wiring if
 *	VM_MAP_REMOVE_KUNWIRE is set.  Waits for kernel wirings to go
 *	away if VM_MAP_REMOVE_WAIT_FOR_KWIRE is set.  Sleeps
 *	interruptibly if VM_MAP_REMOVE_INTERRUPTIBLE is set.
 *
 *	This routine is called with map locked and leaves map locked.
 */</span>
<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_delete</span>(
	vm_map_t		map,
	vm_map_offset_t		start,
	vm_map_offset_t		end,
	<span class="enscript-type">int</span>			flags,
	vm_map_t		zap_map)
{
	vm_map_entry_t		entry, next;
	<span class="enscript-type">struct</span>	 vm_map_entry	*first_entry, tmp_entry;
	<span class="enscript-type">register</span> vm_map_offset_t s;
	<span class="enscript-type">register</span> vm_object_t	object;
	boolean_t		need_wakeup;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		last_timestamp = ~0; <span class="enscript-comment">/* unlikely value */</span>
	<span class="enscript-type">int</span>			interruptible;

	interruptible = (flags &amp; VM_MAP_REMOVE_INTERRUPTIBLE) ? 
		THREAD_ABORTSAFE : THREAD_UNINT;

	<span class="enscript-comment">/*
	 * All our DMA I/O operations in IOKit are currently done by
	 * wiring through the map entries of the task requesting the I/O.
	 * Because of this, we must always wait for kernel wirings
	 * to go away on the entries before deleting them.
	 *
	 * Any caller who wants to actually remove a kernel wiring
	 * should explicitly set the VM_MAP_REMOVE_KUNWIRE flag to
	 * properly remove one wiring instead of blasting through
	 * them all.
	 */</span>
	flags |= VM_MAP_REMOVE_WAIT_FOR_KWIRE;

	<span class="enscript-keyword">while</span>(1) {
		<span class="enscript-comment">/*
		 *	Find the start of the region, and clip it
		 */</span>
		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;first_entry)) {
			entry = first_entry;
			<span class="enscript-keyword">if</span> (map == kalloc_map &amp;&amp;
			    (entry-&gt;vme_start != start ||
			     entry-&gt;vme_end != end)) {
				panic(<span class="enscript-string">&quot;vm_map_delete(%p,0x%llx,0x%llx): &quot;</span>
				      <span class="enscript-string">&quot;mismatched entry %p [0x%llx:0x%llx]\n&quot;</span>,
				      map,
				      (uint64_t)start,
				      (uint64_t)end,
				      entry,
				      (uint64_t)entry-&gt;vme_start,
				      (uint64_t)entry-&gt;vme_end);
			}
			<span class="enscript-keyword">if</span> (entry-&gt;superpage_size &amp;&amp; (start &amp; ~SUPERPAGE_MASK)) { <span class="enscript-comment">/* extend request to whole entry */</span>				start = SUPERPAGE_ROUND_DOWN(start);
				start = SUPERPAGE_ROUND_DOWN(start);
				<span class="enscript-keyword">continue</span>;
			}
			<span class="enscript-keyword">if</span> (start == entry-&gt;vme_start) {
				<span class="enscript-comment">/*
				 * No need to clip.  We don't want to cause
				 * any unnecessary unnesting in this case...
				 */</span>
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-keyword">if</span> ((flags &amp; VM_MAP_REMOVE_NO_MAP_ALIGN) &amp;&amp;
				    entry-&gt;map_aligned &amp;&amp;
				    !VM_MAP_PAGE_ALIGNED(
					    start,
					    VM_MAP_PAGE_MASK(map))) {
					<span class="enscript-comment">/*
					 * The entry will no longer be
					 * map-aligned after clipping
					 * and the caller said it's OK.
					 */</span>
					entry-&gt;map_aligned = FALSE;
				}
				<span class="enscript-keyword">if</span> (map == kalloc_map) {
					panic(<span class="enscript-string">&quot;vm_map_delete(%p,0x%llx,0x%llx):&quot;</span>
					      <span class="enscript-string">&quot; clipping %p at 0x%llx\n&quot;</span>,
					      map,
					      (uint64_t)start,
					      (uint64_t)end,
					      entry,
					      (uint64_t)start);
				}
				vm_map_clip_start(map, entry, start);
			}

			<span class="enscript-comment">/*
			 *	Fix the lookup hint now, rather than each
			 *	time through the loop.
			 */</span>
			SAVE_HINT_MAP_WRITE(map, entry-&gt;vme_prev);
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> (map-&gt;pmap == kernel_pmap &amp;&amp;
			    map-&gt;ref_count != 0) {
				panic(<span class="enscript-string">&quot;vm_map_delete(%p,0x%llx,0x%llx): &quot;</span>
				      <span class="enscript-string">&quot;no map entry at 0x%llx\n&quot;</span>,
				      map,
				      (uint64_t)start,
				      (uint64_t)end,
				      (uint64_t)start);
			}
			entry = first_entry-&gt;vme_next;
		}
		<span class="enscript-keyword">break</span>;
	}
	<span class="enscript-keyword">if</span> (entry-&gt;superpage_size)
		end = SUPERPAGE_ROUND_UP(end);

	need_wakeup = FALSE;
	<span class="enscript-comment">/*
	 *	Step through all entries in this region
	 */</span>
	s = entry-&gt;vme_start;
	<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp; (s &lt; end)) {
		<span class="enscript-comment">/*
		 * At this point, we have deleted all the memory entries
		 * between &quot;start&quot; and &quot;s&quot;.  We still need to delete
		 * all memory entries between &quot;s&quot; and &quot;end&quot;.
		 * While we were blocked and the map was unlocked, some
		 * new memory entries could have been re-allocated between
		 * &quot;start&quot; and &quot;s&quot; and we don't want to mess with those.
		 * Some of those entries could even have been re-assembled
		 * with an entry after &quot;s&quot; (in vm_map_simplify_entry()), so
		 * we may have to vm_map_clip_start() again.
		 */</span>

		<span class="enscript-keyword">if</span> (entry-&gt;vme_start &gt;= s) {
			<span class="enscript-comment">/*
			 * This entry starts on or after &quot;s&quot;
			 * so no need to clip its start.
			 */</span>
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/*
			 * This entry has been re-assembled by a
			 * vm_map_simplify_entry().  We need to
			 * re-clip its start.
			 */</span>
			<span class="enscript-keyword">if</span> ((flags &amp; VM_MAP_REMOVE_NO_MAP_ALIGN) &amp;&amp;
			    entry-&gt;map_aligned &amp;&amp;
			    !VM_MAP_PAGE_ALIGNED(s,
						 VM_MAP_PAGE_MASK(map))) {
				<span class="enscript-comment">/*
				 * The entry will no longer be map-aligned
				 * after clipping and the caller said it's OK.
				 */</span>
				entry-&gt;map_aligned = FALSE;
			}
			<span class="enscript-keyword">if</span> (map == kalloc_map) {
				panic(<span class="enscript-string">&quot;vm_map_delete(%p,0x%llx,0x%llx): &quot;</span>
				      <span class="enscript-string">&quot;clipping %p at 0x%llx\n&quot;</span>,
				      map,
				      (uint64_t)start,
				      (uint64_t)end,
				      entry,
				      (uint64_t)s);
			}
			vm_map_clip_start(map, entry, s);
		}
		<span class="enscript-keyword">if</span> (entry-&gt;vme_end &lt;= end) {
			<span class="enscript-comment">/*
			 * This entry is going away completely, so no need
			 * to clip and possibly cause an unnecessary unnesting.
			 */</span>
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> ((flags &amp; VM_MAP_REMOVE_NO_MAP_ALIGN) &amp;&amp;
			    entry-&gt;map_aligned &amp;&amp;
			    !VM_MAP_PAGE_ALIGNED(end,
						 VM_MAP_PAGE_MASK(map))) {
				<span class="enscript-comment">/*
				 * The entry will no longer be map-aligned
				 * after clipping and the caller said it's OK.
				 */</span>
				entry-&gt;map_aligned = FALSE;
			}
			<span class="enscript-keyword">if</span> (map == kalloc_map) {
				panic(<span class="enscript-string">&quot;vm_map_delete(%p,0x%llx,0x%llx): &quot;</span>
				      <span class="enscript-string">&quot;clipping %p at 0x%llx\n&quot;</span>,
				      map,
				      (uint64_t)start,
				      (uint64_t)end,
				      entry,
				      (uint64_t)end);
			}
			vm_map_clip_end(map, entry, end);
		}

		<span class="enscript-keyword">if</span> (entry-&gt;permanent) {
			panic(<span class="enscript-string">&quot;attempt to remove permanent VM map entry &quot;</span>
			      <span class="enscript-string">&quot;%p [0x%llx:0x%llx]\n&quot;</span>,
			      entry, (uint64_t) s, (uint64_t) end);
		}


		<span class="enscript-keyword">if</span> (entry-&gt;in_transition) {
			wait_result_t wait_result;

			<span class="enscript-comment">/*
			 * Another thread is wiring/unwiring this entry.
			 * Let the other thread know we are waiting.
			 */</span>
			assert(s == entry-&gt;vme_start);
			entry-&gt;needs_wakeup = TRUE;

			<span class="enscript-comment">/*
			 * wake up anybody waiting on entries that we have
			 * already unwired/deleted.
			 */</span>
			<span class="enscript-keyword">if</span> (need_wakeup) {
				vm_map_entry_wakeup(map);
				need_wakeup = FALSE;
			}

			wait_result = vm_map_entry_wait(map, interruptible);

			<span class="enscript-keyword">if</span> (interruptible &amp;&amp;
			    wait_result == THREAD_INTERRUPTED) {
				<span class="enscript-comment">/*
				 * We do not clear the needs_wakeup flag,
				 * since we cannot tell if we were the only one.
				 */</span>
				<span class="enscript-keyword">return</span> KERN_ABORTED;
			}

			<span class="enscript-comment">/*
			 * The entry could have been clipped or it
			 * may not exist anymore.  Look it up again.
			 */</span>
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, s, &amp;first_entry)) {
				<span class="enscript-comment">/*
				 * User: use the next entry
				 */</span>
				entry = first_entry-&gt;vme_next;
				s = entry-&gt;vme_start;
			} <span class="enscript-keyword">else</span> {
				entry = first_entry;
				SAVE_HINT_MAP_WRITE(map, entry-&gt;vme_prev);
			}
			last_timestamp = map-&gt;timestamp;
			<span class="enscript-keyword">continue</span>;
		} <span class="enscript-comment">/* end in_transition */</span>

		<span class="enscript-keyword">if</span> (entry-&gt;wired_count) {
			boolean_t	user_wire;

			user_wire = entry-&gt;user_wired_count &gt; 0;

			<span class="enscript-comment">/*
			 * 	Remove a kernel wiring if requested
			 */</span>
			<span class="enscript-keyword">if</span> (flags &amp; VM_MAP_REMOVE_KUNWIRE) {
				entry-&gt;wired_count--;
			}
			
			<span class="enscript-comment">/*
			 *	Remove all user wirings for proper accounting
			 */</span>
			<span class="enscript-keyword">if</span> (entry-&gt;user_wired_count &gt; 0) {
				<span class="enscript-keyword">while</span> (entry-&gt;user_wired_count)
					subtract_wire_counts(map, entry, user_wire);
			}

			<span class="enscript-keyword">if</span> (entry-&gt;wired_count != 0) {
				assert(map != kernel_map);
				<span class="enscript-comment">/*
				 * Cannot continue.  Typical case is when
				 * a user thread has physical io pending on
				 * on this page.  Either wait for the
				 * kernel wiring to go away or return an
				 * error.
				 */</span>
				<span class="enscript-keyword">if</span> (flags &amp; VM_MAP_REMOVE_WAIT_FOR_KWIRE) {
					wait_result_t wait_result;

					assert(s == entry-&gt;vme_start);
					entry-&gt;needs_wakeup = TRUE;
					wait_result = vm_map_entry_wait(map,
									interruptible);

					<span class="enscript-keyword">if</span> (interruptible &amp;&amp;
					    wait_result == THREAD_INTERRUPTED) {
						<span class="enscript-comment">/*
						 * We do not clear the 
						 * needs_wakeup flag, since we 
						 * cannot tell if we were the 
						 * only one.
						 */</span>
						<span class="enscript-keyword">return</span> KERN_ABORTED;
					}

					<span class="enscript-comment">/*
					 * The entry could have been clipped or
					 * it may not exist anymore.  Look it
					 * up again.
					 */</span>
					<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, s, 
								 &amp;first_entry)) {
						assert(map != kernel_map);
						<span class="enscript-comment">/*
						 * User: use the next entry
						 */</span>
						entry = first_entry-&gt;vme_next;
						s = entry-&gt;vme_start;
					} <span class="enscript-keyword">else</span> {
						entry = first_entry;
						SAVE_HINT_MAP_WRITE(map, entry-&gt;vme_prev);
					}
					last_timestamp = map-&gt;timestamp;
					<span class="enscript-keyword">continue</span>;
				}
				<span class="enscript-keyword">else</span> {
					<span class="enscript-keyword">return</span> KERN_FAILURE;
				}
			}

			entry-&gt;in_transition = TRUE;
			<span class="enscript-comment">/*
			 * copy current entry.  see comment in vm_map_wire()
			 */</span>
			tmp_entry = *entry;
			assert(s == entry-&gt;vme_start);

			<span class="enscript-comment">/*
			 * We can unlock the map now. The in_transition
			 * state guarentees existance of the entry.
			 */</span>
			vm_map_unlock(map);

			<span class="enscript-keyword">if</span> (tmp_entry.is_sub_map) {
				vm_map_t sub_map;
				vm_map_offset_t sub_start, sub_end;
				pmap_t pmap;
				vm_map_offset_t pmap_addr;
				

				sub_map = VME_SUBMAP(&amp;tmp_entry);
				sub_start = VME_OFFSET(&amp;tmp_entry);
				sub_end = sub_start + (tmp_entry.vme_end -
						       tmp_entry.vme_start);
				<span class="enscript-keyword">if</span> (tmp_entry.use_pmap) {
					pmap = sub_map-&gt;pmap;
					pmap_addr = tmp_entry.vme_start;
				} <span class="enscript-keyword">else</span> {
					pmap = map-&gt;pmap;
					pmap_addr = tmp_entry.vme_start;
				}
				(<span class="enscript-type">void</span>) vm_map_unwire_nested(sub_map,
							    sub_start, sub_end,
							    user_wire,
							    pmap, pmap_addr);
			} <span class="enscript-keyword">else</span> {

				<span class="enscript-keyword">if</span> (VME_OBJECT(&amp;tmp_entry) == kernel_object) {
					pmap_protect_options(
						map-&gt;pmap,
						tmp_entry.vme_start,
						tmp_entry.vme_end,
						VM_PROT_NONE,
						PMAP_OPTIONS_REMOVE,
						NULL);
				}
				vm_fault_unwire(map, &amp;tmp_entry,
						VME_OBJECT(&amp;tmp_entry) == kernel_object,
						map-&gt;pmap, tmp_entry.vme_start);
			}

			vm_map_lock(map);

			<span class="enscript-keyword">if</span> (last_timestamp+1 != map-&gt;timestamp) {
				<span class="enscript-comment">/*
				 * Find the entry again.  It could have
				 * been clipped after we unlocked the map.
				 */</span>
				<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, s, &amp;first_entry)){
					assert((map != kernel_map) &amp;&amp; 
					       (!entry-&gt;is_sub_map));
					first_entry = first_entry-&gt;vme_next;
					s = first_entry-&gt;vme_start;
				} <span class="enscript-keyword">else</span> {
					SAVE_HINT_MAP_WRITE(map, entry-&gt;vme_prev);
				}
			} <span class="enscript-keyword">else</span> {
				SAVE_HINT_MAP_WRITE(map, entry-&gt;vme_prev);
				first_entry = entry;
			}

			last_timestamp = map-&gt;timestamp;

			entry = first_entry;
			<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp;
			       (entry-&gt;vme_start &lt; tmp_entry.vme_end)) {
				assert(entry-&gt;in_transition);
				entry-&gt;in_transition = FALSE;
				<span class="enscript-keyword">if</span> (entry-&gt;needs_wakeup) {
					entry-&gt;needs_wakeup = FALSE;
					need_wakeup = TRUE;
				}
				entry = entry-&gt;vme_next;
			}
			<span class="enscript-comment">/*
			 * We have unwired the entry(s).  Go back and
			 * delete them.
			 */</span>
			entry = first_entry;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-comment">/* entry is unwired */</span>
		assert(entry-&gt;wired_count == 0);
		assert(entry-&gt;user_wired_count == 0);

		assert(s == entry-&gt;vme_start);

		<span class="enscript-keyword">if</span> (flags &amp; VM_MAP_REMOVE_NO_PMAP_CLEANUP) {
			<span class="enscript-comment">/*
			 * XXX with the VM_MAP_REMOVE_SAVE_ENTRIES flag to
			 * vm_map_delete(), some map entries might have been
			 * transferred to a &quot;zap_map&quot;, which doesn't have a
			 * pmap.  The original pmap has already been flushed
			 * in the vm_map_delete() call targeting the original
			 * map, but when we get to destroying the &quot;zap_map&quot;,
			 * we don't have any pmap to flush, so let's just skip
			 * all this.
			 */</span>
		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			<span class="enscript-keyword">if</span> (entry-&gt;use_pmap) {
#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
				<span class="enscript-type">int</span> pmap_flags;

				<span class="enscript-keyword">if</span> (flags &amp; VM_MAP_REMOVE_NO_UNNESTING) {
					<span class="enscript-comment">/*
					 * This is the final cleanup of the
					 * address space being terminated.
					 * No new mappings are expected and
					 * we don't really need to unnest the
					 * shared region (and lose the &quot;global&quot;
					 * pmap mappings, if applicable).
					 *
					 * Tell the pmap layer that we're
					 * &quot;clean&quot; wrt nesting.
					 */</span>
					pmap_flags = PMAP_UNNEST_CLEAN;
				} <span class="enscript-keyword">else</span> {
					<span class="enscript-comment">/*
					 * We're unmapping part of the nested
					 * shared region, so we can't keep the
					 * nested pmap.
					 */</span>
					pmap_flags = 0;
				}
				pmap_unnest_options(
					map-&gt;pmap,
					(addr64_t)entry-&gt;vme_start,
					entry-&gt;vme_end - entry-&gt;vme_start,
					pmap_flags);
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* NO_NESTED_PMAP */</span>
				<span class="enscript-keyword">if</span> ((map-&gt;mapped_in_other_pmaps) &amp;&amp; (map-&gt;ref_count)) {
					<span class="enscript-comment">/* clean up parent map/maps */</span>
					vm_map_submap_pmap_clean(
						map, entry-&gt;vme_start,
						entry-&gt;vme_end,
						VME_SUBMAP(entry),
						VME_OFFSET(entry));
				}
			} <span class="enscript-keyword">else</span> {
				vm_map_submap_pmap_clean(
					map, entry-&gt;vme_start, entry-&gt;vme_end,
					VME_SUBMAP(entry),
					VME_OFFSET(entry));
			}
		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (VME_OBJECT(entry) != kernel_object &amp;&amp;
			   VME_OBJECT(entry) != compressor_object) {
			object = VME_OBJECT(entry);
			<span class="enscript-keyword">if</span> ((map-&gt;mapped_in_other_pmaps) &amp;&amp; (map-&gt;ref_count)) {
				vm_object_pmap_protect_options(
					object, VME_OFFSET(entry),
					entry-&gt;vme_end - entry-&gt;vme_start,
					PMAP_NULL,
					entry-&gt;vme_start,
					VM_PROT_NONE,
					PMAP_OPTIONS_REMOVE);
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> ((VME_OBJECT(entry) != VM_OBJECT_NULL) ||
				   (map-&gt;pmap == kernel_pmap)) {
				<span class="enscript-comment">/* Remove translations associated
				 * with this range unless the entry
				 * does not have an object, or
				 * it's the kernel map or a descendant
				 * since the platform could potentially
				 * create &quot;backdoor&quot; mappings invisible
				 * to the VM. It is expected that
				 * objectless, non-kernel ranges
				 * do not have such VM invisible
				 * translations.
				 */</span>
				pmap_remove_options(map-&gt;pmap,
						    (addr64_t)entry-&gt;vme_start,
						    (addr64_t)entry-&gt;vme_end,
						    PMAP_OPTIONS_REMOVE);
			}
		}

		<span class="enscript-keyword">if</span> (entry-&gt;iokit_acct) {
			<span class="enscript-comment">/* alternate accounting */</span>
			vm_map_iokit_unmapped_region(map,
						     (entry-&gt;vme_end -
						      entry-&gt;vme_start));
			entry-&gt;iokit_acct = FALSE;
		}

		<span class="enscript-comment">/*
		 * All pmap mappings for this map entry must have been
		 * cleared by now.
		 */</span>
#<span class="enscript-reference">if</span> <span class="enscript-variable-name">DEBUG</span>
		assert(vm_map_pmap_is_empty(map,
					    entry-&gt;vme_start,
					    entry-&gt;vme_end));
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* DEBUG */</span>

		next = entry-&gt;vme_next;

		<span class="enscript-keyword">if</span> (map-&gt;pmap == kernel_pmap &amp;&amp;
		    map-&gt;ref_count != 0 &amp;&amp;
		    entry-&gt;vme_end &lt; end &amp;&amp;
		    (next == vm_map_to_entry(map) ||
		     next-&gt;vme_start != entry-&gt;vme_end)) {
			panic(<span class="enscript-string">&quot;vm_map_delete(%p,0x%llx,0x%llx): &quot;</span>
			      <span class="enscript-string">&quot;hole after %p at 0x%llx\n&quot;</span>,
			      map,
			      (uint64_t)start,
			      (uint64_t)end,
			      entry,
			      (uint64_t)entry-&gt;vme_end);
		}

		s = next-&gt;vme_start;
		last_timestamp = map-&gt;timestamp;

		<span class="enscript-keyword">if</span> ((flags &amp; VM_MAP_REMOVE_SAVE_ENTRIES) &amp;&amp;
		    zap_map != VM_MAP_NULL) {
			vm_map_size_t entry_size;
			<span class="enscript-comment">/*
			 * The caller wants to save the affected VM map entries
			 * into the &quot;zap_map&quot;.  The caller will take care of
			 * these entries.
			 */</span>
			<span class="enscript-comment">/* unlink the entry from &quot;map&quot; ... */</span>
			vm_map_store_entry_unlink(map, entry);
			<span class="enscript-comment">/* ... and add it to the end of the &quot;zap_map&quot; */</span>
			vm_map_store_entry_link(zap_map,
					  vm_map_last_entry(zap_map),
					  entry);
			entry_size = entry-&gt;vme_end - entry-&gt;vme_start;
			map-&gt;size -= entry_size;
			zap_map-&gt;size += entry_size;
			<span class="enscript-comment">/* we didn't unlock the map, so no timestamp increase */</span>
			last_timestamp--;
		} <span class="enscript-keyword">else</span> {
			vm_map_entry_delete(map, entry);
			<span class="enscript-comment">/* vm_map_entry_delete unlocks the map */</span>
			vm_map_lock(map);
		}

		entry = next;

		<span class="enscript-keyword">if</span>(entry == vm_map_to_entry(map)) {
			<span class="enscript-keyword">break</span>;
		}
		<span class="enscript-keyword">if</span> (last_timestamp+1 != map-&gt;timestamp) {
			<span class="enscript-comment">/*
			 * we are responsible for deleting everything
			 * from the give space, if someone has interfered
			 * we pick up where we left off, back fills should
			 * be all right for anyone except map_delete and
			 * we have to assume that the task has been fully
			 * disabled before we get here
			 */</span>
        		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, s, &amp;entry)){
	               		entry = entry-&gt;vme_next;
				s = entry-&gt;vme_start;
        		} <span class="enscript-keyword">else</span> {
				SAVE_HINT_MAP_WRITE(map, entry-&gt;vme_prev);
       		 	}
			<span class="enscript-comment">/* 
			 * others can not only allocate behind us, we can 
			 * also see coalesce while we don't have the map lock 
			 */</span>
			<span class="enscript-keyword">if</span>(entry == vm_map_to_entry(map)) {
				<span class="enscript-keyword">break</span>;
			}
		}
		last_timestamp = map-&gt;timestamp;
	}

	<span class="enscript-keyword">if</span> (map-&gt;wait_for_space)
		thread_wakeup((event_t) map);
	<span class="enscript-comment">/*
	 * wake up anybody waiting on entries that we have already deleted.
	 */</span>
	<span class="enscript-keyword">if</span> (need_wakeup)
		vm_map_entry_wakeup(map);

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 *	vm_map_remove:
 *
 *	Remove the given address range from the target map.
 *	This is the exported form of vm_map_delete.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_remove</span>(
	<span class="enscript-type">register</span> vm_map_t	map,
	<span class="enscript-type">register</span> vm_map_offset_t	start,
	<span class="enscript-type">register</span> vm_map_offset_t	end,
	<span class="enscript-type">register</span> boolean_t	flags)
{
	<span class="enscript-type">register</span> kern_return_t	result;

	vm_map_lock(map);
	VM_MAP_RANGE_CHECK(map, start, end);
	<span class="enscript-comment">/*
	 * For the zone_map, the kernel controls the allocation/freeing of memory.
	 * Any free to the zone_map should be within the bounds of the map and
	 * should free up memory. If the VM_MAP_RANGE_CHECK() silently converts a
	 * free to the zone_map into a no-op, there is a problem and we should
	 * panic.
	 */</span>
	<span class="enscript-keyword">if</span> ((map == zone_map) &amp;&amp; (start == end))
		panic(<span class="enscript-string">&quot;Nothing being freed to the zone_map. start = end = %p\n&quot;</span>, (<span class="enscript-type">void</span> *)start);
	result = vm_map_delete(map, start, end, flags, VM_MAP_NULL);
	vm_map_unlock(map);

	<span class="enscript-keyword">return</span>(result);
}


<span class="enscript-comment">/*
 *	Routine:	vm_map_copy_discard
 *
 *	Description:
 *		Dispose of a map copy object (returned by
 *		vm_map_copyin).
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_copy_discard</span>(
	vm_map_copy_t	copy)
{
	<span class="enscript-keyword">if</span> (copy == VM_MAP_COPY_NULL)
		<span class="enscript-keyword">return</span>;

	<span class="enscript-keyword">switch</span> (copy-&gt;type) {
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MAP_COPY_ENTRY_LIST</span>:
		<span class="enscript-keyword">while</span> (vm_map_copy_first_entry(copy) !=
		       vm_map_copy_to_entry(copy)) {
			vm_map_entry_t	entry = vm_map_copy_first_entry(copy);

			vm_map_copy_entry_unlink(copy, entry);
			<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
				vm_map_deallocate(VME_SUBMAP(entry));
			} <span class="enscript-keyword">else</span> {
				vm_object_deallocate(VME_OBJECT(entry));
			}
			vm_map_copy_entry_dispose(copy, entry);
		}
		<span class="enscript-keyword">break</span>;
        <span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MAP_COPY_OBJECT</span>:
		vm_object_deallocate(copy-&gt;cpy_object);
		<span class="enscript-keyword">break</span>;
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MAP_COPY_KERNEL_BUFFER</span>:

		<span class="enscript-comment">/*
		 * The vm_map_copy_t and possibly the data buffer were
		 * allocated by a single call to kalloc(), i.e. the
		 * vm_map_copy_t was not allocated out of the zone.
		 */</span>
		<span class="enscript-keyword">if</span> (copy-&gt;size &gt; msg_ool_size_small || copy-&gt;offset)
			panic(<span class="enscript-string">&quot;Invalid vm_map_copy_t sz:%lld, ofst:%lld&quot;</span>,
			      (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)copy-&gt;size, (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)copy-&gt;offset);
		kfree(copy, copy-&gt;size + cpy_kdata_hdr_sz);
		<span class="enscript-keyword">return</span>;
	}
	zfree(vm_map_copy_zone, copy);
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_copy_copy
 *
 *	Description:
 *			Move the information in a map copy object to
 *			a new map copy object, leaving the old one
 *			empty.
 *
 *			This is used by kernel routines that need
 *			to look at out-of-line data (in copyin form)
 *			before deciding whether to return SUCCESS.
 *			If the routine returns FAILURE, the original
 *			copy object will be deallocated; therefore,
 *			these routines must make a copy of the copy
 *			object and leave the original empty so that
 *			deallocation will not fail.
 */</span>
vm_map_copy_t
<span class="enscript-function-name">vm_map_copy_copy</span>(
	vm_map_copy_t	copy)
{
	vm_map_copy_t	new_copy;

	<span class="enscript-keyword">if</span> (copy == VM_MAP_COPY_NULL)
		<span class="enscript-keyword">return</span> VM_MAP_COPY_NULL;

	<span class="enscript-comment">/*
	 * Allocate a new copy object, and copy the information
	 * from the old one into it.
	 */</span>

	new_copy = (vm_map_copy_t) zalloc(vm_map_copy_zone);
	new_copy-&gt;c_u.hdr.rb_head_store.rbh_root = (<span class="enscript-type">void</span>*)(<span class="enscript-type">int</span>)SKIP_RB_TREE;
	*new_copy = *copy;

	<span class="enscript-keyword">if</span> (copy-&gt;type == VM_MAP_COPY_ENTRY_LIST) {
		<span class="enscript-comment">/*
		 * The links in the entry chain must be
		 * changed to point to the new copy object.
		 */</span>
		vm_map_copy_first_entry(copy)-&gt;vme_prev
			= vm_map_copy_to_entry(new_copy);
		vm_map_copy_last_entry(copy)-&gt;vme_next
			= vm_map_copy_to_entry(new_copy);
	}

	<span class="enscript-comment">/*
	 * Change the old copy object into one that contains
	 * nothing to be deallocated.
	 */</span>
	copy-&gt;type = VM_MAP_COPY_OBJECT;
	copy-&gt;cpy_object = VM_OBJECT_NULL;

	<span class="enscript-comment">/*
	 * Return the new object.
	 */</span>
	<span class="enscript-keyword">return</span> new_copy;
}

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_overwrite_submap_recurse</span>(
	vm_map_t	dst_map,
	vm_map_offset_t	dst_addr,
	vm_map_size_t	dst_size)
{
	vm_map_offset_t	dst_end;
	vm_map_entry_t	tmp_entry;
	vm_map_entry_t	entry;
	kern_return_t	result;
	boolean_t	encountered_sub_map = FALSE;



	<span class="enscript-comment">/*
	 *	Verify that the destination is all writeable
	 *	initially.  We have to trunc the destination
	 *	address and round the copy size or we'll end up
	 *	splitting entries in strange ways.
	 */</span>

	dst_end = vm_map_round_page(dst_addr + dst_size,
				    VM_MAP_PAGE_MASK(dst_map));
	vm_map_lock(dst_map);

<span class="enscript-reference">start_pass_1</span>:
	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(dst_map, dst_addr, &amp;tmp_entry)) {
		vm_map_unlock(dst_map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}

	vm_map_clip_start(dst_map,
			  tmp_entry,
			  vm_map_trunc_page(dst_addr,
					    VM_MAP_PAGE_MASK(dst_map)));
	<span class="enscript-keyword">if</span> (tmp_entry-&gt;is_sub_map) {
		<span class="enscript-comment">/* clipping did unnest if needed */</span>
		assert(!tmp_entry-&gt;use_pmap);
	}

	<span class="enscript-keyword">for</span> (entry = tmp_entry;;) {
		vm_map_entry_t	next;

		next = entry-&gt;vme_next;
		<span class="enscript-keyword">while</span>(entry-&gt;is_sub_map) {
			vm_map_offset_t	sub_start;
			vm_map_offset_t	sub_end;
			vm_map_offset_t	local_end;

			<span class="enscript-keyword">if</span> (entry-&gt;in_transition) {
				<span class="enscript-comment">/*
				 * Say that we are waiting, and wait for entry.
				 */</span>
                        	entry-&gt;needs_wakeup = TRUE;
                        	vm_map_entry_wait(dst_map, THREAD_UNINT);

				<span class="enscript-keyword">goto</span> <span class="enscript-reference">start_pass_1</span>;
			}

			encountered_sub_map = TRUE;
			sub_start = VME_OFFSET(entry);

			<span class="enscript-keyword">if</span>(entry-&gt;vme_end &lt; dst_end)
				sub_end = entry-&gt;vme_end;
			<span class="enscript-keyword">else</span> 
				sub_end = dst_end;
			sub_end -= entry-&gt;vme_start;
			sub_end += VME_OFFSET(entry);
			local_end = entry-&gt;vme_end;
			vm_map_unlock(dst_map);
			
			result = vm_map_overwrite_submap_recurse(
				VME_SUBMAP(entry),
				sub_start,
				sub_end - sub_start);

			<span class="enscript-keyword">if</span>(result != KERN_SUCCESS)
				<span class="enscript-keyword">return</span> result;
			<span class="enscript-keyword">if</span> (dst_end &lt;= entry-&gt;vme_end)
				<span class="enscript-keyword">return</span> KERN_SUCCESS;
			vm_map_lock(dst_map);
			<span class="enscript-keyword">if</span>(!vm_map_lookup_entry(dst_map, local_end, 
						&amp;tmp_entry)) {
				vm_map_unlock(dst_map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}
			entry = tmp_entry;
			next = entry-&gt;vme_next;
		}

		<span class="enscript-keyword">if</span> ( ! (entry-&gt;protection &amp; VM_PROT_WRITE)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_PROTECTION_FAILURE);
		}

		<span class="enscript-comment">/*
		 *	If the entry is in transition, we must wait
		 *	for it to exit that state.  Anything could happen
		 *	when we unlock the map, so start over.
		 */</span>
                <span class="enscript-keyword">if</span> (entry-&gt;in_transition) {

                        <span class="enscript-comment">/*
                         * Say that we are waiting, and wait for entry.
                         */</span>
                        entry-&gt;needs_wakeup = TRUE;
                        vm_map_entry_wait(dst_map, THREAD_UNINT);

			<span class="enscript-keyword">goto</span> <span class="enscript-reference">start_pass_1</span>;
		}

<span class="enscript-comment">/*
 *		our range is contained completely within this map entry
 */</span>
		<span class="enscript-keyword">if</span> (dst_end &lt;= entry-&gt;vme_end) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span> KERN_SUCCESS;
		}
<span class="enscript-comment">/*
 *		check that range specified is contiguous region
 */</span>
		<span class="enscript-keyword">if</span> ((next == vm_map_to_entry(dst_map)) ||
		    (next-&gt;vme_start != entry-&gt;vme_end)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}

		<span class="enscript-comment">/*
		 *	Check for permanent objects in the destination.
		 */</span>
		<span class="enscript-keyword">if</span> ((VME_OBJECT(entry) != VM_OBJECT_NULL) &amp;&amp;
		    ((!VME_OBJECT(entry)-&gt;internal) ||
		     (VME_OBJECT(entry)-&gt;true_share))) {
			<span class="enscript-keyword">if</span>(encountered_sub_map) {
				vm_map_unlock(dst_map);
				<span class="enscript-keyword">return</span>(KERN_FAILURE);
			}
		}


		entry = next;
	}<span class="enscript-comment">/* for */</span>
	vm_map_unlock(dst_map);
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_copy_overwrite
 *
 *	Description:
 *		Copy the memory described by the map copy
 *		object (copy; returned by vm_map_copyin) onto
 *		the specified destination region (dst_map, dst_addr).
 *		The destination must be writeable.
 *
 *		Unlike vm_map_copyout, this routine actually
 *		writes over previously-mapped memory.  If the
 *		previous mapping was to a permanent (user-supplied)
 *		memory object, it is preserved.
 *
 *		The attributes (protection and inheritance) of the
 *		destination region are preserved.
 *
 *		If successful, consumes the copy object.
 *		Otherwise, the caller is responsible for it.
 *
 *	Implementation notes:
 *		To overwrite aligned temporary virtual memory, it is
 *		sufficient to remove the previous mapping and insert
 *		the new copy.  This replacement is done either on
 *		the whole region (if no permanent virtual memory
 *		objects are embedded in the destination region) or
 *		in individual map entries.
 *
 *		To overwrite permanent virtual memory , it is necessary
 *		to copy each page, as the external memory management
 *		interface currently does not provide any optimizations.
 *
 *		Unaligned memory also has to be copied.  It is possible
 *		to use 'vm_trickery' to copy the aligned data.  This is
 *		not done but not hard to implement.
 *
 *		Once a page of permanent memory has been overwritten,
 *		it is impossible to interrupt this function; otherwise,
 *		the call would be neither atomic nor location-independent.
 *		The kernel-state portion of a user thread must be
 *		interruptible.
 *
 *		It may be expensive to forward all requests that might
 *		overwrite permanent memory (vm_write, vm_copy) to
 *		uninterruptible kernel threads.  This routine may be
 *		called by interruptible threads; however, success is
 *		not guaranteed -- if the request cannot be performed
 *		atomically and interruptibly, an error indication is
 *		returned.
 */</span>

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_copy_overwrite_nested</span>(
	vm_map_t		dst_map,
	vm_map_address_t	dst_addr,
	vm_map_copy_t		copy,
	boolean_t		interruptible,
	pmap_t			pmap,
	boolean_t		discard_on_success)
{
	vm_map_offset_t		dst_end;
	vm_map_entry_t		tmp_entry;
	vm_map_entry_t		entry;
	kern_return_t		kr;
	boolean_t		aligned = TRUE;
	boolean_t		contains_permanent_objects = FALSE;
	boolean_t		encountered_sub_map = FALSE;
	vm_map_offset_t		base_addr;
	vm_map_size_t		copy_size;
	vm_map_size_t		total_size;


	<span class="enscript-comment">/*
	 *	Check for null copy object.
	 */</span>

	<span class="enscript-keyword">if</span> (copy == VM_MAP_COPY_NULL)
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);

	<span class="enscript-comment">/*
	 *	Check for special kernel buffer allocated
	 *	by new_ipc_kmsg_copyin.
	 */</span>

	<span class="enscript-keyword">if</span> (copy-&gt;type == VM_MAP_COPY_KERNEL_BUFFER) {
		<span class="enscript-keyword">return</span>(vm_map_copyout_kernel_buffer(
			       dst_map, &amp;dst_addr, 
			       copy, TRUE, discard_on_success));
	}

	<span class="enscript-comment">/*
	 *      Only works for entry lists at the moment.  Will
	 *	support page lists later.
	 */</span>

	assert(copy-&gt;type == VM_MAP_COPY_ENTRY_LIST);

	<span class="enscript-keyword">if</span> (copy-&gt;size == 0) {
		<span class="enscript-keyword">if</span> (discard_on_success)
			vm_map_copy_discard(copy);
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}

	<span class="enscript-comment">/*
	 *	Verify that the destination is all writeable
	 *	initially.  We have to trunc the destination
	 *	address and round the copy size or we'll end up
	 *	splitting entries in strange ways.
	 */</span>

	<span class="enscript-keyword">if</span> (!VM_MAP_PAGE_ALIGNED(copy-&gt;size,
				 VM_MAP_PAGE_MASK(dst_map)) ||
	    !VM_MAP_PAGE_ALIGNED(copy-&gt;offset,
				 VM_MAP_PAGE_MASK(dst_map)) ||
	    !VM_MAP_PAGE_ALIGNED(dst_addr,
				 VM_MAP_PAGE_MASK(dst_map)))
	{
		aligned = FALSE;
		dst_end = vm_map_round_page(dst_addr + copy-&gt;size,
					    VM_MAP_PAGE_MASK(dst_map));
	} <span class="enscript-keyword">else</span> {
		dst_end = dst_addr + copy-&gt;size;
	}

	vm_map_lock(dst_map);

	<span class="enscript-comment">/* LP64todo - remove this check when vm_map_commpage64()
	 * no longer has to stuff in a map_entry for the commpage
	 * above the map's max_offset.
	 */</span>
	<span class="enscript-keyword">if</span> (dst_addr &gt;= dst_map-&gt;max_offset) {
		vm_map_unlock(dst_map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}
	 
<span class="enscript-reference">start_pass_1</span>:
	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(dst_map, dst_addr, &amp;tmp_entry)) {
		vm_map_unlock(dst_map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}
	vm_map_clip_start(dst_map,
			  tmp_entry,
			  vm_map_trunc_page(dst_addr,
					    VM_MAP_PAGE_MASK(dst_map)));
	<span class="enscript-keyword">for</span> (entry = tmp_entry;;) {
		vm_map_entry_t	next = entry-&gt;vme_next;

		<span class="enscript-keyword">while</span>(entry-&gt;is_sub_map) {
			vm_map_offset_t	sub_start;
			vm_map_offset_t	sub_end;
			vm_map_offset_t	local_end;

                	<span class="enscript-keyword">if</span> (entry-&gt;in_transition) {

				<span class="enscript-comment">/*
				 * Say that we are waiting, and wait for entry.
				 */</span>
                        	entry-&gt;needs_wakeup = TRUE;
                        	vm_map_entry_wait(dst_map, THREAD_UNINT);

				<span class="enscript-keyword">goto</span> <span class="enscript-reference">start_pass_1</span>;
			}

			local_end = entry-&gt;vme_end;
		        <span class="enscript-keyword">if</span> (!(entry-&gt;needs_copy)) {
				<span class="enscript-comment">/* if needs_copy we are a COW submap */</span>
				<span class="enscript-comment">/* in such a case we just replace so */</span>
				<span class="enscript-comment">/* there is no need for the follow-  */</span>
				<span class="enscript-comment">/* ing check.                        */</span>
				encountered_sub_map = TRUE;
				sub_start = VME_OFFSET(entry);

				<span class="enscript-keyword">if</span>(entry-&gt;vme_end &lt; dst_end)
					sub_end = entry-&gt;vme_end;
				<span class="enscript-keyword">else</span> 
					sub_end = dst_end;
				sub_end -= entry-&gt;vme_start;
				sub_end += VME_OFFSET(entry);
				vm_map_unlock(dst_map);
			
				kr = vm_map_overwrite_submap_recurse(
					VME_SUBMAP(entry),
					sub_start,
					sub_end - sub_start);
				<span class="enscript-keyword">if</span>(kr != KERN_SUCCESS)
					<span class="enscript-keyword">return</span> kr;
				vm_map_lock(dst_map);
			}

			<span class="enscript-keyword">if</span> (dst_end &lt;= entry-&gt;vme_end)
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">start_overwrite</span>;
			<span class="enscript-keyword">if</span>(!vm_map_lookup_entry(dst_map, local_end, 
						&amp;entry)) {
				vm_map_unlock(dst_map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}
			next = entry-&gt;vme_next;
		}

		<span class="enscript-keyword">if</span> ( ! (entry-&gt;protection &amp; VM_PROT_WRITE)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_PROTECTION_FAILURE);
		}

		<span class="enscript-comment">/*
		 *	If the entry is in transition, we must wait
		 *	for it to exit that state.  Anything could happen
		 *	when we unlock the map, so start over.
		 */</span>
                <span class="enscript-keyword">if</span> (entry-&gt;in_transition) {

                        <span class="enscript-comment">/*
                         * Say that we are waiting, and wait for entry.
                         */</span>
                        entry-&gt;needs_wakeup = TRUE;
                        vm_map_entry_wait(dst_map, THREAD_UNINT);

			<span class="enscript-keyword">goto</span> <span class="enscript-reference">start_pass_1</span>;
		}

<span class="enscript-comment">/*
 *		our range is contained completely within this map entry
 */</span>
		<span class="enscript-keyword">if</span> (dst_end &lt;= entry-&gt;vme_end)
			<span class="enscript-keyword">break</span>;
<span class="enscript-comment">/*
 *		check that range specified is contiguous region
 */</span>
		<span class="enscript-keyword">if</span> ((next == vm_map_to_entry(dst_map)) ||
		    (next-&gt;vme_start != entry-&gt;vme_end)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}


		<span class="enscript-comment">/*
		 *	Check for permanent objects in the destination.
		 */</span>
		<span class="enscript-keyword">if</span> ((VME_OBJECT(entry) != VM_OBJECT_NULL) &amp;&amp;
		    ((!VME_OBJECT(entry)-&gt;internal) ||
		     (VME_OBJECT(entry)-&gt;true_share))) {
			contains_permanent_objects = TRUE;
		}

		entry = next;
	}<span class="enscript-comment">/* for */</span>

<span class="enscript-reference">start_overwrite</span>:
	<span class="enscript-comment">/*
	 *	If there are permanent objects in the destination, then
	 *	the copy cannot be interrupted.
	 */</span>

	<span class="enscript-keyword">if</span> (interruptible &amp;&amp; contains_permanent_objects) {
		vm_map_unlock(dst_map);
		<span class="enscript-keyword">return</span>(KERN_FAILURE);	<span class="enscript-comment">/* XXX */</span>
	}

	<span class="enscript-comment">/*
 	 *
	 *	Make a second pass, overwriting the data
	 *	At the beginning of each loop iteration,
	 *	the next entry to be overwritten is &quot;tmp_entry&quot;
	 *	(initially, the value returned from the lookup above),
	 *	and the starting address expected in that entry
	 *	is &quot;start&quot;.
	 */</span>

	total_size = copy-&gt;size;
	<span class="enscript-keyword">if</span>(encountered_sub_map) {
		copy_size = 0;
		<span class="enscript-comment">/* re-calculate tmp_entry since we've had the map */</span>
		<span class="enscript-comment">/* unlocked */</span>
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry( dst_map, dst_addr, &amp;tmp_entry)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}
	} <span class="enscript-keyword">else</span> {
		copy_size = copy-&gt;size;
	}
	
	base_addr = dst_addr;
	<span class="enscript-keyword">while</span>(TRUE) {
		<span class="enscript-comment">/* deconstruct the copy object and do in parts */</span>
		<span class="enscript-comment">/* only in sub_map, interruptable case */</span>
		vm_map_entry_t	copy_entry;
		vm_map_entry_t	previous_prev = VM_MAP_ENTRY_NULL;
		vm_map_entry_t	next_copy = VM_MAP_ENTRY_NULL;
		<span class="enscript-type">int</span>		nentries;
		<span class="enscript-type">int</span>		remaining_entries = 0;
		vm_map_offset_t	new_offset = 0;
	
		<span class="enscript-keyword">for</span> (entry = tmp_entry; copy_size == 0;) {
			vm_map_entry_t	next;

			next = entry-&gt;vme_next;

			<span class="enscript-comment">/* tmp_entry and base address are moved along */</span>
			<span class="enscript-comment">/* each time we encounter a sub-map.  Otherwise */</span>
			<span class="enscript-comment">/* entry can outpase tmp_entry, and the copy_size */</span>
			<span class="enscript-comment">/* may reflect the distance between them */</span>
			<span class="enscript-comment">/* if the current entry is found to be in transition */</span>
			<span class="enscript-comment">/* we will start over at the beginning or the last */</span>
			<span class="enscript-comment">/* encounter of a submap as dictated by base_addr */</span>
			<span class="enscript-comment">/* we will zero copy_size accordingly. */</span>
			<span class="enscript-keyword">if</span> (entry-&gt;in_transition) {
                       		<span class="enscript-comment">/*
                       		 * Say that we are waiting, and wait for entry.
                       		 */</span>
                       		entry-&gt;needs_wakeup = TRUE;
                       		vm_map_entry_wait(dst_map, THREAD_UNINT);

				<span class="enscript-keyword">if</span>(!vm_map_lookup_entry(dst_map, base_addr, 
							&amp;tmp_entry)) {
					vm_map_unlock(dst_map);
					<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
				}
				copy_size = 0;
				entry = tmp_entry;
				<span class="enscript-keyword">continue</span>;
			}
			<span class="enscript-keyword">if</span>(entry-&gt;is_sub_map) {
				vm_map_offset_t	sub_start;
				vm_map_offset_t	sub_end;
				vm_map_offset_t	local_end;

		        	<span class="enscript-keyword">if</span> (entry-&gt;needs_copy) {
					<span class="enscript-comment">/* if this is a COW submap */</span>
					<span class="enscript-comment">/* just back the range with a */</span>
					<span class="enscript-comment">/* anonymous entry */</span>
					<span class="enscript-keyword">if</span>(entry-&gt;vme_end &lt; dst_end)
						sub_end = entry-&gt;vme_end;
					<span class="enscript-keyword">else</span> 
						sub_end = dst_end;
					<span class="enscript-keyword">if</span>(entry-&gt;vme_start &lt; base_addr)
						sub_start = base_addr;
					<span class="enscript-keyword">else</span> 
						sub_start = entry-&gt;vme_start;
					vm_map_clip_end(
						dst_map, entry, sub_end);
					vm_map_clip_start(
						dst_map, entry, sub_start);
					assert(!entry-&gt;use_pmap);
					entry-&gt;is_sub_map = FALSE;
					vm_map_deallocate(
						VME_SUBMAP(entry));
					VME_SUBMAP_SET(entry, NULL);
					entry-&gt;is_shared = FALSE;
					entry-&gt;needs_copy = FALSE;
					VME_OFFSET_SET(entry, 0);
					<span class="enscript-comment">/*
					 * XXX FBDP
					 * We should propagate the protections
					 * of the submap entry here instead
					 * of forcing them to VM_PROT_ALL...
					 * Or better yet, we should inherit
					 * the protection of the copy_entry.
					 */</span>
					entry-&gt;protection = VM_PROT_ALL;
					entry-&gt;max_protection = VM_PROT_ALL;
					entry-&gt;wired_count = 0;
					entry-&gt;user_wired_count = 0;
					<span class="enscript-keyword">if</span>(entry-&gt;inheritance 
					   == VM_INHERIT_SHARE) 
						entry-&gt;inheritance = VM_INHERIT_COPY;
					<span class="enscript-keyword">continue</span>;
				}
				<span class="enscript-comment">/* first take care of any non-sub_map */</span>
				<span class="enscript-comment">/* entries to send */</span>
				<span class="enscript-keyword">if</span>(base_addr &lt; entry-&gt;vme_start) {
					<span class="enscript-comment">/* stuff to send */</span>
					copy_size = 
						entry-&gt;vme_start - base_addr;
					<span class="enscript-keyword">break</span>;
				}
				sub_start = VME_OFFSET(entry);

				<span class="enscript-keyword">if</span>(entry-&gt;vme_end &lt; dst_end)
					sub_end = entry-&gt;vme_end;
				<span class="enscript-keyword">else</span> 
					sub_end = dst_end;
				sub_end -= entry-&gt;vme_start;
				sub_end += VME_OFFSET(entry);
				local_end = entry-&gt;vme_end;
				vm_map_unlock(dst_map);
				copy_size = sub_end - sub_start;

				<span class="enscript-comment">/* adjust the copy object */</span>
				<span class="enscript-keyword">if</span> (total_size &gt; copy_size) {
					vm_map_size_t	local_size = 0;
					vm_map_size_t	entry_size;

					nentries = 1;
					new_offset = copy-&gt;offset;
					copy_entry = vm_map_copy_first_entry(copy);
					<span class="enscript-keyword">while</span>(copy_entry != 
					      vm_map_copy_to_entry(copy)){
						entry_size = copy_entry-&gt;vme_end - 
							copy_entry-&gt;vme_start;
						<span class="enscript-keyword">if</span>((local_size &lt; copy_size) &amp;&amp;
						   ((local_size + entry_size) 
						    &gt;= copy_size)) {
							vm_map_copy_clip_end(copy, 
									     copy_entry, 
									     copy_entry-&gt;vme_start +
									     (copy_size - local_size));
							entry_size = copy_entry-&gt;vme_end - 
								copy_entry-&gt;vme_start;
							local_size += entry_size;
							new_offset += entry_size;
						}
						<span class="enscript-keyword">if</span>(local_size &gt;= copy_size) {
							next_copy = copy_entry-&gt;vme_next;
							copy_entry-&gt;vme_next = 
								vm_map_copy_to_entry(copy);
							previous_prev = 
								copy-&gt;cpy_hdr.links.prev;
							copy-&gt;cpy_hdr.links.prev = copy_entry;
							copy-&gt;size = copy_size;
							remaining_entries = 
								copy-&gt;cpy_hdr.nentries;
							remaining_entries -= nentries;
							copy-&gt;cpy_hdr.nentries = nentries;
							<span class="enscript-keyword">break</span>;
						} <span class="enscript-keyword">else</span> {
							local_size += entry_size;
							new_offset += entry_size;
							nentries++;
						}
						copy_entry = copy_entry-&gt;vme_next;
					}
				}
			
				<span class="enscript-keyword">if</span>((entry-&gt;use_pmap) &amp;&amp; (pmap == NULL)) {
					kr = vm_map_copy_overwrite_nested(
						VME_SUBMAP(entry),
						sub_start,
						copy,
						interruptible, 
						VME_SUBMAP(entry)-&gt;pmap,
						TRUE);
				} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (pmap != NULL) {
					kr = vm_map_copy_overwrite_nested(
						VME_SUBMAP(entry),
						sub_start,
						copy,
						interruptible, pmap,
						TRUE);
				} <span class="enscript-keyword">else</span> {
					kr = vm_map_copy_overwrite_nested(
						VME_SUBMAP(entry),
						sub_start,
						copy,
						interruptible,
						dst_map-&gt;pmap,
						TRUE);
				}
				<span class="enscript-keyword">if</span>(kr != KERN_SUCCESS) {
					<span class="enscript-keyword">if</span>(next_copy != NULL) {
						copy-&gt;cpy_hdr.nentries += 
							remaining_entries;
						copy-&gt;cpy_hdr.links.prev-&gt;vme_next = 
							next_copy;
						copy-&gt;cpy_hdr.links.prev 
							= previous_prev;
						copy-&gt;size = total_size;
					}
					<span class="enscript-keyword">return</span> kr;
				}
				<span class="enscript-keyword">if</span> (dst_end &lt;= local_end) {
					<span class="enscript-keyword">return</span>(KERN_SUCCESS);
				}
				<span class="enscript-comment">/* otherwise copy no longer exists, it was */</span>
				<span class="enscript-comment">/* destroyed after successful copy_overwrite */</span>
			        copy = (vm_map_copy_t) 
					zalloc(vm_map_copy_zone);
				copy-&gt;c_u.hdr.rb_head_store.rbh_root = (<span class="enscript-type">void</span>*)(<span class="enscript-type">int</span>)SKIP_RB_TREE;
				vm_map_copy_first_entry(copy) =
					vm_map_copy_last_entry(copy) =
					vm_map_copy_to_entry(copy);
				copy-&gt;type = VM_MAP_COPY_ENTRY_LIST;
				copy-&gt;offset = new_offset;

				<span class="enscript-comment">/*
				 * XXX FBDP
				 * this does not seem to deal with
				 * the VM map store (R&amp;B tree)
				 */</span>

				total_size -= copy_size;
				copy_size = 0;
				<span class="enscript-comment">/* put back remainder of copy in container */</span>
				<span class="enscript-keyword">if</span>(next_copy != NULL) {
					copy-&gt;cpy_hdr.nentries = remaining_entries;
					copy-&gt;cpy_hdr.links.next = next_copy;
					copy-&gt;cpy_hdr.links.prev = previous_prev;
					copy-&gt;size = total_size;
					next_copy-&gt;vme_prev = 
						vm_map_copy_to_entry(copy);
					next_copy = NULL;
				}
				base_addr = local_end;
				vm_map_lock(dst_map);
				<span class="enscript-keyword">if</span>(!vm_map_lookup_entry(dst_map, 
							local_end, &amp;tmp_entry)) {
					vm_map_unlock(dst_map);
					<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
				}
				entry = tmp_entry;
				<span class="enscript-keyword">continue</span>;
			} 
			<span class="enscript-keyword">if</span> (dst_end &lt;= entry-&gt;vme_end) {
				copy_size = dst_end - base_addr;
				<span class="enscript-keyword">break</span>;
			}

			<span class="enscript-keyword">if</span> ((next == vm_map_to_entry(dst_map)) ||
			    (next-&gt;vme_start != entry-&gt;vme_end)) {
				vm_map_unlock(dst_map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}

			entry = next;
		}<span class="enscript-comment">/* for */</span>

		next_copy = NULL;
		nentries = 1;

		<span class="enscript-comment">/* adjust the copy object */</span>
		<span class="enscript-keyword">if</span> (total_size &gt; copy_size) {
			vm_map_size_t	local_size = 0;
			vm_map_size_t	entry_size;

			new_offset = copy-&gt;offset;
			copy_entry = vm_map_copy_first_entry(copy);
			<span class="enscript-keyword">while</span>(copy_entry != vm_map_copy_to_entry(copy)) {
				entry_size = copy_entry-&gt;vme_end - 
					copy_entry-&gt;vme_start;
				<span class="enscript-keyword">if</span>((local_size &lt; copy_size) &amp;&amp;
				   ((local_size + entry_size) 
				    &gt;= copy_size)) {
					vm_map_copy_clip_end(copy, copy_entry, 
							     copy_entry-&gt;vme_start +
							     (copy_size - local_size));
					entry_size = copy_entry-&gt;vme_end - 
						copy_entry-&gt;vme_start;
					local_size += entry_size;
					new_offset += entry_size;
				}
				<span class="enscript-keyword">if</span>(local_size &gt;= copy_size) {
					next_copy = copy_entry-&gt;vme_next;
					copy_entry-&gt;vme_next = 
						vm_map_copy_to_entry(copy);
					previous_prev = 
						copy-&gt;cpy_hdr.links.prev;
					copy-&gt;cpy_hdr.links.prev = copy_entry;
					copy-&gt;size = copy_size;
					remaining_entries = 
						copy-&gt;cpy_hdr.nentries;
					remaining_entries -= nentries;
					copy-&gt;cpy_hdr.nentries = nentries;
					<span class="enscript-keyword">break</span>;
				} <span class="enscript-keyword">else</span> {
					local_size += entry_size;
					new_offset += entry_size;
					nentries++;
				}
				copy_entry = copy_entry-&gt;vme_next;
			}
		}

		<span class="enscript-keyword">if</span> (aligned) {
			pmap_t	local_pmap;

			<span class="enscript-keyword">if</span>(pmap)
				local_pmap = pmap;
			<span class="enscript-keyword">else</span>
				local_pmap = dst_map-&gt;pmap;

			<span class="enscript-keyword">if</span> ((kr =  vm_map_copy_overwrite_aligned( 
				     dst_map, tmp_entry, copy,
				     base_addr, local_pmap)) != KERN_SUCCESS) {
				<span class="enscript-keyword">if</span>(next_copy != NULL) {
					copy-&gt;cpy_hdr.nentries += 
						remaining_entries;
				        copy-&gt;cpy_hdr.links.prev-&gt;vme_next = 
						next_copy;
			       		copy-&gt;cpy_hdr.links.prev = 
						previous_prev;
					copy-&gt;size += copy_size;
				}
				<span class="enscript-keyword">return</span> kr;
			}
			vm_map_unlock(dst_map);
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/*
			 * Performance gain:
			 *
			 * if the copy and dst address are misaligned but the same
			 * offset within the page we can copy_not_aligned the
			 * misaligned parts and copy aligned the rest.  If they are
			 * aligned but len is unaligned we simply need to copy
			 * the end bit unaligned.  We'll need to split the misaligned
			 * bits of the region in this case !
			 */</span>
			<span class="enscript-comment">/* ALWAYS UNLOCKS THE dst_map MAP */</span>
			kr = vm_map_copy_overwrite_unaligned(
				dst_map,
				tmp_entry,
				copy,
				base_addr,
				discard_on_success);
			<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
				<span class="enscript-keyword">if</span>(next_copy != NULL) {
					copy-&gt;cpy_hdr.nentries +=
						remaining_entries;
			       		copy-&gt;cpy_hdr.links.prev-&gt;vme_next = 
						next_copy;
			       		copy-&gt;cpy_hdr.links.prev = 
						previous_prev;
					copy-&gt;size += copy_size;
				}
				<span class="enscript-keyword">return</span> kr;
			}
		}
		total_size -= copy_size;
		<span class="enscript-keyword">if</span>(total_size == 0)
			<span class="enscript-keyword">break</span>;
		base_addr += copy_size;
		copy_size = 0;
		copy-&gt;offset = new_offset;
		<span class="enscript-keyword">if</span>(next_copy != NULL) {
			copy-&gt;cpy_hdr.nentries = remaining_entries;
			copy-&gt;cpy_hdr.links.next = next_copy;
			copy-&gt;cpy_hdr.links.prev = previous_prev;
			next_copy-&gt;vme_prev = vm_map_copy_to_entry(copy);
			copy-&gt;size = total_size;
		}
		vm_map_lock(dst_map);
		<span class="enscript-keyword">while</span>(TRUE) {
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(dst_map, 
						 base_addr, &amp;tmp_entry)) {
				vm_map_unlock(dst_map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}
                	<span class="enscript-keyword">if</span> (tmp_entry-&gt;in_transition) {
                       		entry-&gt;needs_wakeup = TRUE;
                       		vm_map_entry_wait(dst_map, THREAD_UNINT);
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-keyword">break</span>;
			}
		}
		vm_map_clip_start(dst_map,
				  tmp_entry,
				  vm_map_trunc_page(base_addr,
						    VM_MAP_PAGE_MASK(dst_map)));

		entry = tmp_entry;
	} <span class="enscript-comment">/* while */</span>

	<span class="enscript-comment">/*
	 *	Throw away the vm_map_copy object
	 */</span>
	<span class="enscript-keyword">if</span> (discard_on_success)
		vm_map_copy_discard(copy);

	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}<span class="enscript-comment">/* vm_map_copy_overwrite */</span>

kern_return_t
<span class="enscript-function-name">vm_map_copy_overwrite</span>(
	vm_map_t	dst_map,
	vm_map_offset_t	dst_addr,
	vm_map_copy_t	copy,
	boolean_t	interruptible)
{
	vm_map_size_t	head_size, tail_size;
	vm_map_copy_t	head_copy, tail_copy;
	vm_map_offset_t	head_addr, tail_addr;
	vm_map_entry_t	entry;
	kern_return_t	kr;

	head_size = 0;
	tail_size = 0;
	head_copy = NULL;
	tail_copy = NULL;
	head_addr = 0;
	tail_addr = 0;

	<span class="enscript-keyword">if</span> (interruptible ||
	    copy == VM_MAP_COPY_NULL ||
	    copy-&gt;type != VM_MAP_COPY_ENTRY_LIST) {
		<span class="enscript-comment">/*
		 * We can't split the &quot;copy&quot; map if we're interruptible
		 * or if we don't have a &quot;copy&quot; map...
		 */</span>
	<span class="enscript-reference">blunt_copy</span>:
		<span class="enscript-keyword">return</span> vm_map_copy_overwrite_nested(dst_map,
						    dst_addr,
						    copy,
						    interruptible,
						    (pmap_t) NULL,
						    TRUE);
	}

	<span class="enscript-keyword">if</span> (copy-&gt;size &lt; 3 * PAGE_SIZE) {
		<span class="enscript-comment">/*
		 * Too small to bother with optimizing...
		 */</span>
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">blunt_copy</span>;
	}

	<span class="enscript-keyword">if</span> ((dst_addr &amp; VM_MAP_PAGE_MASK(dst_map)) !=
	    (copy-&gt;offset &amp; VM_MAP_PAGE_MASK(dst_map))) {
		<span class="enscript-comment">/*
		 * Incompatible mis-alignment of source and destination...
		 */</span>
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">blunt_copy</span>;
	}

	<span class="enscript-comment">/*
	 * Proper alignment or identical mis-alignment at the beginning.
	 * Let's try and do a small unaligned copy first (if needed)
	 * and then an aligned copy for the rest.
	 */</span>
	<span class="enscript-keyword">if</span> (!page_aligned(dst_addr)) {
		head_addr = dst_addr;
		head_size = (VM_MAP_PAGE_SIZE(dst_map) -
			     (copy-&gt;offset &amp; VM_MAP_PAGE_MASK(dst_map)));
	}
	<span class="enscript-keyword">if</span> (!page_aligned(copy-&gt;offset + copy-&gt;size)) {
		<span class="enscript-comment">/*
		 * Mis-alignment at the end.
		 * Do an aligned copy up to the last page and
		 * then an unaligned copy for the remaining bytes.
		 */</span>
		tail_size = ((copy-&gt;offset + copy-&gt;size) &amp;
			     VM_MAP_PAGE_MASK(dst_map));
		tail_addr = dst_addr + copy-&gt;size - tail_size;
	}

	<span class="enscript-keyword">if</span> (head_size + tail_size == copy-&gt;size) {
		<span class="enscript-comment">/*
		 * It's all unaligned, no optimization possible...
		 */</span>
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">blunt_copy</span>;
	}

	<span class="enscript-comment">/*
	 * Can't optimize if there are any submaps in the
	 * destination due to the way we free the &quot;copy&quot; map
	 * progressively in vm_map_copy_overwrite_nested()
	 * in that case.
	 */</span>
	vm_map_lock_read(dst_map);
	<span class="enscript-keyword">if</span> (! vm_map_lookup_entry(dst_map, dst_addr, &amp;entry)) {
		vm_map_unlock_read(dst_map);
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">blunt_copy</span>;
	}
	<span class="enscript-keyword">for</span> (;
	     (entry != vm_map_copy_to_entry(copy) &amp;&amp;
	      entry-&gt;vme_start &lt; dst_addr + copy-&gt;size);
	     entry = entry-&gt;vme_next) {
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			vm_map_unlock_read(dst_map);
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">blunt_copy</span>;
		}
	}
	vm_map_unlock_read(dst_map);

	<span class="enscript-keyword">if</span> (head_size) {
		<span class="enscript-comment">/*
		 * Unaligned copy of the first &quot;head_size&quot; bytes, to reach
		 * a page boundary.
		 */</span>
		
		<span class="enscript-comment">/*
		 * Extract &quot;head_copy&quot; out of &quot;copy&quot;.
		 */</span>
		head_copy = (vm_map_copy_t) zalloc(vm_map_copy_zone);
		head_copy-&gt;c_u.hdr.rb_head_store.rbh_root = (<span class="enscript-type">void</span>*)(<span class="enscript-type">int</span>)SKIP_RB_TREE;
		vm_map_copy_first_entry(head_copy) =
			vm_map_copy_to_entry(head_copy);
		vm_map_copy_last_entry(head_copy) =
			vm_map_copy_to_entry(head_copy);
		head_copy-&gt;type = VM_MAP_COPY_ENTRY_LIST;
		head_copy-&gt;cpy_hdr.nentries = 0;
		head_copy-&gt;cpy_hdr.entries_pageable =
			copy-&gt;cpy_hdr.entries_pageable;
		vm_map_store_init(&amp;head_copy-&gt;cpy_hdr);

		head_copy-&gt;offset = copy-&gt;offset;
		head_copy-&gt;size = head_size;

		copy-&gt;offset += head_size;
		copy-&gt;size -= head_size;

		entry = vm_map_copy_first_entry(copy);
		vm_map_copy_clip_end(copy, entry, copy-&gt;offset);
		vm_map_copy_entry_unlink(copy, entry);
		vm_map_copy_entry_link(head_copy,
				       vm_map_copy_to_entry(head_copy),
				       entry);

		<span class="enscript-comment">/*
		 * Do the unaligned copy.
		 */</span>
		kr = vm_map_copy_overwrite_nested(dst_map,
						  head_addr,
						  head_copy,
						  interruptible,
						  (pmap_t) NULL,
						  FALSE);
		<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS)
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
	}

	<span class="enscript-keyword">if</span> (tail_size) {
		<span class="enscript-comment">/*
		 * Extract &quot;tail_copy&quot; out of &quot;copy&quot;.
		 */</span>
		tail_copy = (vm_map_copy_t) zalloc(vm_map_copy_zone);
		tail_copy-&gt;c_u.hdr.rb_head_store.rbh_root = (<span class="enscript-type">void</span>*)(<span class="enscript-type">int</span>)SKIP_RB_TREE;
		vm_map_copy_first_entry(tail_copy) =
			vm_map_copy_to_entry(tail_copy);
		vm_map_copy_last_entry(tail_copy) =
			vm_map_copy_to_entry(tail_copy);
		tail_copy-&gt;type = VM_MAP_COPY_ENTRY_LIST;
		tail_copy-&gt;cpy_hdr.nentries = 0;
		tail_copy-&gt;cpy_hdr.entries_pageable =
			copy-&gt;cpy_hdr.entries_pageable;
		vm_map_store_init(&amp;tail_copy-&gt;cpy_hdr);

		tail_copy-&gt;offset = copy-&gt;offset + copy-&gt;size - tail_size;
		tail_copy-&gt;size = tail_size;

		copy-&gt;size -= tail_size;

		entry = vm_map_copy_last_entry(copy);
		vm_map_copy_clip_start(copy, entry, tail_copy-&gt;offset);
		entry = vm_map_copy_last_entry(copy);
		vm_map_copy_entry_unlink(copy, entry);
		vm_map_copy_entry_link(tail_copy,
				       vm_map_copy_last_entry(tail_copy),
				       entry);
	}

	<span class="enscript-comment">/*
	 * Copy most (or possibly all) of the data.
	 */</span>
	kr = vm_map_copy_overwrite_nested(dst_map,
					  dst_addr + head_size,
					  copy,
					  interruptible,
					  (pmap_t) NULL,
					  FALSE);
	<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
	}

	<span class="enscript-keyword">if</span> (tail_size) {
		kr = vm_map_copy_overwrite_nested(dst_map,
						  tail_addr,
						  tail_copy,
						  interruptible,
						  (pmap_t) NULL,
						  FALSE);
	}

<span class="enscript-reference">done</span>:
	assert(copy-&gt;type == VM_MAP_COPY_ENTRY_LIST);
	<span class="enscript-keyword">if</span> (kr == KERN_SUCCESS) {
		<span class="enscript-comment">/*
		 * Discard all the copy maps.
		 */</span>
		<span class="enscript-keyword">if</span> (head_copy) {
			vm_map_copy_discard(head_copy);
			head_copy = NULL;
		}
		vm_map_copy_discard(copy);
		<span class="enscript-keyword">if</span> (tail_copy) {
			vm_map_copy_discard(tail_copy);
			tail_copy = NULL;
		}
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-comment">/*
		 * Re-assemble the original copy map.
		 */</span>
		<span class="enscript-keyword">if</span> (head_copy) {
			entry = vm_map_copy_first_entry(head_copy);
			vm_map_copy_entry_unlink(head_copy, entry);
			vm_map_copy_entry_link(copy,
					       vm_map_copy_to_entry(copy),
					       entry);
			copy-&gt;offset -= head_size;
			copy-&gt;size += head_size;
			vm_map_copy_discard(head_copy);
			head_copy = NULL;
		}
		<span class="enscript-keyword">if</span> (tail_copy) {
			entry = vm_map_copy_last_entry(tail_copy);
			vm_map_copy_entry_unlink(tail_copy, entry);
			vm_map_copy_entry_link(copy,
					       vm_map_copy_last_entry(copy),
					       entry);
			copy-&gt;size += tail_size;
			vm_map_copy_discard(tail_copy);
			tail_copy = NULL;
		}
	}
	<span class="enscript-keyword">return</span> kr;
}


<span class="enscript-comment">/*
 *	Routine: vm_map_copy_overwrite_unaligned	[internal use only]
 *
 *	Decription:
 *	Physically copy unaligned data
 *
 *	Implementation:
 *	Unaligned parts of pages have to be physically copied.  We use
 *	a modified form of vm_fault_copy (which understands none-aligned
 *	page offsets and sizes) to do the copy.  We attempt to copy as
 *	much memory in one go as possibly, however vm_fault_copy copies
 *	within 1 memory object so we have to find the smaller of &quot;amount left&quot;
 *	&quot;source object data size&quot; and &quot;target object data size&quot;.  With
 *	unaligned data we don't need to split regions, therefore the source
 *	(copy) object should be one map entry, the target range may be split
 *	over multiple map entries however.  In any event we are pessimistic
 *	about these assumptions.
 *
 *	Assumptions:
 *	dst_map is locked on entry and is return locked on success,
 *	unlocked on error.
 */</span>

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_copy_overwrite_unaligned</span>(
	vm_map_t	dst_map,
	vm_map_entry_t	entry,
	vm_map_copy_t	copy,
	vm_map_offset_t	start,
	boolean_t	discard_on_success)
{
	vm_map_entry_t		copy_entry;
	vm_map_entry_t		copy_entry_next;
	vm_map_version_t	version;
	vm_object_t		dst_object;
	vm_object_offset_t	dst_offset;
	vm_object_offset_t	src_offset;
	vm_object_offset_t	entry_offset;
	vm_map_offset_t		entry_end;
	vm_map_size_t		src_size,
				dst_size,
				copy_size,
				amount_left;
	kern_return_t		kr = KERN_SUCCESS;

	
	copy_entry = vm_map_copy_first_entry(copy);

	vm_map_lock_write_to_read(dst_map);

	src_offset = copy-&gt;offset - vm_object_trunc_page(copy-&gt;offset);
	amount_left = copy-&gt;size;
<span class="enscript-comment">/*
 *	unaligned so we never clipped this entry, we need the offset into
 *	the vm_object not just the data.
 */</span>	
	<span class="enscript-keyword">while</span> (amount_left &gt; 0) {

		<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(dst_map)) {
			vm_map_unlock_read(dst_map);
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}

		<span class="enscript-comment">/* &quot;start&quot; must be within the current map entry */</span>
		assert ((start&gt;=entry-&gt;vme_start) &amp;&amp; (start&lt;entry-&gt;vme_end));

		dst_offset = start - entry-&gt;vme_start;

		dst_size = entry-&gt;vme_end - start;

		src_size = copy_entry-&gt;vme_end -
			(copy_entry-&gt;vme_start + src_offset);

		<span class="enscript-keyword">if</span> (dst_size &lt; src_size) {
<span class="enscript-comment">/*
 *			we can only copy dst_size bytes before
 *			we have to get the next destination entry
 */</span>
			copy_size = dst_size;
		} <span class="enscript-keyword">else</span> {
<span class="enscript-comment">/*
 *			we can only copy src_size bytes before
 *			we have to get the next source copy entry
 */</span>
			copy_size = src_size;
		}

		<span class="enscript-keyword">if</span> (copy_size &gt; amount_left) {
			copy_size = amount_left;
		}
<span class="enscript-comment">/*
 *		Entry needs copy, create a shadow shadow object for
 *		Copy on write region.
 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;needs_copy &amp;&amp;
		    ((entry-&gt;protection &amp; VM_PROT_WRITE) != 0))
		{
			<span class="enscript-keyword">if</span> (vm_map_lock_read_to_write(dst_map)) {
				vm_map_lock_read(dst_map);
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">RetryLookup</span>;
			}
			VME_OBJECT_SHADOW(entry,
					  (vm_map_size_t)(entry-&gt;vme_end
							  - entry-&gt;vme_start));
			entry-&gt;needs_copy = FALSE;
			vm_map_lock_write_to_read(dst_map);
		}
		dst_object = VME_OBJECT(entry);
<span class="enscript-comment">/*
 *		unlike with the virtual (aligned) copy we're going
 *		to fault on it therefore we need a target object.
 */</span>
                <span class="enscript-keyword">if</span> (dst_object == VM_OBJECT_NULL) {
			<span class="enscript-keyword">if</span> (vm_map_lock_read_to_write(dst_map)) {
				vm_map_lock_read(dst_map);
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">RetryLookup</span>;
			}
			dst_object = vm_object_allocate((vm_map_size_t)
							entry-&gt;vme_end - entry-&gt;vme_start);
			VME_OBJECT(entry) = dst_object;
			VME_OFFSET_SET(entry, 0);
			assert(entry-&gt;use_pmap);
			vm_map_lock_write_to_read(dst_map);
		}
<span class="enscript-comment">/*
 *		Take an object reference and unlock map. The &quot;entry&quot; may
 *		disappear or change when the map is unlocked.
 */</span>
		vm_object_reference(dst_object);
		version.main_timestamp = dst_map-&gt;timestamp;
		entry_offset = VME_OFFSET(entry);
		entry_end = entry-&gt;vme_end;
		vm_map_unlock_read(dst_map);
<span class="enscript-comment">/*
 *		Copy as much as possible in one pass
 */</span>
		kr = vm_fault_copy(
			VME_OBJECT(copy_entry),
			VME_OFFSET(copy_entry) + src_offset,
			&amp;copy_size,
			dst_object,
			entry_offset + dst_offset,
			dst_map,
			&amp;version,
			THREAD_UNINT );

		start += copy_size;
		src_offset += copy_size;
		amount_left -= copy_size;
<span class="enscript-comment">/*
 *		Release the object reference
 */</span>
		vm_object_deallocate(dst_object);
<span class="enscript-comment">/*
 *		If a hard error occurred, return it now
 */</span>
		<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS)
			<span class="enscript-keyword">return</span> kr;

		<span class="enscript-keyword">if</span> ((copy_entry-&gt;vme_start + src_offset) == copy_entry-&gt;vme_end
		    || amount_left == 0)
		{
<span class="enscript-comment">/*
 *			all done with this copy entry, dispose.
 */</span>
			copy_entry_next = copy_entry-&gt;vme_next;

			<span class="enscript-keyword">if</span> (discard_on_success) {
				vm_map_copy_entry_unlink(copy, copy_entry);
				assert(!copy_entry-&gt;is_sub_map);
				vm_object_deallocate(VME_OBJECT(copy_entry));
				vm_map_copy_entry_dispose(copy, copy_entry);
			}

			<span class="enscript-keyword">if</span> (copy_entry_next == vm_map_copy_to_entry(copy) &amp;&amp;
			    amount_left) {
<span class="enscript-comment">/*
 *				not finished copying but run out of source
 */</span>
				<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
			}

			copy_entry = copy_entry_next;

			src_offset = 0;
		}

		<span class="enscript-keyword">if</span> (amount_left == 0)
			<span class="enscript-keyword">return</span> KERN_SUCCESS;

		vm_map_lock_read(dst_map);
		<span class="enscript-keyword">if</span> (version.main_timestamp == dst_map-&gt;timestamp) {
			<span class="enscript-keyword">if</span> (start == entry_end) {
<span class="enscript-comment">/*
 *				destination region is split.  Use the version
 *				information to avoid a lookup in the normal
 *				case.
 */</span>
				entry = entry-&gt;vme_next;
<span class="enscript-comment">/*
 *				should be contiguous. Fail if we encounter
 *				a hole in the destination.
 */</span>
				<span class="enscript-keyword">if</span> (start != entry-&gt;vme_start) {
					vm_map_unlock_read(dst_map);
					<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS ;
				}
			}
		} <span class="enscript-keyword">else</span> {
<span class="enscript-comment">/*
 *			Map version check failed.
 *			we must lookup the entry because somebody
 *			might have changed the map behind our backs.
 */</span>
		<span class="enscript-reference">RetryLookup</span>:
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(dst_map, start, &amp;entry))
			{
				vm_map_unlock_read(dst_map);
				<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS ;
			}
		}
	}<span class="enscript-comment">/* while */</span>

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}<span class="enscript-comment">/* vm_map_copy_overwrite_unaligned */</span>

<span class="enscript-comment">/*
 *	Routine: vm_map_copy_overwrite_aligned	[internal use only]
 *
 *	Description:
 *	Does all the vm_trickery possible for whole pages.
 *
 *	Implementation:
 *
 *	If there are no permanent objects in the destination,
 *	and the source and destination map entry zones match,
 *	and the destination map entry is not shared,
 *	then the map entries can be deleted and replaced
 *	with those from the copy.  The following code is the
 *	basic idea of what to do, but there are lots of annoying
 *	little details about getting protection and inheritance
 *	right.  Should add protection, inheritance, and sharing checks
 *	to the above pass and make sure that no wiring is involved.
 */</span>

<span class="enscript-type">int</span> vm_map_copy_overwrite_aligned_src_not_internal = 0;
<span class="enscript-type">int</span> vm_map_copy_overwrite_aligned_src_not_symmetric = 0;
<span class="enscript-type">int</span> vm_map_copy_overwrite_aligned_src_large = 0;

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_copy_overwrite_aligned</span>(
	vm_map_t	dst_map,
	vm_map_entry_t	tmp_entry,
	vm_map_copy_t	copy,
	vm_map_offset_t	start,
	__unused pmap_t	pmap)
{
	vm_object_t	object;
	vm_map_entry_t	copy_entry;
	vm_map_size_t	copy_size;
	vm_map_size_t	size;
	vm_map_entry_t	entry;
		
	<span class="enscript-keyword">while</span> ((copy_entry = vm_map_copy_first_entry(copy))
	       != vm_map_copy_to_entry(copy))
	{
		copy_size = (copy_entry-&gt;vme_end - copy_entry-&gt;vme_start);
		
		entry = tmp_entry;
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			<span class="enscript-comment">/* unnested when clipped earlier */</span>
			assert(!entry-&gt;use_pmap);
		}
		<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(dst_map)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}
		size = (entry-&gt;vme_end - entry-&gt;vme_start);
		<span class="enscript-comment">/*
		 *	Make sure that no holes popped up in the
		 *	address map, and that the protection is
		 *	still valid, in case the map was unlocked
		 *	earlier.
		 */</span>

		<span class="enscript-keyword">if</span> ((entry-&gt;vme_start != start) || ((entry-&gt;is_sub_map)
						    &amp;&amp; !entry-&gt;needs_copy)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}
		assert(entry != vm_map_to_entry(dst_map));

		<span class="enscript-comment">/*
		 *	Check protection again
		 */</span>

		<span class="enscript-keyword">if</span> ( ! (entry-&gt;protection &amp; VM_PROT_WRITE)) {
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_PROTECTION_FAILURE);
		}

		<span class="enscript-comment">/*
		 *	Adjust to source size first
		 */</span>

		<span class="enscript-keyword">if</span> (copy_size &lt; size) {
			<span class="enscript-keyword">if</span> (entry-&gt;map_aligned &amp;&amp;
			    !VM_MAP_PAGE_ALIGNED(entry-&gt;vme_start + copy_size,
						 VM_MAP_PAGE_MASK(dst_map))) {
				<span class="enscript-comment">/* no longer map-aligned */</span>
				entry-&gt;map_aligned = FALSE;
			}
			vm_map_clip_end(dst_map, entry, entry-&gt;vme_start + copy_size);
			size = copy_size;
		}

		<span class="enscript-comment">/*
		 *	Adjust to destination size
		 */</span>

		<span class="enscript-keyword">if</span> (size &lt; copy_size) {
			vm_map_copy_clip_end(copy, copy_entry,
					     copy_entry-&gt;vme_start + size);
			copy_size = size;
		}

		assert((entry-&gt;vme_end - entry-&gt;vme_start) == size);
		assert((tmp_entry-&gt;vme_end - tmp_entry-&gt;vme_start) == size);
		assert((copy_entry-&gt;vme_end - copy_entry-&gt;vme_start) == size);

		<span class="enscript-comment">/*
		 *	If the destination contains temporary unshared memory,
		 *	we can perform the copy by throwing it away and
		 *	installing the source data.
		 */</span>

		object = VME_OBJECT(entry);
		<span class="enscript-keyword">if</span> ((!entry-&gt;is_shared &amp;&amp; 
		     ((object == VM_OBJECT_NULL) || 
		      (object-&gt;internal &amp;&amp; !object-&gt;true_share))) ||
		    entry-&gt;needs_copy) {
			vm_object_t	old_object = VME_OBJECT(entry);
			vm_object_offset_t	old_offset = VME_OFFSET(entry);
			vm_object_offset_t	offset;

			<span class="enscript-comment">/*
			 * Ensure that the source and destination aren't
			 * identical
			 */</span>
			<span class="enscript-keyword">if</span> (old_object == VME_OBJECT(copy_entry) &amp;&amp;
			    old_offset == VME_OFFSET(copy_entry)) {
				vm_map_copy_entry_unlink(copy, copy_entry);
				vm_map_copy_entry_dispose(copy, copy_entry);

				<span class="enscript-keyword">if</span> (old_object != VM_OBJECT_NULL)
					vm_object_deallocate(old_object);

				start = tmp_entry-&gt;vme_end;
				tmp_entry = tmp_entry-&gt;vme_next;
				<span class="enscript-keyword">continue</span>;
			}

#<span class="enscript-reference">define</span> <span class="enscript-variable-name">__TRADEOFF1_OBJ_SIZE</span> (64 * 1024 * 1024)	<span class="enscript-comment">/* 64 MB */</span>
#<span class="enscript-reference">define</span> <span class="enscript-variable-name">__TRADEOFF1_COPY_SIZE</span> (128 * 1024)	<span class="enscript-comment">/* 128 KB */</span>
			<span class="enscript-keyword">if</span> (VME_OBJECT(copy_entry) != VM_OBJECT_NULL &amp;&amp;
			    VME_OBJECT(copy_entry)-&gt;vo_size &gt;= __TRADEOFF1_OBJ_SIZE &amp;&amp;
			    copy_size &lt;= __TRADEOFF1_COPY_SIZE) {
				<span class="enscript-comment">/*
				 * Virtual vs. Physical copy tradeoff #1.
				 *
				 * Copying only a few pages out of a large
				 * object:  do a physical copy instead of
				 * a virtual copy, to avoid possibly keeping
				 * the entire large object alive because of
				 * those few copy-on-write pages.
				 */</span>
				vm_map_copy_overwrite_aligned_src_large++;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">slow_copy</span>;
			}

			<span class="enscript-keyword">if</span> ((dst_map-&gt;pmap != kernel_pmap) &amp;&amp;
			    (VME_ALIAS(entry) &gt;= VM_MEMORY_MALLOC) &amp;&amp;
			    (VME_ALIAS(entry) &lt;= VM_MEMORY_MALLOC_LARGE_REUSED)) {
				vm_object_t new_object, new_shadow;

				<span class="enscript-comment">/*
				 * We're about to map something over a mapping
				 * established by malloc()...
				 */</span>
				new_object = VME_OBJECT(copy_entry);
				<span class="enscript-keyword">if</span> (new_object != VM_OBJECT_NULL) {
					vm_object_lock_shared(new_object);
				}
				<span class="enscript-keyword">while</span> (new_object != VM_OBJECT_NULL &amp;&amp;
				       !new_object-&gt;true_share &amp;&amp;
				       new_object-&gt;copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC &amp;&amp;
				       new_object-&gt;internal) {
					new_shadow = new_object-&gt;shadow;
					<span class="enscript-keyword">if</span> (new_shadow == VM_OBJECT_NULL) {
						<span class="enscript-keyword">break</span>;
					}
					vm_object_lock_shared(new_shadow);
					vm_object_unlock(new_object);
					new_object = new_shadow;
				}
				<span class="enscript-keyword">if</span> (new_object != VM_OBJECT_NULL) {
					<span class="enscript-keyword">if</span> (!new_object-&gt;internal) {
						<span class="enscript-comment">/*
						 * The new mapping is backed
						 * by an external object.  We
						 * don't want malloc'ed memory
						 * to be replaced with such a
						 * non-anonymous mapping, so
						 * let's go off the optimized
						 * path...
						 */</span>
						vm_map_copy_overwrite_aligned_src_not_internal++;
						vm_object_unlock(new_object);
						<span class="enscript-keyword">goto</span> <span class="enscript-reference">slow_copy</span>;
					}
					<span class="enscript-keyword">if</span> (new_object-&gt;true_share ||
					    new_object-&gt;copy_strategy != MEMORY_OBJECT_COPY_SYMMETRIC) {
						<span class="enscript-comment">/*
						 * Same if there's a &quot;true_share&quot;
						 * object in the shadow chain, or
						 * an object with a non-default
						 * (SYMMETRIC) copy strategy.
						 */</span>
						vm_map_copy_overwrite_aligned_src_not_symmetric++;
						vm_object_unlock(new_object);
						<span class="enscript-keyword">goto</span> <span class="enscript-reference">slow_copy</span>;
					}
					vm_object_unlock(new_object);
				}
				<span class="enscript-comment">/*
				 * The new mapping is still backed by
				 * anonymous (internal) memory, so it's
				 * OK to substitute it for the original
				 * malloc() mapping.
				 */</span>
			}

			<span class="enscript-keyword">if</span> (old_object != VM_OBJECT_NULL) {
				<span class="enscript-keyword">if</span>(entry-&gt;is_sub_map) {
					<span class="enscript-keyword">if</span>(entry-&gt;use_pmap) {
#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
						pmap_unnest(dst_map-&gt;pmap, 
							    (addr64_t)entry-&gt;vme_start,
							    entry-&gt;vme_end - entry-&gt;vme_start);
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* NO_NESTED_PMAP */</span>
						<span class="enscript-keyword">if</span>(dst_map-&gt;mapped_in_other_pmaps) {
							<span class="enscript-comment">/* clean up parent */</span>
							<span class="enscript-comment">/* map/maps */</span>
							vm_map_submap_pmap_clean(
								dst_map, entry-&gt;vme_start,
								entry-&gt;vme_end,
								VME_SUBMAP(entry),
								VME_OFFSET(entry));
						}
					} <span class="enscript-keyword">else</span> {
						vm_map_submap_pmap_clean(
							dst_map, entry-&gt;vme_start, 
							entry-&gt;vme_end,
							VME_SUBMAP(entry),
							VME_OFFSET(entry));
					}
				   	vm_map_deallocate(VME_SUBMAP(entry));
			   	} <span class="enscript-keyword">else</span> {
					<span class="enscript-keyword">if</span>(dst_map-&gt;mapped_in_other_pmaps) {
						vm_object_pmap_protect_options(
							VME_OBJECT(entry),
							VME_OFFSET(entry),
							entry-&gt;vme_end 
							- entry-&gt;vme_start,
							PMAP_NULL,
							entry-&gt;vme_start,
							VM_PROT_NONE,
							PMAP_OPTIONS_REMOVE);
					} <span class="enscript-keyword">else</span> {
						pmap_remove_options(
							dst_map-&gt;pmap, 
							(addr64_t)(entry-&gt;vme_start), 
							(addr64_t)(entry-&gt;vme_end),
							PMAP_OPTIONS_REMOVE);
					}
					vm_object_deallocate(old_object);
			   	}
			}

			entry-&gt;is_sub_map = FALSE;
			VME_OBJECT_SET(entry, VME_OBJECT(copy_entry));
			object = VME_OBJECT(entry);
			entry-&gt;needs_copy = copy_entry-&gt;needs_copy;
			entry-&gt;wired_count = 0;
			entry-&gt;user_wired_count = 0;
			offset = VME_OFFSET(copy_entry);
			VME_OFFSET_SET(entry, offset); 

			vm_map_copy_entry_unlink(copy, copy_entry);
			vm_map_copy_entry_dispose(copy, copy_entry);

			<span class="enscript-comment">/*
			 * we could try to push pages into the pmap at this point, BUT
			 * this optimization only saved on average 2 us per page if ALL
			 * the pages in the source were currently mapped
			 * and ALL the pages in the dest were touched, if there were fewer
			 * than 2/3 of the pages touched, this optimization actually cost more cycles
			 * it also puts a lot of pressure on the pmap layer w/r to mapping structures
			 */</span>

			<span class="enscript-comment">/*
			 *	Set up for the next iteration.  The map
			 *	has not been unlocked, so the next
			 *	address should be at the end of this
			 *	entry, and the next map entry should be
			 *	the one following it.
			 */</span>

			start = tmp_entry-&gt;vme_end;
			tmp_entry = tmp_entry-&gt;vme_next;
		} <span class="enscript-keyword">else</span> {
			vm_map_version_t	version;
			vm_object_t		dst_object;
			vm_object_offset_t	dst_offset;
			kern_return_t		r;

		<span class="enscript-reference">slow_copy</span>:
			<span class="enscript-keyword">if</span> (entry-&gt;needs_copy) {
				VME_OBJECT_SHADOW(entry,
						  (entry-&gt;vme_end -
						   entry-&gt;vme_start));
				entry-&gt;needs_copy = FALSE;
			}

			dst_object = VME_OBJECT(entry);
			dst_offset = VME_OFFSET(entry);

			<span class="enscript-comment">/*
			 *	Take an object reference, and record
			 *	the map version information so that the
			 *	map can be safely unlocked.
			 */</span>

			<span class="enscript-keyword">if</span> (dst_object == VM_OBJECT_NULL) {
				<span class="enscript-comment">/*
				 * We would usually have just taken the
				 * optimized path above if the destination
				 * object has not been allocated yet.  But we
				 * now disable that optimization if the copy
				 * entry's object is not backed by anonymous
				 * memory to avoid replacing malloc'ed
				 * (i.e. re-usable) anonymous memory with a
				 * not-so-anonymous mapping.
				 * So we have to handle this case here and
				 * allocate a new VM object for this map entry.
				 */</span>
				dst_object = vm_object_allocate(
					entry-&gt;vme_end - entry-&gt;vme_start);
				dst_offset = 0;
				VME_OBJECT_SET(entry, dst_object);
				VME_OFFSET_SET(entry, dst_offset);
				assert(entry-&gt;use_pmap);
				
			}

			vm_object_reference(dst_object);

			<span class="enscript-comment">/* account for unlock bumping up timestamp */</span>
			version.main_timestamp = dst_map-&gt;timestamp + 1;

			vm_map_unlock(dst_map);

			<span class="enscript-comment">/*
			 *	Copy as much as possible in one pass
			 */</span>

			copy_size = size;
			r = vm_fault_copy(
				VME_OBJECT(copy_entry),
				VME_OFFSET(copy_entry),
				&amp;copy_size,
				dst_object,
				dst_offset,
				dst_map,
				&amp;version,
				THREAD_UNINT );

			<span class="enscript-comment">/*
			 *	Release the object reference
			 */</span>

			vm_object_deallocate(dst_object);

			<span class="enscript-comment">/*
			 *	If a hard error occurred, return it now
			 */</span>

			<span class="enscript-keyword">if</span> (r != KERN_SUCCESS)
				<span class="enscript-keyword">return</span>(r);

			<span class="enscript-keyword">if</span> (copy_size != 0) {
				<span class="enscript-comment">/*
				 *	Dispose of the copied region
				 */</span>

				vm_map_copy_clip_end(copy, copy_entry,
						     copy_entry-&gt;vme_start + copy_size);
				vm_map_copy_entry_unlink(copy, copy_entry);
				vm_object_deallocate(VME_OBJECT(copy_entry));
				vm_map_copy_entry_dispose(copy, copy_entry);
			}

			<span class="enscript-comment">/*
			 *	Pick up in the destination map where we left off.
			 *
			 *	Use the version information to avoid a lookup
			 *	in the normal case.
			 */</span>

			start += copy_size;
			vm_map_lock(dst_map);
			<span class="enscript-keyword">if</span> (version.main_timestamp == dst_map-&gt;timestamp &amp;&amp;
			    copy_size != 0) {
				<span class="enscript-comment">/* We can safely use saved tmp_entry value */</span>

				<span class="enscript-keyword">if</span> (tmp_entry-&gt;map_aligned &amp;&amp;
				    !VM_MAP_PAGE_ALIGNED(
					    start,
					    VM_MAP_PAGE_MASK(dst_map))) {
					<span class="enscript-comment">/* no longer map-aligned */</span>
					tmp_entry-&gt;map_aligned = FALSE;
				}
				vm_map_clip_end(dst_map, tmp_entry, start);
				tmp_entry = tmp_entry-&gt;vme_next;
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/* Must do lookup of tmp_entry */</span>

				<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(dst_map, start, &amp;tmp_entry)) {
					vm_map_unlock(dst_map);
					<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
				}
				<span class="enscript-keyword">if</span> (tmp_entry-&gt;map_aligned &amp;&amp;
				    !VM_MAP_PAGE_ALIGNED(
					    start,
					    VM_MAP_PAGE_MASK(dst_map))) {
					<span class="enscript-comment">/* no longer map-aligned */</span>
					tmp_entry-&gt;map_aligned = FALSE;
				}
				vm_map_clip_start(dst_map, tmp_entry, start);
			}
		}
	}<span class="enscript-comment">/* while */</span>

	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}<span class="enscript-comment">/* vm_map_copy_overwrite_aligned */</span>

<span class="enscript-comment">/*
 *	Routine: vm_map_copyin_kernel_buffer [internal use only]
 *
 *	Description:
 *		Copy in data to a kernel buffer from space in the
 *		source map. The original space may be optionally
 *		deallocated.
 *
 *		If successful, returns a new copy object.
 */</span>
<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_copyin_kernel_buffer</span>(
	vm_map_t	src_map,
	vm_map_offset_t	src_addr,
	vm_map_size_t	len,
	boolean_t	src_destroy,
	vm_map_copy_t	*copy_result)
{
	kern_return_t kr;
	vm_map_copy_t copy;
	vm_size_t kalloc_size;

	<span class="enscript-keyword">if</span> (len &gt; msg_ool_size_small)
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;

	kalloc_size = (vm_size_t)(cpy_kdata_hdr_sz + len);

	copy = (vm_map_copy_t)kalloc(kalloc_size);
	<span class="enscript-keyword">if</span> (copy == VM_MAP_COPY_NULL)
		<span class="enscript-keyword">return</span> KERN_RESOURCE_SHORTAGE;
	copy-&gt;type = VM_MAP_COPY_KERNEL_BUFFER;
	copy-&gt;size = len;
	copy-&gt;offset = 0;

	kr = copyinmap(src_map, src_addr, copy-&gt;cpy_kdata, (vm_size_t)len);
	<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
		kfree(copy, kalloc_size);
		<span class="enscript-keyword">return</span> kr;
	}
	<span class="enscript-keyword">if</span> (src_destroy) {
		(<span class="enscript-type">void</span>) vm_map_remove(
			src_map,
			vm_map_trunc_page(src_addr,
					  VM_MAP_PAGE_MASK(src_map)), 
			vm_map_round_page(src_addr + len,
					  VM_MAP_PAGE_MASK(src_map)),
			(VM_MAP_REMOVE_INTERRUPTIBLE |
			 VM_MAP_REMOVE_WAIT_FOR_KWIRE |
			 (src_map == kernel_map) ? VM_MAP_REMOVE_KUNWIRE : 0));
	}
	*copy_result = copy;
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 *	Routine: vm_map_copyout_kernel_buffer	[internal use only]
 *
 *	Description:
 *		Copy out data from a kernel buffer into space in the
 *		destination map. The space may be otpionally dynamically
 *		allocated.
 *
 *		If successful, consumes the copy object.
 *		Otherwise, the caller is responsible for it.
 */</span>
<span class="enscript-type">static</span> <span class="enscript-type">int</span> vm_map_copyout_kernel_buffer_failures = 0;
<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_copyout_kernel_buffer</span>(
	vm_map_t		map,
	vm_map_address_t	*addr,	<span class="enscript-comment">/* IN/OUT */</span>
	vm_map_copy_t		copy,
	boolean_t		overwrite,
	boolean_t		consume_on_success)
{
	kern_return_t kr = KERN_SUCCESS;
	thread_t thread = current_thread();

	<span class="enscript-comment">/*
	 * check for corrupted vm_map_copy structure
	 */</span>
	<span class="enscript-keyword">if</span> (copy-&gt;size &gt; msg_ool_size_small || copy-&gt;offset)
		panic(<span class="enscript-string">&quot;Invalid vm_map_copy_t sz:%lld, ofst:%lld&quot;</span>,
		      (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)copy-&gt;size, (<span class="enscript-type">long</span> <span class="enscript-type">long</span>)copy-&gt;offset);

	<span class="enscript-keyword">if</span> (!overwrite) {

		<span class="enscript-comment">/*
		 * Allocate space in the target map for the data
		 */</span>
		*addr = 0;
		kr = vm_map_enter(map, 
				  addr, 
				  vm_map_round_page(copy-&gt;size,
						    VM_MAP_PAGE_MASK(map)),
				  (vm_map_offset_t) 0, 
				  VM_FLAGS_ANYWHERE,
				  VM_OBJECT_NULL, 
				  (vm_object_offset_t) 0, 
				  FALSE,
				  VM_PROT_DEFAULT, 
				  VM_PROT_ALL,
				  VM_INHERIT_DEFAULT);
		<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS)
			<span class="enscript-keyword">return</span> kr;
	}

	<span class="enscript-comment">/*
	 * Copyout the data from the kernel buffer to the target map.
	 */</span>	
	<span class="enscript-keyword">if</span> (thread-&gt;map == map) {
	
		<span class="enscript-comment">/*
		 * If the target map is the current map, just do
		 * the copy.
		 */</span>
		assert((vm_size_t) copy-&gt;size == copy-&gt;size);
		<span class="enscript-keyword">if</span> (copyout(copy-&gt;cpy_kdata, *addr, (vm_size_t) copy-&gt;size)) {
			kr = KERN_INVALID_ADDRESS;
		}
	}
	<span class="enscript-keyword">else</span> {
		vm_map_t oldmap;

		<span class="enscript-comment">/*
		 * If the target map is another map, assume the
		 * target's address space identity for the duration
		 * of the copy.
		 */</span>
		vm_map_reference(map);
		oldmap = vm_map_switch(map);

		assert((vm_size_t) copy-&gt;size == copy-&gt;size);
		<span class="enscript-keyword">if</span> (copyout(copy-&gt;cpy_kdata, *addr, (vm_size_t) copy-&gt;size)) {
			vm_map_copyout_kernel_buffer_failures++;
			kr = KERN_INVALID_ADDRESS;
		}
	
		(<span class="enscript-type">void</span>) vm_map_switch(oldmap);
		vm_map_deallocate(map);
	}

	<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
		<span class="enscript-comment">/* the copy failed, clean up */</span>
		<span class="enscript-keyword">if</span> (!overwrite) {
			<span class="enscript-comment">/*
			 * Deallocate the space we allocated in the target map.
			 */</span>
			(<span class="enscript-type">void</span>) vm_map_remove(
				map,
				vm_map_trunc_page(*addr,
						  VM_MAP_PAGE_MASK(map)),
				vm_map_round_page((*addr +
						   vm_map_round_page(copy-&gt;size,
								     VM_MAP_PAGE_MASK(map))),
						  VM_MAP_PAGE_MASK(map)),
				VM_MAP_NO_FLAGS);
			*addr = 0;
		}
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-comment">/* copy was successful, dicard the copy structure */</span>
		<span class="enscript-keyword">if</span> (consume_on_success) {
			kfree(copy, copy-&gt;size + cpy_kdata_hdr_sz);
		}
	}

	<span class="enscript-keyword">return</span> kr;
}
		
<span class="enscript-comment">/*
 *	Macro:		vm_map_copy_insert
 *	
 *	Description:
 *		Link a copy chain (&quot;copy&quot;) into a map at the
 *		specified location (after &quot;where&quot;).
 *	Side effects:
 *		The copy chain is destroyed.
 *	Warning:
 *		The arguments are evaluated multiple times.
 */</span>
#<span class="enscript-reference">define</span>	<span class="enscript-function-name">vm_map_copy_insert</span>(map, where, copy)				\
MACRO_BEGIN								\
	vm_map_store_copy_insert(map, where, copy);	  \
	zfree(vm_map_copy_zone, copy);		\
MACRO_END

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_copy_remap</span>(
	vm_map_t	map,
	vm_map_entry_t	where,
	vm_map_copy_t	copy,
	vm_map_offset_t	adjustment,
	vm_prot_t	cur_prot,
	vm_prot_t	max_prot,
	vm_inherit_t	inheritance)
{
	vm_map_entry_t	copy_entry, new_entry;

	<span class="enscript-keyword">for</span> (copy_entry = vm_map_copy_first_entry(copy);
	     copy_entry != vm_map_copy_to_entry(copy);
	     copy_entry = copy_entry-&gt;vme_next) {
		<span class="enscript-comment">/* get a new VM map entry for the map */</span>
		new_entry = vm_map_entry_create(map,
						!map-&gt;hdr.entries_pageable);
		<span class="enscript-comment">/* copy the &quot;copy entry&quot; to the new entry */</span>
		vm_map_entry_copy(new_entry, copy_entry);
		<span class="enscript-comment">/* adjust &quot;start&quot; and &quot;end&quot; */</span>
		new_entry-&gt;vme_start += adjustment;
		new_entry-&gt;vme_end += adjustment;
		<span class="enscript-comment">/* clear some attributes */</span>
		new_entry-&gt;inheritance = inheritance;
		new_entry-&gt;protection = cur_prot;
		new_entry-&gt;max_protection = max_prot;
		new_entry-&gt;behavior = VM_BEHAVIOR_DEFAULT;
		<span class="enscript-comment">/* take an extra reference on the entry's &quot;object&quot; */</span>
		<span class="enscript-keyword">if</span> (new_entry-&gt;is_sub_map) {
			assert(!new_entry-&gt;use_pmap); <span class="enscript-comment">/* not nested */</span>
			vm_map_lock(VME_SUBMAP(new_entry));
			vm_map_reference(VME_SUBMAP(new_entry));
			vm_map_unlock(VME_SUBMAP(new_entry));
		} <span class="enscript-keyword">else</span> {
			vm_object_reference(VME_OBJECT(new_entry));
		}
		<span class="enscript-comment">/* insert the new entry in the map */</span>
		vm_map_store_entry_link(map, where, new_entry);
		<span class="enscript-comment">/* continue inserting the &quot;copy entries&quot; after the new entry */</span>
		where = new_entry;
	}
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_copyout
 *
 *	Description:
 *		Copy out a copy chain (&quot;copy&quot;) into newly-allocated
 *		space in the destination map.
 *
 *		If successful, consumes the copy object.
 *		Otherwise, the caller is responsible for it.
 */</span>

kern_return_t
<span class="enscript-function-name">vm_map_copyout</span>(
	vm_map_t		dst_map,
	vm_map_address_t	*dst_addr,	<span class="enscript-comment">/* OUT */</span>
	vm_map_copy_t		copy)
{
	<span class="enscript-keyword">return</span> vm_map_copyout_internal(dst_map, dst_addr, copy,
				       TRUE, <span class="enscript-comment">/* consume_on_success */</span>
				       VM_PROT_DEFAULT,
				       VM_PROT_ALL,
				       VM_INHERIT_DEFAULT);
}

kern_return_t
<span class="enscript-function-name">vm_map_copyout_internal</span>(
	vm_map_t		dst_map,
	vm_map_address_t	*dst_addr,	<span class="enscript-comment">/* OUT */</span>
	vm_map_copy_t		copy,
	boolean_t		consume_on_success,
	vm_prot_t		cur_protection,
	vm_prot_t		max_protection,
	vm_inherit_t		inheritance)
{
	vm_map_size_t		size;
	vm_map_size_t		adjustment;
	vm_map_offset_t		start;
	vm_object_offset_t	vm_copy_start;
	vm_map_entry_t		last;
	vm_map_entry_t		entry;
	vm_map_entry_t		hole_entry;

	<span class="enscript-comment">/*
	 *	Check for null copy object.
	 */</span>

	<span class="enscript-keyword">if</span> (copy == VM_MAP_COPY_NULL) {
		*dst_addr = 0;
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}

	<span class="enscript-comment">/*
	 *	Check for special copy object, created
	 *	by vm_map_copyin_object.
	 */</span>

	<span class="enscript-keyword">if</span> (copy-&gt;type == VM_MAP_COPY_OBJECT) {
		vm_object_t 		object = copy-&gt;cpy_object;
		kern_return_t 		kr;
		vm_object_offset_t	offset;

		offset = vm_object_trunc_page(copy-&gt;offset);
		size = vm_map_round_page((copy-&gt;size + 
					  (vm_map_size_t)(copy-&gt;offset -
							  offset)),
					 VM_MAP_PAGE_MASK(dst_map));
		*dst_addr = 0;
		kr = vm_map_enter(dst_map, dst_addr, size,
				  (vm_map_offset_t) 0, VM_FLAGS_ANYWHERE,
				  object, offset, FALSE,
				  VM_PROT_DEFAULT, VM_PROT_ALL,
				  VM_INHERIT_DEFAULT);
		<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS)
			<span class="enscript-keyword">return</span>(kr);
		<span class="enscript-comment">/* Account for non-pagealigned copy object */</span>
		*dst_addr += (vm_map_offset_t)(copy-&gt;offset - offset);
		<span class="enscript-keyword">if</span> (consume_on_success)
			zfree(vm_map_copy_zone, copy);
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}

	<span class="enscript-comment">/*
	 *	Check for special kernel buffer allocated
	 *	by new_ipc_kmsg_copyin.
	 */</span>

	<span class="enscript-keyword">if</span> (copy-&gt;type == VM_MAP_COPY_KERNEL_BUFFER) {
		<span class="enscript-keyword">return</span> vm_map_copyout_kernel_buffer(dst_map, dst_addr, 
						    copy, FALSE,
						    consume_on_success);
	}


	<span class="enscript-comment">/*
	 *	Find space for the data
	 */</span>

	vm_copy_start = vm_map_trunc_page((vm_map_size_t)copy-&gt;offset,
					  VM_MAP_COPY_PAGE_MASK(copy));
	size = vm_map_round_page((vm_map_size_t)copy-&gt;offset + copy-&gt;size,
				 VM_MAP_COPY_PAGE_MASK(copy))
		- vm_copy_start;


<span class="enscript-reference">StartAgain</span>: ;

	vm_map_lock(dst_map);
	<span class="enscript-keyword">if</span>( dst_map-&gt;disable_vmentry_reuse == TRUE) {
		VM_MAP_HIGHEST_ENTRY(dst_map, entry, start);
		last = entry;
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-keyword">if</span> (dst_map-&gt;holelistenabled) {
			hole_entry = (vm_map_entry_t)dst_map-&gt;holes_list;

			<span class="enscript-keyword">if</span> (hole_entry == NULL) {
				<span class="enscript-comment">/*
				 * No more space in the map?
				 */</span>
				vm_map_unlock(dst_map);
				<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
			}

			last = hole_entry;
			start = last-&gt;vme_start;
		} <span class="enscript-keyword">else</span> {
			assert(first_free_is_valid(dst_map));
			start = ((last = dst_map-&gt;first_free) == vm_map_to_entry(dst_map)) ?
			vm_map_min(dst_map) : last-&gt;vme_end;
		}
		start = vm_map_round_page(start,
					  VM_MAP_PAGE_MASK(dst_map));
	}

	<span class="enscript-keyword">while</span> (TRUE) {
		vm_map_entry_t	next = last-&gt;vme_next;
		vm_map_offset_t	end = start + size;

		<span class="enscript-keyword">if</span> ((end &gt; dst_map-&gt;max_offset) || (end &lt; start)) {
			<span class="enscript-keyword">if</span> (dst_map-&gt;wait_for_space) {
				<span class="enscript-keyword">if</span> (size &lt;= (dst_map-&gt;max_offset - dst_map-&gt;min_offset)) {
					assert_wait((event_t) dst_map,
						    THREAD_INTERRUPTIBLE);
					vm_map_unlock(dst_map);
					thread_block(THREAD_CONTINUE_NULL);
					<span class="enscript-keyword">goto</span> <span class="enscript-reference">StartAgain</span>;
				}
			}
			vm_map_unlock(dst_map);
			<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
		}

		<span class="enscript-keyword">if</span> (dst_map-&gt;holelistenabled) {
			<span class="enscript-keyword">if</span> (last-&gt;vme_end &gt;= end)
				<span class="enscript-keyword">break</span>;
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/*
			 *	If there are no more entries, we must win.
			 *
			 *	OR
			 *
			 *	If there is another entry, it must be
			 *	after the end of the potential new region.
			 */</span>

			<span class="enscript-keyword">if</span> (next == vm_map_to_entry(dst_map))
				<span class="enscript-keyword">break</span>;

			<span class="enscript-keyword">if</span> (next-&gt;vme_start &gt;= end)
				<span class="enscript-keyword">break</span>;
		}

		last = next;

		<span class="enscript-keyword">if</span> (dst_map-&gt;holelistenabled) {
			<span class="enscript-keyword">if</span> (last == (vm_map_entry_t) dst_map-&gt;holes_list) {
				<span class="enscript-comment">/*
				 * Wrapped around
				 */</span>
				vm_map_unlock(dst_map);
				<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
			}
			start = last-&gt;vme_start;
		} <span class="enscript-keyword">else</span> {
			start = last-&gt;vme_end;
		}
		start = vm_map_round_page(start,
					  VM_MAP_PAGE_MASK(dst_map));
	}

	<span class="enscript-keyword">if</span> (dst_map-&gt;holelistenabled) {
		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(dst_map, last-&gt;vme_start, &amp;last)) {
			panic(<span class="enscript-string">&quot;Found an existing entry (%p) instead of potential hole at address: 0x%llx.\n&quot;</span>, last, (<span class="enscript-type">unsigned</span> <span class="enscript-type">long</span> <span class="enscript-type">long</span>)last-&gt;vme_start);
		}
	}


	adjustment = start - vm_copy_start;
	<span class="enscript-keyword">if</span> (! consume_on_success) {
		<span class="enscript-comment">/*
		 * We're not allowed to consume &quot;copy&quot;, so we'll have to
		 * copy its map entries into the destination map below.
		 * No need to re-allocate map entries from the correct
		 * (pageable or not) zone, since we'll get new map entries
		 * during the transfer.
		 * We'll also adjust the map entries's &quot;start&quot; and &quot;end&quot;
		 * during the transfer, to keep &quot;copy&quot;'s entries consistent
		 * with its &quot;offset&quot;.
		 */</span>
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">after_adjustments</span>;
	}

	<span class="enscript-comment">/*
	 *	Since we're going to just drop the map
	 *	entries from the copy into the destination
	 *	map, they must come from the same pool.
	 */</span>

	<span class="enscript-keyword">if</span> (copy-&gt;cpy_hdr.entries_pageable != dst_map-&gt;hdr.entries_pageable) {
		<span class="enscript-comment">/*
		 * Mismatches occur when dealing with the default
		 * pager.
		 */</span>
		zone_t		old_zone;
		vm_map_entry_t	next, new;

		<span class="enscript-comment">/*
		 * Find the zone that the copies were allocated from
		 */</span>

		entry = vm_map_copy_first_entry(copy);

		<span class="enscript-comment">/*
		 * Reinitialize the copy so that vm_map_copy_entry_link
		 * will work.
		 */</span>
		vm_map_store_copy_reset(copy, entry);
		copy-&gt;cpy_hdr.entries_pageable = dst_map-&gt;hdr.entries_pageable;

		<span class="enscript-comment">/*
		 * Copy each entry.
		 */</span>
		<span class="enscript-keyword">while</span> (entry != vm_map_copy_to_entry(copy)) {
			new = vm_map_copy_entry_create(copy, !copy-&gt;cpy_hdr.entries_pageable);
			vm_map_entry_copy_full(new, entry);
			assert(!new-&gt;iokit_acct);
			<span class="enscript-keyword">if</span> (new-&gt;is_sub_map) {
				<span class="enscript-comment">/* clr address space specifics */</span>
				new-&gt;use_pmap = FALSE;
			}
			vm_map_copy_entry_link(copy,
					       vm_map_copy_last_entry(copy),
					       new);
			next = entry-&gt;vme_next;
			old_zone = entry-&gt;from_reserved_zone ? vm_map_entry_reserved_zone : vm_map_entry_zone;
			zfree(old_zone, entry);
			entry = next;
		}
	}

	<span class="enscript-comment">/*
	 *	Adjust the addresses in the copy chain, and
	 *	reset the region attributes.
	 */</span>

	<span class="enscript-keyword">for</span> (entry = vm_map_copy_first_entry(copy);
	     entry != vm_map_copy_to_entry(copy);
	     entry = entry-&gt;vme_next) {
		<span class="enscript-keyword">if</span> (VM_MAP_PAGE_SHIFT(dst_map) == PAGE_SHIFT) {
			<span class="enscript-comment">/*
			 * We're injecting this copy entry into a map that
			 * has the standard page alignment, so clear
			 * &quot;map_aligned&quot; (which might have been inherited
			 * from the original map entry).
			 */</span>
			entry-&gt;map_aligned = FALSE;
		}

		entry-&gt;vme_start += adjustment;
		entry-&gt;vme_end += adjustment;

		<span class="enscript-keyword">if</span> (entry-&gt;map_aligned) {
			assert(VM_MAP_PAGE_ALIGNED(entry-&gt;vme_start,
						   VM_MAP_PAGE_MASK(dst_map)));
			assert(VM_MAP_PAGE_ALIGNED(entry-&gt;vme_end,
						   VM_MAP_PAGE_MASK(dst_map)));
		}

		entry-&gt;inheritance = VM_INHERIT_DEFAULT;
		entry-&gt;protection = VM_PROT_DEFAULT;
		entry-&gt;max_protection = VM_PROT_ALL;
		entry-&gt;behavior = VM_BEHAVIOR_DEFAULT;

		<span class="enscript-comment">/*
		 * If the entry is now wired,
		 * map the pages into the destination map.
		 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;wired_count != 0) {
			<span class="enscript-type">register</span> vm_map_offset_t va;
			vm_object_offset_t	 offset;
			<span class="enscript-type">register</span> vm_object_t object;
			vm_prot_t prot;
			<span class="enscript-type">int</span>	type_of_fault;

			object = VME_OBJECT(entry);
			offset = VME_OFFSET(entry);
			va = entry-&gt;vme_start;

			pmap_pageable(dst_map-&gt;pmap,
				      entry-&gt;vme_start,
				      entry-&gt;vme_end,
				      TRUE);

			<span class="enscript-keyword">while</span> (va &lt; entry-&gt;vme_end) {
				<span class="enscript-type">register</span> vm_page_t	m;

				<span class="enscript-comment">/*
				 * Look up the page in the object.
				 * Assert that the page will be found in the
				 * top object:
				 * either
				 *	the object was newly created by
				 *	vm_object_copy_slowly, and has
				 *	copies of all of the pages from
				 *	the source object
				 * or
				 *	the object was moved from the old
				 *	map entry; because the old map
				 *	entry was wired, all of the pages
				 *	were in the top-level object.
				 *	(XXX not true if we wire pages for
				 *	 reading)
				 */</span>
				vm_object_lock(object);

				m = vm_page_lookup(object, offset);
				<span class="enscript-keyword">if</span> (m == VM_PAGE_NULL || !VM_PAGE_WIRED(m) ||
				    m-&gt;absent)
					panic(<span class="enscript-string">&quot;vm_map_copyout: wiring %p&quot;</span>, m);

				<span class="enscript-comment">/*
				 * ENCRYPTED SWAP:
				 * The page is assumed to be wired here, so it
				 * shouldn't be encrypted.  Otherwise, we
				 * couldn't enter it in the page table, since
				 * we don't want the user to see the encrypted
				 * data.
				 */</span>
				ASSERT_PAGE_DECRYPTED(m);

				prot = entry-&gt;protection;

				<span class="enscript-keyword">if</span> (override_nx(dst_map, VME_ALIAS(entry)) &amp;&amp;
				    prot)
				        prot |= VM_PROT_EXECUTE;

				type_of_fault = DBG_CACHE_HIT_FAULT;

				vm_fault_enter(m, dst_map-&gt;pmap, va, prot, prot,
					       VM_PAGE_WIRED(m), FALSE, FALSE,
					       FALSE, VME_ALIAS(entry),
					       ((entry-&gt;iokit_acct ||
						 (!entry-&gt;is_sub_map &amp;&amp;
						  !entry-&gt;use_pmap))
						? PMAP_OPTIONS_ALT_ACCT
						: 0),
					       NULL, &amp;type_of_fault);

				vm_object_unlock(object);

				offset += PAGE_SIZE_64;
				va += PAGE_SIZE;
			}
		}
	}

<span class="enscript-reference">after_adjustments</span>:

	<span class="enscript-comment">/*
	 *	Correct the page alignment for the result
	 */</span>

	*dst_addr = start + (copy-&gt;offset - vm_copy_start);

	<span class="enscript-comment">/*
	 *	Update the hints and the map size
	 */</span>

	<span class="enscript-keyword">if</span> (consume_on_success) {
		SAVE_HINT_MAP_WRITE(dst_map, vm_map_copy_last_entry(copy));
	} <span class="enscript-keyword">else</span> {
		SAVE_HINT_MAP_WRITE(dst_map, last);
	}

	dst_map-&gt;size += size;

	<span class="enscript-comment">/*
	 *	Link in the copy
	 */</span>

	<span class="enscript-keyword">if</span> (consume_on_success) {
		vm_map_copy_insert(dst_map, last, copy);
	} <span class="enscript-keyword">else</span> {
		vm_map_copy_remap(dst_map, last, copy, adjustment,
				  cur_protection, max_protection,
				  inheritance);
	}

	vm_map_unlock(dst_map);

	<span class="enscript-comment">/*
	 * XXX	If wiring_required, call vm_map_pageable
	 */</span>

	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_copyin
 *
 *	Description:
 *		see vm_map_copyin_common.  Exported via Unsupported.exports.
 *
 */</span>

#<span class="enscript-reference">undef</span> <span class="enscript-variable-name">vm_map_copyin</span>

kern_return_t
<span class="enscript-function-name">vm_map_copyin</span>(
	vm_map_t			src_map,
	vm_map_address_t	src_addr,
	vm_map_size_t		len,
	boolean_t			src_destroy,
	vm_map_copy_t		*copy_result)	<span class="enscript-comment">/* OUT */</span>
{
	<span class="enscript-keyword">return</span>(vm_map_copyin_common(src_map, src_addr, len, src_destroy,
					FALSE, copy_result, FALSE));
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_copyin_common
 *
 *	Description:
 *		Copy the specified region (src_addr, len) from the
 *		source address space (src_map), possibly removing
 *		the region from the source address space (src_destroy).
 *
 *	Returns:
 *		A vm_map_copy_t object (copy_result), suitable for
 *		insertion into another address space (using vm_map_copyout),
 *		copying over another address space region (using
 *		vm_map_copy_overwrite).  If the copy is unused, it
 *		should be destroyed (using vm_map_copy_discard).
 *
 *	In/out conditions:
 *		The source map should not be locked on entry.
 */</span>

<span class="enscript-type">typedef</span> <span class="enscript-type">struct</span> submap_map {
	vm_map_t	parent_map;
	vm_map_offset_t	base_start;
	vm_map_offset_t	base_end;
	vm_map_size_t	base_len;
	<span class="enscript-type">struct</span> submap_map *next;
} submap_map_t;

kern_return_t
<span class="enscript-function-name">vm_map_copyin_common</span>(
	vm_map_t	src_map,
	vm_map_address_t src_addr,
	vm_map_size_t	len,
	boolean_t	src_destroy,
	__unused boolean_t	src_volatile,
	vm_map_copy_t	*copy_result,	<span class="enscript-comment">/* OUT */</span>
	boolean_t	use_maxprot)
{
	vm_map_entry_t	tmp_entry;	<span class="enscript-comment">/* Result of last map lookup --
					 * in multi-level lookup, this
					 * entry contains the actual
					 * vm_object/offset.
					 */</span>
	<span class="enscript-type">register</span>
	vm_map_entry_t	new_entry = VM_MAP_ENTRY_NULL;	<span class="enscript-comment">/* Map entry for copy */</span>

	vm_map_offset_t	src_start;	<span class="enscript-comment">/* Start of current entry --
					 * where copy is taking place now
					 */</span>
	vm_map_offset_t	src_end;	<span class="enscript-comment">/* End of entire region to be
					 * copied */</span>
	vm_map_offset_t src_base;
	vm_map_t	base_map = src_map;
	boolean_t	map_share=FALSE;
	submap_map_t	*parent_maps = NULL;

	<span class="enscript-type">register</span>
	vm_map_copy_t	copy;		<span class="enscript-comment">/* Resulting copy */</span>
	vm_map_address_t copy_addr;
	vm_map_size_t	copy_size;

	<span class="enscript-comment">/*
	 *	Check for copies of zero bytes.
	 */</span>

	<span class="enscript-keyword">if</span> (len == 0) {
		*copy_result = VM_MAP_COPY_NULL;
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}

	<span class="enscript-comment">/*
	 *	Check that the end address doesn't overflow
	 */</span>
	src_end = src_addr + len;
	<span class="enscript-keyword">if</span> (src_end &lt; src_addr)
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;

	<span class="enscript-comment">/*
	 * If the copy is sufficiently small, use a kernel buffer instead
	 * of making a virtual copy.  The theory being that the cost of
	 * setting up VM (and taking C-O-W faults) dominates the copy costs
	 * for small regions.
	 */</span>
	<span class="enscript-keyword">if</span> ((len &lt; msg_ool_size_small) &amp;&amp; !use_maxprot)
		<span class="enscript-keyword">return</span> vm_map_copyin_kernel_buffer(src_map, src_addr, len,
						   src_destroy, copy_result);

	<span class="enscript-comment">/*
	 *	Compute (page aligned) start and end of region
	 */</span>
	src_start = vm_map_trunc_page(src_addr,
				      VM_MAP_PAGE_MASK(src_map));
	src_end = vm_map_round_page(src_end,
				    VM_MAP_PAGE_MASK(src_map));

	XPR(XPR_VM_MAP, <span class="enscript-string">&quot;vm_map_copyin_common map 0x%x addr 0x%x len 0x%x dest %d\n&quot;</span>, src_map, src_addr, len, src_destroy, 0);

	<span class="enscript-comment">/*
	 *	Allocate a header element for the list.
	 *
	 *	Use the start and end in the header to 
	 *	remember the endpoints prior to rounding.
	 */</span>

	copy = (vm_map_copy_t) zalloc(vm_map_copy_zone);
	copy-&gt;c_u.hdr.rb_head_store.rbh_root = (<span class="enscript-type">void</span>*)(<span class="enscript-type">int</span>)SKIP_RB_TREE;
	vm_map_copy_first_entry(copy) =
		vm_map_copy_last_entry(copy) = vm_map_copy_to_entry(copy);
	copy-&gt;type = VM_MAP_COPY_ENTRY_LIST;
	copy-&gt;cpy_hdr.nentries = 0;
	copy-&gt;cpy_hdr.entries_pageable = TRUE;
#<span class="enscript-reference">if</span> 00
	copy-&gt;cpy_hdr.page_shift = src_map-&gt;hdr.page_shift;
#<span class="enscript-reference">else</span>
	<span class="enscript-comment">/*
	 * The copy entries can be broken down for a variety of reasons,
	 * so we can't guarantee that they will remain map-aligned...
	 * Will need to adjust the first copy_entry's &quot;vme_start&quot; and
	 * the last copy_entry's &quot;vme_end&quot; to be rounded to PAGE_MASK
	 * rather than the original map's alignment.
	 */</span>
	copy-&gt;cpy_hdr.page_shift = PAGE_SHIFT;
#<span class="enscript-reference">endif</span>

	vm_map_store_init( &amp;(copy-&gt;cpy_hdr) );

	copy-&gt;offset = src_addr;
	copy-&gt;size = len;
	
	new_entry = vm_map_copy_entry_create(copy, !copy-&gt;cpy_hdr.entries_pageable);

#<span class="enscript-reference">define</span>	<span class="enscript-function-name">RETURN</span>(x)						\
	MACRO_BEGIN						\
	vm_map_unlock(src_map);					\
	<span class="enscript-keyword">if</span>(src_map != base_map)					\
		vm_map_deallocate(src_map);			\
	<span class="enscript-keyword">if</span> (new_entry != VM_MAP_ENTRY_NULL)			\
		vm_map_copy_entry_dispose(copy,new_entry);	\
	vm_map_copy_discard(copy);				\
	{							\
		submap_map_t	*_ptr;				\
								\
		<span class="enscript-keyword">for</span>(_ptr = parent_maps; _ptr != NULL; _ptr = parent_maps) { \
			parent_maps=parent_maps-&gt;next;		\
			<span class="enscript-keyword">if</span> (_ptr-&gt;parent_map != base_map)	\
				vm_map_deallocate(_ptr-&gt;parent_map);	\
			kfree(_ptr, <span class="enscript-keyword">sizeof</span>(submap_map_t));	\
		}						\
	}							\
	MACRO_RETURN(x);					\
	MACRO_END

	<span class="enscript-comment">/*
	 *	Find the beginning of the region.
	 */</span>

 	vm_map_lock(src_map);

	<span class="enscript-comment">/*
	 * Lookup the original &quot;src_addr&quot; rather than the truncated
	 * &quot;src_start&quot;, in case &quot;src_start&quot; falls in a non-map-aligned
	 * map entry *before* the map entry that contains &quot;src_addr&quot;...
	 */</span>
	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(src_map, src_addr, &amp;tmp_entry))
		RETURN(KERN_INVALID_ADDRESS);
	<span class="enscript-keyword">if</span>(!tmp_entry-&gt;is_sub_map) {
		<span class="enscript-comment">/*
		 * ... but clip to the map-rounded &quot;src_start&quot; rather than
		 * &quot;src_addr&quot; to preserve map-alignment.  We'll adjust the
		 * first copy entry at the end, if needed.
		 */</span>
		vm_map_clip_start(src_map, tmp_entry, src_start);
	}
	<span class="enscript-keyword">if</span> (src_start &lt; tmp_entry-&gt;vme_start) {
		<span class="enscript-comment">/*
		 * Move &quot;src_start&quot; up to the start of the
		 * first map entry to copy.
		 */</span>
		src_start = tmp_entry-&gt;vme_start;
	}
	<span class="enscript-comment">/* set for later submap fix-up */</span>
	copy_addr = src_start;

	<span class="enscript-comment">/*
	 *	Go through entries until we get to the end.
	 */</span>

	<span class="enscript-keyword">while</span> (TRUE) {
		<span class="enscript-type">register</span>
		vm_map_entry_t	src_entry = tmp_entry;	<span class="enscript-comment">/* Top-level entry */</span>
		vm_map_size_t	src_size;		<span class="enscript-comment">/* Size of source
							 * map entry (in both
							 * maps)
							 */</span>

		<span class="enscript-type">register</span>
		vm_object_t		src_object;	<span class="enscript-comment">/* Object to copy */</span>
		vm_object_offset_t	src_offset;

		boolean_t	src_needs_copy;		<span class="enscript-comment">/* Should source map
							 * be made read-only
							 * for copy-on-write?
							 */</span>

		boolean_t	new_entry_needs_copy;	<span class="enscript-comment">/* Will new entry be COW? */</span>

		boolean_t	was_wired;		<span class="enscript-comment">/* Was source wired? */</span>
		vm_map_version_t version;		<span class="enscript-comment">/* Version before locks
							 * dropped to make copy
							 */</span>
		kern_return_t	result;			<span class="enscript-comment">/* Return value from
							 * copy_strategically.
							 */</span>
		<span class="enscript-keyword">while</span>(tmp_entry-&gt;is_sub_map) {
			vm_map_size_t submap_len;
			submap_map_t *ptr;

			ptr = (submap_map_t *)kalloc(<span class="enscript-keyword">sizeof</span>(submap_map_t));
			ptr-&gt;next = parent_maps;
			parent_maps = ptr;
			ptr-&gt;parent_map = src_map;
			ptr-&gt;base_start = src_start;
			ptr-&gt;base_end = src_end;
			submap_len = tmp_entry-&gt;vme_end - src_start;
			<span class="enscript-keyword">if</span>(submap_len &gt; (src_end-src_start))
				submap_len = src_end-src_start;
			ptr-&gt;base_len = submap_len;
	
			src_start -= tmp_entry-&gt;vme_start;
			src_start += VME_OFFSET(tmp_entry);
			src_end = src_start + submap_len;
			src_map = VME_SUBMAP(tmp_entry);
			vm_map_lock(src_map);
			<span class="enscript-comment">/* keep an outstanding reference for all maps in */</span>
			<span class="enscript-comment">/* the parents tree except the base map */</span>
			vm_map_reference(src_map);
			vm_map_unlock(ptr-&gt;parent_map);
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(
				    src_map, src_start, &amp;tmp_entry))
				RETURN(KERN_INVALID_ADDRESS);
			map_share = TRUE;
			<span class="enscript-keyword">if</span>(!tmp_entry-&gt;is_sub_map)
				vm_map_clip_start(src_map, tmp_entry, src_start);
			src_entry = tmp_entry;
		}
		<span class="enscript-comment">/* we are now in the lowest level submap... */</span>

		<span class="enscript-keyword">if</span> ((VME_OBJECT(tmp_entry) != VM_OBJECT_NULL) &amp;&amp; 
		    (VME_OBJECT(tmp_entry)-&gt;phys_contiguous)) {
			<span class="enscript-comment">/* This is not, supported for now.In future */</span>
			<span class="enscript-comment">/* we will need to detect the phys_contig   */</span>
			<span class="enscript-comment">/* condition and then upgrade copy_slowly   */</span>
			<span class="enscript-comment">/* to do physical copy from the device mem  */</span>
			<span class="enscript-comment">/* based object. We can piggy-back off of   */</span>
			<span class="enscript-comment">/* the was wired boolean to set-up the      */</span>
			<span class="enscript-comment">/* proper handling */</span>
			RETURN(KERN_PROTECTION_FAILURE);
		}
		<span class="enscript-comment">/*
		 *	Create a new address map entry to hold the result. 
		 *	Fill in the fields from the appropriate source entries.
		 *	We must unlock the source map to do this if we need
		 *	to allocate a map entry.
		 */</span>
		<span class="enscript-keyword">if</span> (new_entry == VM_MAP_ENTRY_NULL) {
			version.main_timestamp = src_map-&gt;timestamp;
			vm_map_unlock(src_map);

			new_entry = vm_map_copy_entry_create(copy, !copy-&gt;cpy_hdr.entries_pageable);

			vm_map_lock(src_map);
			<span class="enscript-keyword">if</span> ((version.main_timestamp + 1) != src_map-&gt;timestamp) {
				<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(src_map, src_start,
							 &amp;tmp_entry)) {
					RETURN(KERN_INVALID_ADDRESS);
				}
				<span class="enscript-keyword">if</span> (!tmp_entry-&gt;is_sub_map)
					vm_map_clip_start(src_map, tmp_entry, src_start);
				<span class="enscript-keyword">continue</span>; <span class="enscript-comment">/* restart w/ new tmp_entry */</span>
			}
		}

		<span class="enscript-comment">/*
		 *	Verify that the region can be read.
		 */</span>
		<span class="enscript-keyword">if</span> (((src_entry-&gt;protection &amp; VM_PROT_READ) == VM_PROT_NONE &amp;&amp;
		     !use_maxprot) ||
		    (src_entry-&gt;max_protection &amp; VM_PROT_READ) == 0)
			RETURN(KERN_PROTECTION_FAILURE);

		<span class="enscript-comment">/*
		 *	Clip against the endpoints of the entire region.
		 */</span>

		vm_map_clip_end(src_map, src_entry, src_end);

		src_size = src_entry-&gt;vme_end - src_start;
		src_object = VME_OBJECT(src_entry);
		src_offset = VME_OFFSET(src_entry);
		was_wired = (src_entry-&gt;wired_count != 0);

		vm_map_entry_copy(new_entry, src_entry);
		<span class="enscript-keyword">if</span> (new_entry-&gt;is_sub_map) {
			<span class="enscript-comment">/* clr address space specifics */</span>
			new_entry-&gt;use_pmap = FALSE;
		}

		<span class="enscript-comment">/*
		 *	Attempt non-blocking copy-on-write optimizations.
		 */</span>

		<span class="enscript-keyword">if</span> (src_destroy &amp;&amp; 
		    (src_object == VM_OBJECT_NULL || 
		     (src_object-&gt;internal &amp;&amp; !src_object-&gt;true_share
		      &amp;&amp; !map_share))) {
			<span class="enscript-comment">/*
			 * If we are destroying the source, and the object
			 * is internal, we can move the object reference
			 * from the source to the copy.  The copy is
			 * copy-on-write only if the source is.
			 * We make another reference to the object, because
			 * destroying the source entry will deallocate it.
			 */</span>
			vm_object_reference(src_object);

			<span class="enscript-comment">/*
			 * Copy is always unwired.  vm_map_copy_entry
			 * set its wired count to zero.
			 */</span>

			<span class="enscript-keyword">goto</span> <span class="enscript-reference">CopySuccessful</span>;
		}


	<span class="enscript-reference">RestartCopy</span>:
		XPR(XPR_VM_MAP, <span class="enscript-string">&quot;vm_map_copyin_common src_obj 0x%x ent 0x%x obj 0x%x was_wired %d\n&quot;</span>,
		    src_object, new_entry, VME_OBJECT(new_entry),
		    was_wired, 0);
		<span class="enscript-keyword">if</span> ((src_object == VM_OBJECT_NULL ||
		     (!was_wired &amp;&amp; !map_share &amp;&amp; !tmp_entry-&gt;is_shared)) &amp;&amp;
		    vm_object_copy_quickly(
			    &amp;VME_OBJECT(new_entry),
			    src_offset,
			    src_size,
			    &amp;src_needs_copy,
			    &amp;new_entry_needs_copy)) {

			new_entry-&gt;needs_copy = new_entry_needs_copy;

			<span class="enscript-comment">/*
			 *	Handle copy-on-write obligations
			 */</span>

			<span class="enscript-keyword">if</span> (src_needs_copy &amp;&amp; !tmp_entry-&gt;needs_copy) {
			        vm_prot_t prot;

				prot = src_entry-&gt;protection &amp; ~VM_PROT_WRITE;

				<span class="enscript-keyword">if</span> (override_nx(src_map, VME_ALIAS(src_entry))
				    &amp;&amp; prot)
				        prot |= VM_PROT_EXECUTE;

				vm_object_pmap_protect(
					src_object,
					src_offset,
					src_size,
			      		(src_entry-&gt;is_shared ? 
					 PMAP_NULL
					 : src_map-&gt;pmap),
					src_entry-&gt;vme_start,
					prot);

				assert(tmp_entry-&gt;wired_count == 0);
				tmp_entry-&gt;needs_copy = TRUE;
			}

			<span class="enscript-comment">/*
			 *	The map has never been unlocked, so it's safe
			 *	to move to the next entry rather than doing
			 *	another lookup.
			 */</span>

			<span class="enscript-keyword">goto</span> <span class="enscript-reference">CopySuccessful</span>;
		}

		<span class="enscript-comment">/*
		 *	Take an object reference, so that we may
		 *	release the map lock(s).
		 */</span>

		assert(src_object != VM_OBJECT_NULL);
		vm_object_reference(src_object);

		<span class="enscript-comment">/*
		 *	Record the timestamp for later verification.
		 *	Unlock the map.
		 */</span>

		version.main_timestamp = src_map-&gt;timestamp;
		vm_map_unlock(src_map);	<span class="enscript-comment">/* Increments timestamp once! */</span>

		<span class="enscript-comment">/*
		 *	Perform the copy
		 */</span>

		<span class="enscript-keyword">if</span> (was_wired) {
		<span class="enscript-reference">CopySlowly</span>:
			vm_object_lock(src_object);
			result = vm_object_copy_slowly(
				src_object,
				src_offset,
				src_size,
				THREAD_UNINT,
				&amp;VME_OBJECT(new_entry));
			VME_OFFSET_SET(new_entry, 0);
			new_entry-&gt;needs_copy = FALSE;

		}
		<span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (src_object-&gt;copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC &amp;&amp;
			 (tmp_entry-&gt;is_shared  || map_share)) {
		  	vm_object_t new_object;

			vm_object_lock_shared(src_object);
			new_object = vm_object_copy_delayed(
				src_object,
				src_offset,	
				src_size,
				TRUE);
			<span class="enscript-keyword">if</span> (new_object == VM_OBJECT_NULL)
			  	<span class="enscript-keyword">goto</span> <span class="enscript-reference">CopySlowly</span>;

			VME_OBJECT_SET(new_entry, new_object);
			assert(new_entry-&gt;wired_count == 0);
			new_entry-&gt;needs_copy = TRUE;
			assert(!new_entry-&gt;iokit_acct);
			assert(new_object-&gt;purgable == VM_PURGABLE_DENY);
			new_entry-&gt;use_pmap = TRUE;
			result = KERN_SUCCESS;

		} <span class="enscript-keyword">else</span> {
			vm_object_offset_t new_offset;
			new_offset = VME_OFFSET(new_entry);
			result = vm_object_copy_strategically(src_object,
							      src_offset,
							      src_size,
							      &amp;VME_OBJECT(new_entry),
							      &amp;new_offset,
							      &amp;new_entry_needs_copy);
			<span class="enscript-keyword">if</span> (new_offset != VME_OFFSET(new_entry)) {
				VME_OFFSET_SET(new_entry, new_offset);
			}

			new_entry-&gt;needs_copy = new_entry_needs_copy;
		}

		<span class="enscript-keyword">if</span> (result != KERN_SUCCESS &amp;&amp;
		    result != KERN_MEMORY_RESTART_COPY) {
			vm_map_lock(src_map);
			RETURN(result);
		}

		<span class="enscript-comment">/*
		 *	Throw away the extra reference
		 */</span>

		vm_object_deallocate(src_object);

		<span class="enscript-comment">/*
		 *	Verify that the map has not substantially
		 *	changed while the copy was being made.
		 */</span>

		vm_map_lock(src_map);

		<span class="enscript-keyword">if</span> ((version.main_timestamp + 1) == src_map-&gt;timestamp)
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">VerificationSuccessful</span>;

		<span class="enscript-comment">/*
		 *	Simple version comparison failed.
		 *
		 *	Retry the lookup and verify that the
		 *	same object/offset are still present.
		 *
		 *	[Note: a memory manager that colludes with
		 *	the calling task can detect that we have
		 *	cheated.  While the map was unlocked, the
		 *	mapping could have been changed and restored.]
		 */</span>

		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(src_map, src_start, &amp;tmp_entry)) {
			<span class="enscript-keyword">if</span> (result != KERN_MEMORY_RESTART_COPY) {
				vm_object_deallocate(VME_OBJECT(new_entry));
				VME_OBJECT_SET(new_entry, VM_OBJECT_NULL);
				assert(!new_entry-&gt;iokit_acct);
				new_entry-&gt;use_pmap = TRUE;
			}
			RETURN(KERN_INVALID_ADDRESS);
		}

		src_entry = tmp_entry;
		vm_map_clip_start(src_map, src_entry, src_start);

		<span class="enscript-keyword">if</span> ((((src_entry-&gt;protection &amp; VM_PROT_READ) == VM_PROT_NONE) &amp;&amp;
		     !use_maxprot) ||
		    ((src_entry-&gt;max_protection &amp; VM_PROT_READ) == 0))
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">VerificationFailed</span>;

		<span class="enscript-keyword">if</span> (src_entry-&gt;vme_end &lt; new_entry-&gt;vme_end) {
			assert(VM_MAP_PAGE_ALIGNED(src_entry-&gt;vme_end,
						   VM_MAP_COPY_PAGE_MASK(copy)));
			new_entry-&gt;vme_end = src_entry-&gt;vme_end;
			src_size = new_entry-&gt;vme_end - src_start;
		}

		<span class="enscript-keyword">if</span> ((VME_OBJECT(src_entry) != src_object) ||
		    (VME_OFFSET(src_entry) != src_offset) ) {

			<span class="enscript-comment">/*
			 *	Verification failed.
			 *
			 *	Start over with this top-level entry.
			 */</span>

		<span class="enscript-reference">VerificationFailed</span>: ;

			vm_object_deallocate(VME_OBJECT(new_entry));
			tmp_entry = src_entry;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-comment">/*
		 *	Verification succeeded.
		 */</span>

	<span class="enscript-reference">VerificationSuccessful</span>: ;

		<span class="enscript-keyword">if</span> (result == KERN_MEMORY_RESTART_COPY)
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">RestartCopy</span>;

		<span class="enscript-comment">/*
		 *	Copy succeeded.
		 */</span>

	<span class="enscript-reference">CopySuccessful</span>: ;

		<span class="enscript-comment">/*
		 *	Link in the new copy entry.
		 */</span>

		vm_map_copy_entry_link(copy, vm_map_copy_last_entry(copy),
				       new_entry);
		
		<span class="enscript-comment">/*
		 *	Determine whether the entire region
		 *	has been copied.
		 */</span>
		src_base = src_start;
		src_start = new_entry-&gt;vme_end;
		new_entry = VM_MAP_ENTRY_NULL;
		<span class="enscript-keyword">while</span> ((src_start &gt;= src_end) &amp;&amp; (src_end != 0)) {
			submap_map_t	*ptr;

			<span class="enscript-keyword">if</span> (src_map == base_map) {
				<span class="enscript-comment">/* back to the top */</span>
				<span class="enscript-keyword">break</span>;
			}

			ptr = parent_maps;
			assert(ptr != NULL);
			parent_maps = parent_maps-&gt;next;

			<span class="enscript-comment">/* fix up the damage we did in that submap */</span>
			vm_map_simplify_range(src_map,
					      src_base,
					      src_end);

			vm_map_unlock(src_map);
			vm_map_deallocate(src_map);
			vm_map_lock(ptr-&gt;parent_map);
			src_map = ptr-&gt;parent_map;
			src_base = ptr-&gt;base_start;
			src_start = ptr-&gt;base_start + ptr-&gt;base_len;
			src_end = ptr-&gt;base_end;
			<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(src_map,
						 src_start,
						 &amp;tmp_entry) &amp;&amp;
			    (src_end &gt; src_start)) {
				RETURN(KERN_INVALID_ADDRESS);
			}
			kfree(ptr, <span class="enscript-keyword">sizeof</span>(submap_map_t));
			<span class="enscript-keyword">if</span> (parent_maps == NULL)
				map_share = FALSE;
			src_entry = tmp_entry-&gt;vme_prev;
		}

		<span class="enscript-keyword">if</span> ((VM_MAP_PAGE_SHIFT(src_map) != PAGE_SHIFT) &amp;&amp;
		    (src_start &gt;= src_addr + len) &amp;&amp;
		    (src_addr + len != 0)) {
			<span class="enscript-comment">/*
			 * Stop copying now, even though we haven't reached
			 * &quot;src_end&quot;.  We'll adjust the end of the last copy
			 * entry at the end, if needed.
			 *
			 * If src_map's aligment is different from the
			 * system's page-alignment, there could be
			 * extra non-map-aligned map entries between
			 * the original (non-rounded) &quot;src_addr + len&quot;
			 * and the rounded &quot;src_end&quot;.
			 * We do not want to copy those map entries since
			 * they're not part of the copied range.
			 */</span>
			<span class="enscript-keyword">break</span>;
		}

		<span class="enscript-keyword">if</span> ((src_start &gt;= src_end) &amp;&amp; (src_end != 0))
			<span class="enscript-keyword">break</span>;

		<span class="enscript-comment">/*
		 *	Verify that there are no gaps in the region
		 */</span>

		tmp_entry = src_entry-&gt;vme_next;
		<span class="enscript-keyword">if</span> ((tmp_entry-&gt;vme_start != src_start) ||
		    (tmp_entry == vm_map_to_entry(src_map))) {
			RETURN(KERN_INVALID_ADDRESS);
		}
	}

	<span class="enscript-comment">/*
	 * If the source should be destroyed, do it now, since the
	 * copy was successful. 
	 */</span>
	<span class="enscript-keyword">if</span> (src_destroy) {
		(<span class="enscript-type">void</span>) vm_map_delete(
			src_map,
			vm_map_trunc_page(src_addr,
					  VM_MAP_PAGE_MASK(src_map)),
			src_end,
			((src_map == kernel_map) ?
			 VM_MAP_REMOVE_KUNWIRE :
			 VM_MAP_NO_FLAGS),
			VM_MAP_NULL);
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-comment">/* fix up the damage we did in the base map */</span>
		vm_map_simplify_range(
			src_map,
			vm_map_trunc_page(src_addr,
					  VM_MAP_PAGE_MASK(src_map)), 
			vm_map_round_page(src_end,
					  VM_MAP_PAGE_MASK(src_map)));
	}

	vm_map_unlock(src_map);

	<span class="enscript-keyword">if</span> (VM_MAP_PAGE_SHIFT(src_map) != PAGE_SHIFT) {
		vm_map_offset_t original_start, original_offset, original_end;
		
		assert(VM_MAP_COPY_PAGE_MASK(copy) == PAGE_MASK);

		<span class="enscript-comment">/* adjust alignment of first copy_entry's &quot;vme_start&quot; */</span>
		tmp_entry = vm_map_copy_first_entry(copy);
		<span class="enscript-keyword">if</span> (tmp_entry != vm_map_copy_to_entry(copy)) {
			vm_map_offset_t adjustment;

			original_start = tmp_entry-&gt;vme_start;
			original_offset = VME_OFFSET(tmp_entry);

			<span class="enscript-comment">/* map-align the start of the first copy entry... */</span>
			adjustment = (tmp_entry-&gt;vme_start -
				      vm_map_trunc_page(
					      tmp_entry-&gt;vme_start,
					      VM_MAP_PAGE_MASK(src_map)));
			tmp_entry-&gt;vme_start -= adjustment;
			VME_OFFSET_SET(tmp_entry,
				       VME_OFFSET(tmp_entry) - adjustment);
			copy_addr -= adjustment;
			assert(tmp_entry-&gt;vme_start &lt; tmp_entry-&gt;vme_end);
			<span class="enscript-comment">/* ... adjust for mis-aligned start of copy range */</span>
			adjustment =
				(vm_map_trunc_page(copy-&gt;offset,
						   PAGE_MASK) -
				 vm_map_trunc_page(copy-&gt;offset,
						   VM_MAP_PAGE_MASK(src_map)));
			<span class="enscript-keyword">if</span> (adjustment) {
				assert(page_aligned(adjustment));
				assert(adjustment &lt; VM_MAP_PAGE_SIZE(src_map));
				tmp_entry-&gt;vme_start += adjustment;
				VME_OFFSET_SET(tmp_entry,
					       (VME_OFFSET(tmp_entry) +
						adjustment));
				copy_addr += adjustment;
				assert(tmp_entry-&gt;vme_start &lt; tmp_entry-&gt;vme_end);
			}

			<span class="enscript-comment">/*
			 * Assert that the adjustments haven't exposed
			 * more than was originally copied...
			 */</span>
			assert(tmp_entry-&gt;vme_start &gt;= original_start);
			assert(VME_OFFSET(tmp_entry) &gt;= original_offset);
			<span class="enscript-comment">/*
			 * ... and that it did not adjust outside of a
			 * a single 16K page.
			 */</span>
			assert(vm_map_trunc_page(tmp_entry-&gt;vme_start,
						 VM_MAP_PAGE_MASK(src_map)) ==
			       vm_map_trunc_page(original_start,
						 VM_MAP_PAGE_MASK(src_map)));
		}

		<span class="enscript-comment">/* adjust alignment of last copy_entry's &quot;vme_end&quot; */</span>
		tmp_entry = vm_map_copy_last_entry(copy);
		<span class="enscript-keyword">if</span> (tmp_entry != vm_map_copy_to_entry(copy)) {
			vm_map_offset_t adjustment;

			original_end = tmp_entry-&gt;vme_end;

			<span class="enscript-comment">/* map-align the end of the last copy entry... */</span>
			tmp_entry-&gt;vme_end =
				vm_map_round_page(tmp_entry-&gt;vme_end,
						  VM_MAP_PAGE_MASK(src_map));
			<span class="enscript-comment">/* ... adjust for mis-aligned end of copy range */</span>
			adjustment =
				(vm_map_round_page((copy-&gt;offset +
						    copy-&gt;size),
						   VM_MAP_PAGE_MASK(src_map)) -
				 vm_map_round_page((copy-&gt;offset +
						    copy-&gt;size),
						   PAGE_MASK));
			<span class="enscript-keyword">if</span> (adjustment) {
				assert(page_aligned(adjustment));
				assert(adjustment &lt; VM_MAP_PAGE_SIZE(src_map));
				tmp_entry-&gt;vme_end -= adjustment;
				assert(tmp_entry-&gt;vme_start &lt; tmp_entry-&gt;vme_end);
			}

			<span class="enscript-comment">/*
			 * Assert that the adjustments haven't exposed
			 * more than was originally copied...
			 */</span>
			assert(tmp_entry-&gt;vme_end &lt;= original_end);
			<span class="enscript-comment">/*
			 * ... and that it did not adjust outside of a
			 * a single 16K page.
			 */</span>
			assert(vm_map_round_page(tmp_entry-&gt;vme_end,
						 VM_MAP_PAGE_MASK(src_map)) ==
			       vm_map_round_page(original_end,
						 VM_MAP_PAGE_MASK(src_map)));
		}
	}

	<span class="enscript-comment">/* Fix-up start and end points in copy.  This is necessary */</span>
	<span class="enscript-comment">/* when the various entries in the copy object were picked */</span>
	<span class="enscript-comment">/* up from different sub-maps */</span>

	tmp_entry = vm_map_copy_first_entry(copy);
	copy_size = 0; <span class="enscript-comment">/* compute actual size */</span>
	<span class="enscript-keyword">while</span> (tmp_entry != vm_map_copy_to_entry(copy)) {
		assert(VM_MAP_PAGE_ALIGNED(
			       copy_addr + (tmp_entry-&gt;vme_end -
					    tmp_entry-&gt;vme_start),
			       VM_MAP_COPY_PAGE_MASK(copy)));
		assert(VM_MAP_PAGE_ALIGNED(
			       copy_addr,
			       VM_MAP_COPY_PAGE_MASK(copy)));

		<span class="enscript-comment">/*
		 * The copy_entries will be injected directly into the
		 * destination map and might not be &quot;map aligned&quot; there...
		 */</span>
		tmp_entry-&gt;map_aligned = FALSE;

		tmp_entry-&gt;vme_end = copy_addr + 
			(tmp_entry-&gt;vme_end - tmp_entry-&gt;vme_start);
		tmp_entry-&gt;vme_start = copy_addr;
		assert(tmp_entry-&gt;vme_start &lt; tmp_entry-&gt;vme_end);
		copy_addr += tmp_entry-&gt;vme_end - tmp_entry-&gt;vme_start;
		copy_size += tmp_entry-&gt;vme_end - tmp_entry-&gt;vme_start;
		tmp_entry = (<span class="enscript-type">struct</span> vm_map_entry *)tmp_entry-&gt;vme_next;
	}

	<span class="enscript-keyword">if</span> (VM_MAP_PAGE_SHIFT(src_map) != PAGE_SHIFT &amp;&amp;
	    copy_size &lt; copy-&gt;size) {
		<span class="enscript-comment">/*
		 * The actual size of the VM map copy is smaller than what
		 * was requested by the caller.  This must be because some
		 * PAGE_SIZE-sized pages are missing at the end of the last
		 * VM_MAP_PAGE_SIZE(src_map)-sized chunk of the range.
		 * The caller might not have been aware of those missing
		 * pages and might not want to be aware of it, which is
		 * fine as long as they don't try to access (and crash on)
		 * those missing pages.
		 * Let's adjust the size of the &quot;copy&quot;, to avoid failing
		 * in vm_map_copyout() or vm_map_copy_overwrite().
		 */</span>
		assert(vm_map_round_page(copy_size,
					 VM_MAP_PAGE_MASK(src_map)) ==
		       vm_map_round_page(copy-&gt;size,
					 VM_MAP_PAGE_MASK(src_map)));
		copy-&gt;size = copy_size;
	}

	*copy_result = copy;
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);

#<span class="enscript-reference">undef</span>	<span class="enscript-variable-name">RETURN</span>
}

kern_return_t
<span class="enscript-function-name">vm_map_copy_extract</span>(
	vm_map_t		src_map,
	vm_map_address_t	src_addr,
	vm_map_size_t		len,
	vm_map_copy_t		*copy_result,	<span class="enscript-comment">/* OUT */</span>
	vm_prot_t		*cur_prot,	<span class="enscript-comment">/* OUT */</span>
	vm_prot_t		*max_prot)
{
	vm_map_offset_t	src_start, src_end;
	vm_map_copy_t	copy;
	kern_return_t	kr;

	<span class="enscript-comment">/*
	 *	Check for copies of zero bytes.
	 */</span>

	<span class="enscript-keyword">if</span> (len == 0) {
		*copy_result = VM_MAP_COPY_NULL;
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}

	<span class="enscript-comment">/*
	 *	Check that the end address doesn't overflow
	 */</span>
	src_end = src_addr + len;
	<span class="enscript-keyword">if</span> (src_end &lt; src_addr)
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;

	<span class="enscript-comment">/*
	 *	Compute (page aligned) start and end of region
	 */</span>
	src_start = vm_map_trunc_page(src_addr, PAGE_MASK);
	src_end = vm_map_round_page(src_end, PAGE_MASK);

	<span class="enscript-comment">/*
	 *	Allocate a header element for the list.
	 *
	 *	Use the start and end in the header to 
	 *	remember the endpoints prior to rounding.
	 */</span>

	copy = (vm_map_copy_t) zalloc(vm_map_copy_zone);
	copy-&gt;c_u.hdr.rb_head_store.rbh_root = (<span class="enscript-type">void</span>*)(<span class="enscript-type">int</span>)SKIP_RB_TREE;
	vm_map_copy_first_entry(copy) =
		vm_map_copy_last_entry(copy) = vm_map_copy_to_entry(copy);
	copy-&gt;type = VM_MAP_COPY_ENTRY_LIST;
	copy-&gt;cpy_hdr.nentries = 0;
	copy-&gt;cpy_hdr.entries_pageable = TRUE;

	vm_map_store_init(&amp;copy-&gt;cpy_hdr);

	copy-&gt;offset = 0;
	copy-&gt;size = len;

	kr = vm_map_remap_extract(src_map,
				  src_addr,
				  len,
				  FALSE, <span class="enscript-comment">/* copy */</span>
				  &amp;copy-&gt;cpy_hdr,
				  cur_prot,
				  max_prot,
				  VM_INHERIT_SHARE,
				  TRUE); <span class="enscript-comment">/* pageable */</span>
	<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
		vm_map_copy_discard(copy);
		<span class="enscript-keyword">return</span> kr;
	}

	*copy_result = copy;
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 *	vm_map_copyin_object:
 *
 *	Create a copy object from an object.
 *	Our caller donates an object reference.
 */</span>

kern_return_t
<span class="enscript-function-name">vm_map_copyin_object</span>(
	vm_object_t		object,
	vm_object_offset_t	offset,	<span class="enscript-comment">/* offset of region in object */</span>
	vm_object_size_t	size,	<span class="enscript-comment">/* size of region in object */</span>
	vm_map_copy_t	*copy_result)	<span class="enscript-comment">/* OUT */</span>
{
	vm_map_copy_t	copy;		<span class="enscript-comment">/* Resulting copy */</span>

	<span class="enscript-comment">/*
	 *	We drop the object into a special copy object
	 *	that contains the object directly.
	 */</span>

	copy = (vm_map_copy_t) zalloc(vm_map_copy_zone);
	copy-&gt;c_u.hdr.rb_head_store.rbh_root = (<span class="enscript-type">void</span>*)(<span class="enscript-type">int</span>)SKIP_RB_TREE;
	copy-&gt;type = VM_MAP_COPY_OBJECT;
	copy-&gt;cpy_object = object;
	copy-&gt;offset = offset;
	copy-&gt;size = size;

	*copy_result = copy;
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}

<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_fork_share</span>(
	vm_map_t	old_map,
	vm_map_entry_t	old_entry,
	vm_map_t	new_map)
{
	vm_object_t 	object;
	vm_map_entry_t 	new_entry;

	<span class="enscript-comment">/*
	 *	New sharing code.  New map entry
	 *	references original object.  Internal
	 *	objects use asynchronous copy algorithm for
	 *	future copies.  First make sure we have
	 *	the right object.  If we need a shadow,
	 *	or someone else already has one, then
	 *	make a new shadow and share it.
	 */</span>
	
	object = VME_OBJECT(old_entry);
	<span class="enscript-keyword">if</span> (old_entry-&gt;is_sub_map) {
		assert(old_entry-&gt;wired_count == 0);
#<span class="enscript-reference">ifndef</span> <span class="enscript-variable-name">NO_NESTED_PMAP</span>
		<span class="enscript-keyword">if</span>(old_entry-&gt;use_pmap) {
			kern_return_t	result;

			result = pmap_nest(new_map-&gt;pmap, 
					   (VME_SUBMAP(old_entry))-&gt;pmap, 
					   (addr64_t)old_entry-&gt;vme_start,
					   (addr64_t)old_entry-&gt;vme_start,
					   (uint64_t)(old_entry-&gt;vme_end - old_entry-&gt;vme_start));
			<span class="enscript-keyword">if</span>(result)
				panic(<span class="enscript-string">&quot;vm_map_fork_share: pmap_nest failed!&quot;</span>);
		}
#<span class="enscript-reference">endif</span>	<span class="enscript-comment">/* NO_NESTED_PMAP */</span>
	} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
		object = vm_object_allocate((vm_map_size_t)(old_entry-&gt;vme_end -
							    old_entry-&gt;vme_start));
		VME_OFFSET_SET(old_entry, 0);
		VME_OBJECT_SET(old_entry, object);
		old_entry-&gt;use_pmap = TRUE;
		assert(!old_entry-&gt;needs_copy);
	} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (object-&gt;copy_strategy !=
		   MEMORY_OBJECT_COPY_SYMMETRIC) {
		
		<span class="enscript-comment">/*
		 *	We are already using an asymmetric
		 *	copy, and therefore we already have
		 *	the right object.
		 */</span>
		
		assert(! old_entry-&gt;needs_copy);
	}
	<span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (old_entry-&gt;needs_copy ||	<span class="enscript-comment">/* case 1 */</span>
		 object-&gt;shadowed ||		<span class="enscript-comment">/* case 2 */</span>
		 (!object-&gt;true_share &amp;&amp; 	<span class="enscript-comment">/* case 3 */</span>
		  !old_entry-&gt;is_shared &amp;&amp;
		  (object-&gt;vo_size &gt;
		   (vm_map_size_t)(old_entry-&gt;vme_end -
				   old_entry-&gt;vme_start)))) {
		
		<span class="enscript-comment">/*
		 *	We need to create a shadow.
		 *	There are three cases here.
		 *	In the first case, we need to
		 *	complete a deferred symmetrical
		 *	copy that we participated in.
		 *	In the second and third cases,
		 *	we need to create the shadow so
		 *	that changes that we make to the
		 *	object do not interfere with
		 *	any symmetrical copies which
		 *	have occured (case 2) or which
		 *	might occur (case 3).
		 *
		 *	The first case is when we had
		 *	deferred shadow object creation
		 *	via the entry-&gt;needs_copy mechanism.
		 *	This mechanism only works when
		 *	only one entry points to the source
		 *	object, and we are about to create
		 *	a second entry pointing to the
		 *	same object. The problem is that
		 *	there is no way of mapping from
		 *	an object to the entries pointing
		 *	to it. (Deferred shadow creation
		 *	works with one entry because occurs
		 *	at fault time, and we walk from the
		 *	entry to the object when handling
		 *	the fault.)
		 *
		 *	The second case is when the object
		 *	to be shared has already been copied
		 *	with a symmetric copy, but we point
		 *	directly to the object without
		 *	needs_copy set in our entry. (This
		 *	can happen because different ranges
		 *	of an object can be pointed to by
		 *	different entries. In particular,
		 *	a single entry pointing to an object
		 *	can be split by a call to vm_inherit,
		 *	which, combined with task_create, can
		 *	result in the different entries
		 *	having different needs_copy values.)
		 *	The shadowed flag in the object allows
		 *	us to detect this case. The problem
		 *	with this case is that if this object
		 *	has or will have shadows, then we
		 *	must not perform an asymmetric copy
		 *	of this object, since such a copy
		 *	allows the object to be changed, which
		 *	will break the previous symmetrical
		 *	copies (which rely upon the object
		 *	not changing). In a sense, the shadowed
		 *	flag says &quot;don't change this object&quot;.
		 *	We fix this by creating a shadow
		 *	object for this object, and sharing
		 *	that. This works because we are free
		 *	to change the shadow object (and thus
		 *	to use an asymmetric copy strategy);
		 *	this is also semantically correct,
		 *	since this object is temporary, and
		 *	therefore a copy of the object is
		 *	as good as the object itself. (This
		 *	is not true for permanent objects,
		 *	since the pager needs to see changes,
		 *	which won't happen if the changes
		 *	are made to a copy.)
		 *
		 *	The third case is when the object
		 *	to be shared has parts sticking
		 *	outside of the entry we're working
		 *	with, and thus may in the future
		 *	be subject to a symmetrical copy.
		 *	(This is a preemptive version of
		 *	case 2.)
		 */</span>
		VME_OBJECT_SHADOW(old_entry,
				  (vm_map_size_t) (old_entry-&gt;vme_end -
						   old_entry-&gt;vme_start));
		
		<span class="enscript-comment">/*
		 *	If we're making a shadow for other than
		 *	copy on write reasons, then we have
		 *	to remove write permission.
		 */</span>

		<span class="enscript-keyword">if</span> (!old_entry-&gt;needs_copy &amp;&amp;
		    (old_entry-&gt;protection &amp; VM_PROT_WRITE)) {
		        vm_prot_t prot;

			prot = old_entry-&gt;protection &amp; ~VM_PROT_WRITE;

			<span class="enscript-keyword">if</span> (override_nx(old_map, VME_ALIAS(old_entry)) &amp;&amp; prot)
			        prot |= VM_PROT_EXECUTE;

			<span class="enscript-keyword">if</span> (old_map-&gt;mapped_in_other_pmaps) {
				vm_object_pmap_protect(
					VME_OBJECT(old_entry),
					VME_OFFSET(old_entry),
					(old_entry-&gt;vme_end -
					 old_entry-&gt;vme_start),
					PMAP_NULL,
					old_entry-&gt;vme_start,
					prot);
			} <span class="enscript-keyword">else</span> {
				pmap_protect(old_map-&gt;pmap,
					     old_entry-&gt;vme_start,
					     old_entry-&gt;vme_end,
					     prot);
			}
		}
		
		old_entry-&gt;needs_copy = FALSE;
		object = VME_OBJECT(old_entry);
	}

	
	<span class="enscript-comment">/*
	 *	If object was using a symmetric copy strategy,
	 *	change its copy strategy to the default
	 *	asymmetric copy strategy, which is copy_delay
	 *	in the non-norma case and copy_call in the
	 *	norma case. Bump the reference count for the
	 *	new entry.
	 */</span>
	
	<span class="enscript-keyword">if</span>(old_entry-&gt;is_sub_map) {
		vm_map_lock(VME_SUBMAP(old_entry));
		vm_map_reference(VME_SUBMAP(old_entry));
		vm_map_unlock(VME_SUBMAP(old_entry));
	} <span class="enscript-keyword">else</span> {
		vm_object_lock(object);
		vm_object_reference_locked(object);
		<span class="enscript-keyword">if</span> (object-&gt;copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC) {
			object-&gt;copy_strategy = MEMORY_OBJECT_COPY_DELAY;
		}
		vm_object_unlock(object);
	}
	
	<span class="enscript-comment">/*
	 *	Clone the entry, using object ref from above.
	 *	Mark both entries as shared.
	 */</span>
	
	new_entry = vm_map_entry_create(new_map, FALSE); <span class="enscript-comment">/* Never the kernel
							  * map or descendants */</span>
	vm_map_entry_copy(new_entry, old_entry);
	old_entry-&gt;is_shared = TRUE;
	new_entry-&gt;is_shared = TRUE;
	
	<span class="enscript-comment">/*
	 *	Insert the entry into the new map -- we
	 *	know we're inserting at the end of the new
	 *	map.
	 */</span>
	
	vm_map_store_entry_link(new_map, vm_map_last_entry(new_map), new_entry);
	
	<span class="enscript-comment">/*
	 *	Update the physical map
	 */</span>
	
	<span class="enscript-keyword">if</span> (old_entry-&gt;is_sub_map) {
		<span class="enscript-comment">/* Bill Angell pmap support goes here */</span>
	} <span class="enscript-keyword">else</span> {
		pmap_copy(new_map-&gt;pmap, old_map-&gt;pmap, new_entry-&gt;vme_start,
			  old_entry-&gt;vme_end - old_entry-&gt;vme_start,
			  old_entry-&gt;vme_start);
	}
}

<span class="enscript-type">static</span> boolean_t
<span class="enscript-function-name">vm_map_fork_copy</span>(
	vm_map_t	old_map,
	vm_map_entry_t	*old_entry_p,
	vm_map_t	new_map)
{
	vm_map_entry_t old_entry = *old_entry_p;
	vm_map_size_t entry_size = old_entry-&gt;vme_end - old_entry-&gt;vme_start;
	vm_map_offset_t start = old_entry-&gt;vme_start;
	vm_map_copy_t copy;
	vm_map_entry_t last = vm_map_last_entry(new_map);

	vm_map_unlock(old_map);
	<span class="enscript-comment">/*
	 *	Use maxprot version of copyin because we
	 *	care about whether this memory can ever
	 *	be accessed, not just whether it's accessible
	 *	right now.
	 */</span>
	<span class="enscript-keyword">if</span> (vm_map_copyin_maxprot(old_map, start, entry_size, FALSE, &amp;copy)
	    != KERN_SUCCESS) {
		<span class="enscript-comment">/*
		 *	The map might have changed while it
		 *	was unlocked, check it again.  Skip
		 *	any blank space or permanently
		 *	unreadable region.
		 */</span>
		vm_map_lock(old_map);
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(old_map, start, &amp;last) ||
		    (last-&gt;max_protection &amp; VM_PROT_READ) == VM_PROT_NONE) {
			last = last-&gt;vme_next;
		}
		*old_entry_p = last;

		<span class="enscript-comment">/*
		 * XXX	For some error returns, want to
		 * XXX	skip to the next element.  Note
		 *	that INVALID_ADDRESS and
		 *	PROTECTION_FAILURE are handled above.
		 */</span>
		
		<span class="enscript-keyword">return</span> FALSE;
	}
	
	<span class="enscript-comment">/*
	 *	Insert the copy into the new map
	 */</span>
	
	vm_map_copy_insert(new_map, last, copy);
	
	<span class="enscript-comment">/*
	 *	Pick up the traversal at the end of
	 *	the copied region.
	 */</span>
	
	vm_map_lock(old_map);
	start += entry_size;
	<span class="enscript-keyword">if</span> (! vm_map_lookup_entry(old_map, start, &amp;last)) {
		last = last-&gt;vme_next;
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-keyword">if</span> (last-&gt;vme_start == start) {
			<span class="enscript-comment">/*
			 * No need to clip here and we don't
			 * want to cause any unnecessary
			 * unnesting...
			 */</span>
		} <span class="enscript-keyword">else</span> {
			vm_map_clip_start(old_map, last, start);
		}
	}
	*old_entry_p = last;

	<span class="enscript-keyword">return</span> TRUE;
}

<span class="enscript-comment">/*
 *	vm_map_fork:
 *
 *	Create and return a new map based on the old
 *	map, according to the inheritance values on the
 *	regions in that map.
 *
 *	The source map must not be locked.
 */</span>
vm_map_t
<span class="enscript-function-name">vm_map_fork</span>(
	ledger_t	ledger,
	vm_map_t	old_map)
{
	pmap_t		new_pmap;
	vm_map_t	new_map;
	vm_map_entry_t	old_entry;
	vm_map_size_t	new_size = 0, entry_size;
	vm_map_entry_t	new_entry;
	boolean_t	src_needs_copy;
	boolean_t	new_entry_needs_copy;
	boolean_t	pmap_is64bit;

	pmap_is64bit =
#<span class="enscript-reference">if</span> <span class="enscript-reference">defined</span>(<span class="enscript-variable-name">__i386__</span>) || <span class="enscript-reference">defined</span>(<span class="enscript-variable-name">__x86_64__</span>)
			       old_map-&gt;pmap-&gt;pm_task_map != TASK_MAP_32BIT;
#<span class="enscript-reference">else</span>
#<span class="enscript-reference">error</span> <span class="enscript-variable-name">Unknown</span> <span class="enscript-variable-name">architecture</span>.
#<span class="enscript-reference">endif</span>

	new_pmap = pmap_create(ledger, (vm_map_size_t) 0, pmap_is64bit);

	vm_map_reference_swap(old_map);
	vm_map_lock(old_map);

	new_map = vm_map_create(new_pmap,
				old_map-&gt;min_offset,
				old_map-&gt;max_offset,
				old_map-&gt;hdr.entries_pageable);
	<span class="enscript-comment">/* inherit the parent map's page size */</span>
	vm_map_set_page_shift(new_map, VM_MAP_PAGE_SHIFT(old_map));
	<span class="enscript-keyword">for</span> (
		old_entry = vm_map_first_entry(old_map);
		old_entry != vm_map_to_entry(old_map);
		) {

		entry_size = old_entry-&gt;vme_end - old_entry-&gt;vme_start;

		<span class="enscript-keyword">switch</span> (old_entry-&gt;inheritance) {
		<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_INHERIT_NONE</span>:
			<span class="enscript-keyword">break</span>;

		<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_INHERIT_SHARE</span>:
			vm_map_fork_share(old_map, old_entry, new_map);
			new_size += entry_size;
			<span class="enscript-keyword">break</span>;

		<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_INHERIT_COPY</span>:

			<span class="enscript-comment">/*
			 *	Inline the copy_quickly case;
			 *	upon failure, fall back on call
			 *	to vm_map_fork_copy.
			 */</span>

			<span class="enscript-keyword">if</span>(old_entry-&gt;is_sub_map)
				<span class="enscript-keyword">break</span>;
			<span class="enscript-keyword">if</span> ((old_entry-&gt;wired_count != 0) ||
			    ((VME_OBJECT(old_entry) != NULL) &amp;&amp;
			     (VME_OBJECT(old_entry)-&gt;true_share))) {
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">slow_vm_map_fork_copy</span>;
			}

			new_entry = vm_map_entry_create(new_map, FALSE); <span class="enscript-comment">/* never the kernel map or descendants */</span>
			vm_map_entry_copy(new_entry, old_entry);
			<span class="enscript-keyword">if</span> (new_entry-&gt;is_sub_map) {
				<span class="enscript-comment">/* clear address space specifics */</span>
				new_entry-&gt;use_pmap = FALSE;
			}

			<span class="enscript-keyword">if</span> (! vm_object_copy_quickly(
				    &amp;VME_OBJECT(new_entry),
				    VME_OFFSET(old_entry),
				    (old_entry-&gt;vme_end -
				     old_entry-&gt;vme_start),
				    &amp;src_needs_copy,
				    &amp;new_entry_needs_copy)) {
				vm_map_entry_dispose(new_map, new_entry);
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">slow_vm_map_fork_copy</span>;
			}

			<span class="enscript-comment">/*
			 *	Handle copy-on-write obligations
			 */</span>
			
			<span class="enscript-keyword">if</span> (src_needs_copy &amp;&amp; !old_entry-&gt;needs_copy) {
			        vm_prot_t prot;

				prot = old_entry-&gt;protection &amp; ~VM_PROT_WRITE;

				<span class="enscript-keyword">if</span> (override_nx(old_map, VME_ALIAS(old_entry))
				    &amp;&amp; prot)
				        prot |= VM_PROT_EXECUTE;

				vm_object_pmap_protect(
					VME_OBJECT(old_entry),
					VME_OFFSET(old_entry),
					(old_entry-&gt;vme_end -
					 old_entry-&gt;vme_start),
					((old_entry-&gt;is_shared 
					  || old_map-&gt;mapped_in_other_pmaps)
					 ? PMAP_NULL :
					 old_map-&gt;pmap),
					old_entry-&gt;vme_start,
					prot);

				assert(old_entry-&gt;wired_count == 0);
				old_entry-&gt;needs_copy = TRUE;
			}
			new_entry-&gt;needs_copy = new_entry_needs_copy;
			
			<span class="enscript-comment">/*
			 *	Insert the entry at the end
			 *	of the map.
			 */</span>
			
			vm_map_store_entry_link(new_map, vm_map_last_entry(new_map),
					  new_entry);
			new_size += entry_size;
			<span class="enscript-keyword">break</span>;

		<span class="enscript-reference">slow_vm_map_fork_copy</span>:
			<span class="enscript-keyword">if</span> (vm_map_fork_copy(old_map, &amp;old_entry, new_map)) {
				new_size += entry_size;
			}
			<span class="enscript-keyword">continue</span>;
		}
		old_entry = old_entry-&gt;vme_next;
	}


	new_map-&gt;size = new_size;
	vm_map_unlock(old_map);
	vm_map_deallocate(old_map);

	<span class="enscript-keyword">return</span>(new_map);
}

<span class="enscript-comment">/*
 * vm_map_exec:
 *
 * 	Setup the &quot;new_map&quot; with the proper execution environment according
 *	to the type of executable (platform, 64bit, chroot environment).
 *	Map the comm page and shared region, etc...
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_exec</span>(
	vm_map_t	new_map,
	task_t		task,
	<span class="enscript-type">void</span>		*fsroot,
	cpu_type_t	cpu)
{
	SHARED_REGION_TRACE_DEBUG(
		(<span class="enscript-string">&quot;shared_region: task %p: vm_map_exec(%p,%p,%p,0x%x): -&gt;\n&quot;</span>,
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(current_task()),
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(new_map),
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(task),
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(fsroot),
		 cpu));
	(<span class="enscript-type">void</span>) vm_commpage_enter(new_map, task);
	(<span class="enscript-type">void</span>) vm_shared_region_enter(new_map, task, fsroot, cpu);
	SHARED_REGION_TRACE_DEBUG(
		(<span class="enscript-string">&quot;shared_region: task %p: vm_map_exec(%p,%p,%p,0x%x): &lt;-\n&quot;</span>,
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(current_task()),
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(new_map),
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(task),
		 (<span class="enscript-type">void</span> *)VM_KERNEL_ADDRPERM(fsroot),
		 cpu));
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 *	vm_map_lookup_locked:
 *
 *	Finds the VM object, offset, and
 *	protection for a given virtual address in the
 *	specified map, assuming a page fault of the
 *	type specified.
 *
 *	Returns the (object, offset, protection) for
 *	this address, whether it is wired down, and whether
 *	this map has the only reference to the data in question.
 *	In order to later verify this lookup, a &quot;version&quot;
 *	is returned.
 *
 *	The map MUST be locked by the caller and WILL be
 *	locked on exit.  In order to guarantee the
 *	existence of the returned object, it is returned
 *	locked.
 *
 *	If a lookup is requested with &quot;write protection&quot;
 *	specified, the map may be changed to perform virtual
 *	copying operations, although the data referenced will
 *	remain the same.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_lookup_locked</span>(
	vm_map_t		*var_map,	<span class="enscript-comment">/* IN/OUT */</span>
	vm_map_offset_t		vaddr,
	vm_prot_t		fault_type,
	<span class="enscript-type">int</span>			object_lock_type,
	vm_map_version_t	*out_version,	<span class="enscript-comment">/* OUT */</span>
	vm_object_t		*object,	<span class="enscript-comment">/* OUT */</span>
	vm_object_offset_t	*offset,	<span class="enscript-comment">/* OUT */</span>
	vm_prot_t		*out_prot,	<span class="enscript-comment">/* OUT */</span>
	boolean_t		*wired,		<span class="enscript-comment">/* OUT */</span>
	vm_object_fault_info_t	fault_info,	<span class="enscript-comment">/* OUT */</span>
	vm_map_t		*real_map)
{
	vm_map_entry_t			entry;
	<span class="enscript-type">register</span> vm_map_t		map = *var_map;
	vm_map_t			old_map = *var_map;
	vm_map_t			cow_sub_map_parent = VM_MAP_NULL;
	vm_map_offset_t			cow_parent_vaddr = 0;
	vm_map_offset_t			old_start = 0;
	vm_map_offset_t			old_end = 0;
	<span class="enscript-type">register</span> vm_prot_t		prot;
	boolean_t			mask_protections;
	boolean_t			force_copy;
	vm_prot_t			original_fault_type;

	<span class="enscript-comment">/*
	 * VM_PROT_MASK means that the caller wants us to use &quot;fault_type&quot;
	 * as a mask against the mapping's actual protections, not as an
	 * absolute value.
	 */</span>
	mask_protections = (fault_type &amp; VM_PROT_IS_MASK) ? TRUE : FALSE;
	force_copy = (fault_type &amp; VM_PROT_COPY) ? TRUE : FALSE;
	fault_type &amp;= VM_PROT_ALL;
	original_fault_type = fault_type;

	*real_map = map;

<span class="enscript-reference">RetryLookup</span>:
	fault_type = original_fault_type;

	<span class="enscript-comment">/*
	 *	If the map has an interesting hint, try it before calling
	 *	full blown lookup routine.
	 */</span>
	entry = map-&gt;hint;

	<span class="enscript-keyword">if</span> ((entry == vm_map_to_entry(map)) ||
	    (vaddr &lt; entry-&gt;vme_start) || (vaddr &gt;= entry-&gt;vme_end)) {
		vm_map_entry_t	tmp_entry;

		<span class="enscript-comment">/*
		 *	Entry was either not a valid hint, or the vaddr
		 *	was not contained in the entry, so do a full lookup.
		 */</span>
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, vaddr, &amp;tmp_entry)) {
			<span class="enscript-keyword">if</span>((cow_sub_map_parent) &amp;&amp; (cow_sub_map_parent != map))
				vm_map_unlock(cow_sub_map_parent);
			<span class="enscript-keyword">if</span>((*real_map != map) 
			   &amp;&amp; (*real_map != cow_sub_map_parent))
				vm_map_unlock(*real_map);
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}

		entry = tmp_entry;
	}
	<span class="enscript-keyword">if</span>(map == old_map) {
		old_start = entry-&gt;vme_start;
		old_end = entry-&gt;vme_end;
	}

	<span class="enscript-comment">/*
	 *	Handle submaps.  Drop lock on upper map, submap is
	 *	returned locked.
	 */</span>

<span class="enscript-reference">submap_recurse</span>:
	<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
		vm_map_offset_t		local_vaddr;
		vm_map_offset_t		end_delta;
		vm_map_offset_t		start_delta; 
		vm_map_entry_t		submap_entry;
		boolean_t		mapped_needs_copy=FALSE;

		local_vaddr = vaddr;

		<span class="enscript-keyword">if</span> ((entry-&gt;use_pmap &amp;&amp; !(fault_type &amp; VM_PROT_WRITE))) {
			<span class="enscript-comment">/* if real_map equals map we unlock below */</span>
			<span class="enscript-keyword">if</span> ((*real_map != map) &amp;&amp; 
			    (*real_map != cow_sub_map_parent))
				vm_map_unlock(*real_map);
			*real_map = VME_SUBMAP(entry);
		}

		<span class="enscript-keyword">if</span>(entry-&gt;needs_copy &amp;&amp; (fault_type &amp; VM_PROT_WRITE)) {
			<span class="enscript-keyword">if</span> (!mapped_needs_copy) {
				<span class="enscript-keyword">if</span> (vm_map_lock_read_to_write(map)) {
					vm_map_lock_read(map);
					*real_map = map;
					<span class="enscript-keyword">goto</span> <span class="enscript-reference">RetryLookup</span>;
				}
				vm_map_lock_read(VME_SUBMAP(entry));
				*var_map = VME_SUBMAP(entry);
				cow_sub_map_parent = map;
				<span class="enscript-comment">/* reset base to map before cow object */</span>
				<span class="enscript-comment">/* this is the map which will accept   */</span>
				<span class="enscript-comment">/* the new cow object */</span>
				old_start = entry-&gt;vme_start;
				old_end = entry-&gt;vme_end;
				cow_parent_vaddr = vaddr;
				mapped_needs_copy = TRUE;
			} <span class="enscript-keyword">else</span> {
				vm_map_lock_read(VME_SUBMAP(entry));
				*var_map = VME_SUBMAP(entry);
				<span class="enscript-keyword">if</span>((cow_sub_map_parent != map) &amp;&amp;
				   (*real_map != map))
					vm_map_unlock(map);
			}
		} <span class="enscript-keyword">else</span> {
			vm_map_lock_read(VME_SUBMAP(entry));
			*var_map = VME_SUBMAP(entry);	
			<span class="enscript-comment">/* leave map locked if it is a target */</span>
			<span class="enscript-comment">/* cow sub_map above otherwise, just  */</span>
			<span class="enscript-comment">/* follow the maps down to the object */</span>
			<span class="enscript-comment">/* here we unlock knowing we are not  */</span>
			<span class="enscript-comment">/* revisiting the map.  */</span>
			<span class="enscript-keyword">if</span>((*real_map != map) &amp;&amp; (map != cow_sub_map_parent))
				vm_map_unlock_read(map);
		}

		map = *var_map;

		<span class="enscript-comment">/* calculate the offset in the submap for vaddr */</span>
		local_vaddr = (local_vaddr - entry-&gt;vme_start) + VME_OFFSET(entry);

	<span class="enscript-reference">RetrySubMap</span>:
		<span class="enscript-keyword">if</span>(!vm_map_lookup_entry(map, local_vaddr, &amp;submap_entry)) {
			<span class="enscript-keyword">if</span>((cow_sub_map_parent) &amp;&amp; (cow_sub_map_parent != map)){
				vm_map_unlock(cow_sub_map_parent);
			}
			<span class="enscript-keyword">if</span>((*real_map != map) 
			   &amp;&amp; (*real_map != cow_sub_map_parent)) {
				vm_map_unlock(*real_map);
			}
			*real_map = map;
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}

		<span class="enscript-comment">/* find the attenuated shadow of the underlying object */</span>
		<span class="enscript-comment">/* on our target map */</span>

		<span class="enscript-comment">/* in english the submap object may extend beyond the     */</span>
		<span class="enscript-comment">/* region mapped by the entry or, may only fill a portion */</span>
		<span class="enscript-comment">/* of it.  For our purposes, we only care if the object   */</span>
		<span class="enscript-comment">/* doesn't fill.  In this case the area which will        */</span>
		<span class="enscript-comment">/* ultimately be clipped in the top map will only need    */</span>
		<span class="enscript-comment">/* to be as big as the portion of the underlying entry    */</span>
		<span class="enscript-comment">/* which is mapped */</span>
		start_delta = submap_entry-&gt;vme_start &gt; VME_OFFSET(entry) ?
			submap_entry-&gt;vme_start - VME_OFFSET(entry) : 0;

		end_delta = 
			(VME_OFFSET(entry) + start_delta + (old_end - old_start)) &lt;=
			submap_entry-&gt;vme_end ?
			0 : (VME_OFFSET(entry) + 
			     (old_end - old_start))
			- submap_entry-&gt;vme_end; 

		old_start += start_delta;
		old_end -= end_delta;

		<span class="enscript-keyword">if</span>(submap_entry-&gt;is_sub_map) {
			entry = submap_entry;
			vaddr = local_vaddr;
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">submap_recurse</span>;
		}

		<span class="enscript-keyword">if</span>(((fault_type &amp; VM_PROT_WRITE) &amp;&amp; cow_sub_map_parent)) {

			vm_object_t	sub_object, copy_object;
			vm_object_offset_t copy_offset;
			vm_map_offset_t	local_start;
			vm_map_offset_t	local_end;
			boolean_t		copied_slowly = FALSE;

			<span class="enscript-keyword">if</span> (vm_map_lock_read_to_write(map)) {
				vm_map_lock_read(map);
				old_start -= start_delta;
				old_end += end_delta;
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">RetrySubMap</span>;
			}


			sub_object = VME_OBJECT(submap_entry);
			<span class="enscript-keyword">if</span> (sub_object == VM_OBJECT_NULL) {
				sub_object =
					vm_object_allocate(
						(vm_map_size_t)
						(submap_entry-&gt;vme_end -
						 submap_entry-&gt;vme_start));
				VME_OBJECT_SET(submap_entry, sub_object);
				VME_OFFSET_SET(submap_entry, 0);
			}
			local_start =  local_vaddr - 
				(cow_parent_vaddr - old_start);
			local_end = local_vaddr + 
				(old_end - cow_parent_vaddr);
			vm_map_clip_start(map, submap_entry, local_start);
			vm_map_clip_end(map, submap_entry, local_end);
			<span class="enscript-keyword">if</span> (submap_entry-&gt;is_sub_map) {
				<span class="enscript-comment">/* unnesting was done when clipping */</span>
				assert(!submap_entry-&gt;use_pmap);
			}

			<span class="enscript-comment">/* This is the COW case, lets connect */</span>
			<span class="enscript-comment">/* an entry in our space to the underlying */</span>
			<span class="enscript-comment">/* object in the submap, bypassing the  */</span>
			<span class="enscript-comment">/* submap. */</span>


			<span class="enscript-keyword">if</span>(submap_entry-&gt;wired_count != 0 ||
			   (sub_object-&gt;copy_strategy ==
			    MEMORY_OBJECT_COPY_NONE)) {
				vm_object_lock(sub_object);
				vm_object_copy_slowly(sub_object,
						      VME_OFFSET(submap_entry),
						      (submap_entry-&gt;vme_end -
						       submap_entry-&gt;vme_start),
						      FALSE,
						      &amp;copy_object);
				copied_slowly = TRUE;
			} <span class="enscript-keyword">else</span> {
				
				<span class="enscript-comment">/* set up shadow object */</span>
				copy_object = sub_object;
				vm_object_reference(copy_object);
				sub_object-&gt;shadowed = TRUE;
				assert(submap_entry-&gt;wired_count == 0);
				submap_entry-&gt;needs_copy = TRUE;

				prot = submap_entry-&gt;protection &amp; ~VM_PROT_WRITE;

				<span class="enscript-keyword">if</span> (override_nx(old_map,
						VME_ALIAS(submap_entry))
				    &amp;&amp; prot)
				        prot |= VM_PROT_EXECUTE;

				vm_object_pmap_protect(
					sub_object,
					VME_OFFSET(submap_entry),
					submap_entry-&gt;vme_end - 
					submap_entry-&gt;vme_start,
					(submap_entry-&gt;is_shared 
					 || map-&gt;mapped_in_other_pmaps) ?
					PMAP_NULL : map-&gt;pmap,
					submap_entry-&gt;vme_start,
					prot);
			}
			
			<span class="enscript-comment">/*
			 * Adjust the fault offset to the submap entry.
			 */</span>
			copy_offset = (local_vaddr -
				       submap_entry-&gt;vme_start +
				       VME_OFFSET(submap_entry));

			<span class="enscript-comment">/* This works diffently than the   */</span>
			<span class="enscript-comment">/* normal submap case. We go back  */</span>
			<span class="enscript-comment">/* to the parent of the cow map and*/</span>
			<span class="enscript-comment">/* clip out the target portion of  */</span>
			<span class="enscript-comment">/* the sub_map, substituting the   */</span>
			<span class="enscript-comment">/* new copy object,                */</span>

			vm_map_unlock(map);
			local_start = old_start;
			local_end = old_end;
			map = cow_sub_map_parent;
			*var_map = cow_sub_map_parent;
			vaddr = cow_parent_vaddr;
			cow_sub_map_parent = NULL;

			<span class="enscript-keyword">if</span>(!vm_map_lookup_entry(map, 
						vaddr, &amp;entry)) {
				vm_object_deallocate(
					copy_object);
				vm_map_lock_write_to_read(map);
				<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
			}
					
			<span class="enscript-comment">/* clip out the portion of space */</span>
			<span class="enscript-comment">/* mapped by the sub map which   */</span>
			<span class="enscript-comment">/* corresponds to the underlying */</span>
			<span class="enscript-comment">/* object */</span>

			<span class="enscript-comment">/*
			 * Clip (and unnest) the smallest nested chunk
			 * possible around the faulting address...
			 */</span>
			local_start = vaddr &amp; ~(pmap_nesting_size_min - 1);
			local_end = local_start + pmap_nesting_size_min;
			<span class="enscript-comment">/*
			 * ... but don't go beyond the &quot;old_start&quot; to &quot;old_end&quot;
			 * range, to avoid spanning over another VM region
			 * with a possibly different VM object and/or offset.
			 */</span>
			<span class="enscript-keyword">if</span> (local_start &lt; old_start) {
				local_start = old_start;
			}
			<span class="enscript-keyword">if</span> (local_end &gt; old_end) {
				local_end = old_end;
			}
			<span class="enscript-comment">/*
			 * Adjust copy_offset to the start of the range.
			 */</span>
			copy_offset -= (vaddr - local_start);

			vm_map_clip_start(map, entry, local_start);
			vm_map_clip_end(map, entry, local_end);
			<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
				<span class="enscript-comment">/* unnesting was done when clipping */</span>
				assert(!entry-&gt;use_pmap);
			}

			<span class="enscript-comment">/* substitute copy object for */</span>
			<span class="enscript-comment">/* shared map entry           */</span>
			vm_map_deallocate(VME_SUBMAP(entry));
			assert(!entry-&gt;iokit_acct);
			entry-&gt;is_sub_map = FALSE;
			entry-&gt;use_pmap = TRUE;
			VME_OBJECT_SET(entry, copy_object);

			<span class="enscript-comment">/* propagate the submap entry's protections */</span>
			entry-&gt;protection |= submap_entry-&gt;protection;
			entry-&gt;max_protection |= submap_entry-&gt;max_protection;

			<span class="enscript-keyword">if</span>(copied_slowly) {
				VME_OFFSET_SET(entry, local_start - old_start);
				entry-&gt;needs_copy = FALSE;
				entry-&gt;is_shared = FALSE;
			} <span class="enscript-keyword">else</span> {
				VME_OFFSET_SET(entry, copy_offset);
				assert(entry-&gt;wired_count == 0);
				entry-&gt;needs_copy = TRUE;
				<span class="enscript-keyword">if</span>(entry-&gt;inheritance == VM_INHERIT_SHARE) 
					entry-&gt;inheritance = VM_INHERIT_COPY;
				<span class="enscript-keyword">if</span> (map != old_map)
					entry-&gt;is_shared = TRUE;
			}
			<span class="enscript-keyword">if</span>(entry-&gt;inheritance == VM_INHERIT_SHARE) 
				entry-&gt;inheritance = VM_INHERIT_COPY;

			vm_map_lock_write_to_read(map);
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span>((cow_sub_map_parent)
			   &amp;&amp; (cow_sub_map_parent != *real_map)
			   &amp;&amp; (cow_sub_map_parent != map)) {
				vm_map_unlock(cow_sub_map_parent);
			}
			entry = submap_entry;
			vaddr = local_vaddr;
		}
	}
		
	<span class="enscript-comment">/*
	 *	Check whether this task is allowed to have
	 *	this page.
	 */</span>

	prot = entry-&gt;protection;

	<span class="enscript-keyword">if</span> (override_nx(old_map, VME_ALIAS(entry)) &amp;&amp; prot) {
	        <span class="enscript-comment">/*
		 * HACK -- if not a stack, then allow execution
		 */</span>
	        prot |= VM_PROT_EXECUTE;
	}

	<span class="enscript-keyword">if</span> (mask_protections) {
		fault_type &amp;= prot;
		<span class="enscript-keyword">if</span> (fault_type == VM_PROT_NONE) {
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">protection_failure</span>;
		}
	}
	<span class="enscript-keyword">if</span> ((fault_type &amp; (prot)) != fault_type) {
	<span class="enscript-reference">protection_failure</span>:
		<span class="enscript-keyword">if</span> (*real_map != map) {
			vm_map_unlock(*real_map);
		}
		*real_map = map;

		<span class="enscript-keyword">if</span> ((fault_type &amp; VM_PROT_EXECUTE) &amp;&amp; prot)
		        log_stack_execution_failure((addr64_t)vaddr, prot);

		DTRACE_VM2(prot_fault, <span class="enscript-type">int</span>, 1, (uint64_t *), NULL);
		<span class="enscript-keyword">return</span> KERN_PROTECTION_FAILURE;
	}

	<span class="enscript-comment">/*
	 *	If this page is not pageable, we have to get
	 *	it for all possible accesses.
	 */</span>

	*wired = (entry-&gt;wired_count != 0);
	<span class="enscript-keyword">if</span> (*wired)
	        fault_type = prot;

	<span class="enscript-comment">/*
	 *	If the entry was copy-on-write, we either ...
	 */</span>

	<span class="enscript-keyword">if</span> (entry-&gt;needs_copy) {
	    	<span class="enscript-comment">/*
		 *	If we want to write the page, we may as well
		 *	handle that now since we've got the map locked.
		 *
		 *	If we don't need to write the page, we just
		 *	demote the permissions allowed.
		 */</span>

		<span class="enscript-keyword">if</span> ((fault_type &amp; VM_PROT_WRITE) || *wired || force_copy) {
			<span class="enscript-comment">/*
			 *	Make a new object, and place it in the
			 *	object chain.  Note that no new references
			 *	have appeared -- one just moved from the
			 *	map to the new object.
			 */</span>

			<span class="enscript-keyword">if</span> (vm_map_lock_read_to_write(map)) {
				vm_map_lock_read(map);
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">RetryLookup</span>;
			}
			VME_OBJECT_SHADOW(entry,
					  (vm_map_size_t) (entry-&gt;vme_end -
							   entry-&gt;vme_start));

			VME_OBJECT(entry)-&gt;shadowed = TRUE;
			entry-&gt;needs_copy = FALSE;
			vm_map_lock_write_to_read(map);
		}
		<span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/*
			 *	We're attempting to read a copy-on-write
			 *	page -- don't allow writes.
			 */</span>

			prot &amp;= (~VM_PROT_WRITE);
		}
	}

	<span class="enscript-comment">/*
	 *	Create an object if necessary.
	 */</span>
	<span class="enscript-keyword">if</span> (VME_OBJECT(entry) == VM_OBJECT_NULL) {

		<span class="enscript-keyword">if</span> (vm_map_lock_read_to_write(map)) {
			vm_map_lock_read(map);
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">RetryLookup</span>;
		}

		VME_OBJECT_SET(entry,
			       vm_object_allocate(
				       (vm_map_size_t)(entry-&gt;vme_end -
						       entry-&gt;vme_start)));
		VME_OFFSET_SET(entry, 0);
		vm_map_lock_write_to_read(map);
	}

	<span class="enscript-comment">/*
	 *	Return the object/offset from this entry.  If the entry
	 *	was copy-on-write or empty, it has been fixed up.  Also
	 *	return the protection.
	 */</span>

        *offset = (vaddr - entry-&gt;vme_start) + VME_OFFSET(entry);
        *object = VME_OBJECT(entry);
	*out_prot = prot;

	<span class="enscript-keyword">if</span> (fault_info) {
		fault_info-&gt;interruptible = THREAD_UNINT; <span class="enscript-comment">/* for now... */</span>
		<span class="enscript-comment">/* ... the caller will change &quot;interruptible&quot; if needed */</span>
	        fault_info-&gt;cluster_size = 0;
		fault_info-&gt;user_tag = VME_ALIAS(entry);
		fault_info-&gt;pmap_options = 0;
		<span class="enscript-keyword">if</span> (entry-&gt;iokit_acct ||
		    (!entry-&gt;is_sub_map &amp;&amp; !entry-&gt;use_pmap)) {
			fault_info-&gt;pmap_options |= PMAP_OPTIONS_ALT_ACCT;
		}
	        fault_info-&gt;behavior = entry-&gt;behavior;
		fault_info-&gt;lo_offset = VME_OFFSET(entry);
		fault_info-&gt;hi_offset =
			(entry-&gt;vme_end - entry-&gt;vme_start) + VME_OFFSET(entry);
		fault_info-&gt;no_cache  = entry-&gt;no_cache;
		fault_info-&gt;stealth = FALSE;
		fault_info-&gt;io_sync = FALSE;
		<span class="enscript-keyword">if</span> (entry-&gt;used_for_jit ||
		    entry-&gt;vme_resilient_codesign) {
			fault_info-&gt;cs_bypass = TRUE;
		} <span class="enscript-keyword">else</span> {
			fault_info-&gt;cs_bypass = FALSE;
		}
		fault_info-&gt;mark_zf_absent = FALSE;
		fault_info-&gt;batch_pmap_op = FALSE;
	}

	<span class="enscript-comment">/*
	 *	Lock the object to prevent it from disappearing
	 */</span>
	<span class="enscript-keyword">if</span> (object_lock_type == OBJECT_LOCK_EXCLUSIVE)
	        vm_object_lock(*object);
	<span class="enscript-keyword">else</span>
	        vm_object_lock_shared(*object);
	
	<span class="enscript-comment">/*
	 *	Save the version number
	 */</span>

	out_version-&gt;main_timestamp = map-&gt;timestamp;

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}


<span class="enscript-comment">/*
 *	vm_map_verify:
 *
 *	Verifies that the map in question has not changed
 *	since the given version.  If successful, the map
 *	will not change until vm_map_verify_done() is called.
 */</span>
boolean_t
<span class="enscript-function-name">vm_map_verify</span>(
	<span class="enscript-type">register</span> vm_map_t		map,
	<span class="enscript-type">register</span> vm_map_version_t	*version)	<span class="enscript-comment">/* REF */</span>
{
	boolean_t	result;

	vm_map_lock_read(map);
	result = (map-&gt;timestamp == version-&gt;main_timestamp);

	<span class="enscript-keyword">if</span> (!result)
		vm_map_unlock_read(map);

	<span class="enscript-keyword">return</span>(result);
}

<span class="enscript-comment">/*
 *	vm_map_verify_done:
 *
 *	Releases locks acquired by a vm_map_verify.
 *
 *	This is now a macro in vm/vm_map.h.  It does a
 *	vm_map_unlock_read on the map.
 */</span>


<span class="enscript-comment">/*
 *	TEMPORARYTEMPORARYTEMPORARYTEMPORARYTEMPORARYTEMPORARY
 *	Goes away after regular vm_region_recurse function migrates to
 *	64 bits
 *	vm_region_recurse: A form of vm_region which follows the
 *	submaps in a target map
 *
 */</span>

kern_return_t
<span class="enscript-function-name">vm_map_region_recurse_64</span>(
	vm_map_t		 map,
	vm_map_offset_t	*address,		<span class="enscript-comment">/* IN/OUT */</span>
	vm_map_size_t		*size,			<span class="enscript-comment">/* OUT */</span>
	natural_t	 	*nesting_depth,	<span class="enscript-comment">/* IN/OUT */</span>
	vm_region_submap_info_64_t	submap_info,	<span class="enscript-comment">/* IN/OUT */</span>
	mach_msg_type_number_t	*count)	<span class="enscript-comment">/* IN/OUT */</span>
{
	mach_msg_type_number_t	original_count;
	vm_region_extended_info_data_t	extended;
	vm_map_entry_t			tmp_entry;
	vm_map_offset_t			user_address;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>			user_max_depth;

	<span class="enscript-comment">/*
	 * &quot;curr_entry&quot; is the VM map entry preceding or including the
	 * address we're looking for.
	 * &quot;curr_map&quot; is the map or sub-map containing &quot;curr_entry&quot;.
	 * &quot;curr_address&quot; is the equivalent of the top map's &quot;user_address&quot; 
	 * in the current map.
	 * &quot;curr_offset&quot; is the cumulated offset of &quot;curr_map&quot; in the
	 * target task's address space.
	 * &quot;curr_depth&quot; is the depth of &quot;curr_map&quot; in the chain of
	 * sub-maps.
	 * 
	 * &quot;curr_max_below&quot; and &quot;curr_max_above&quot; limit the range (around
	 * &quot;curr_address&quot;) we should take into account in the current (sub)map.
	 * They limit the range to what's visible through the map entries
	 * we've traversed from the top map to the current map.

	 */</span>
	vm_map_entry_t			curr_entry;
	vm_map_address_t		curr_address;
	vm_map_offset_t			curr_offset;
	vm_map_t			curr_map;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>			curr_depth;
	vm_map_offset_t			curr_max_below, curr_max_above;
	vm_map_offset_t			curr_skip;

	<span class="enscript-comment">/*
	 * &quot;next_&quot; is the same as &quot;curr_&quot; but for the VM region immediately
	 * after the address we're looking for.  We need to keep track of this
	 * too because we want to return info about that region if the
	 * address we're looking for is not mapped.
	 */</span>
	vm_map_entry_t			next_entry;
	vm_map_offset_t			next_offset;
	vm_map_offset_t			next_address;
	vm_map_t			next_map;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>			next_depth;
	vm_map_offset_t			next_max_below, next_max_above;
	vm_map_offset_t			next_skip;

	boolean_t			look_for_pages;
	vm_region_submap_short_info_64_t short_info;

	<span class="enscript-keyword">if</span> (map == VM_MAP_NULL) {
		<span class="enscript-comment">/* no address space to work on */</span>
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}

	
	<span class="enscript-keyword">if</span> (*count &lt; VM_REGION_SUBMAP_SHORT_INFO_COUNT_64) {
		<span class="enscript-comment">/*
		 * &quot;info&quot; structure is not big enough and
		 * would overflow
		 */</span>
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}
	
	original_count = *count;
	
	<span class="enscript-keyword">if</span> (original_count &lt; VM_REGION_SUBMAP_INFO_V0_COUNT_64) {
		*count = VM_REGION_SUBMAP_SHORT_INFO_COUNT_64;
		look_for_pages = FALSE;
		short_info = (vm_region_submap_short_info_64_t) submap_info;
		submap_info = NULL;
	} <span class="enscript-keyword">else</span> {
		look_for_pages = TRUE;
		*count = VM_REGION_SUBMAP_INFO_V0_COUNT_64;
		short_info = NULL;
		
		<span class="enscript-keyword">if</span> (original_count &gt;= VM_REGION_SUBMAP_INFO_V1_COUNT_64) {
			*count = VM_REGION_SUBMAP_INFO_V1_COUNT_64;
		}
	}
	
	user_address = *address;
	user_max_depth = *nesting_depth;
	
	<span class="enscript-keyword">if</span> (not_in_kdp) {
		vm_map_lock_read(map);
	}

<span class="enscript-reference">recurse_again</span>:
	curr_entry = NULL;
	curr_map = map;
	curr_address = user_address;
	curr_offset = 0;
	curr_skip = 0;
	curr_depth = 0;
	curr_max_above = ((vm_map_offset_t) -1) - curr_address;
	curr_max_below = curr_address;

	next_entry = NULL;
	next_map = NULL;
	next_address = 0;
	next_offset = 0;
	next_skip = 0;
	next_depth = 0;
	next_max_above = (vm_map_offset_t) -1;
	next_max_below = (vm_map_offset_t) -1;

	<span class="enscript-keyword">for</span> (;;) {
		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(curr_map,
					curr_address,
					&amp;tmp_entry)) {
			<span class="enscript-comment">/* tmp_entry contains the address we're looking for */</span>
			curr_entry = tmp_entry;
		} <span class="enscript-keyword">else</span> {
			vm_map_offset_t skip;
			<span class="enscript-comment">/*
			 * The address is not mapped.  &quot;tmp_entry&quot; is the
			 * map entry preceding the address.  We want the next
			 * one, if it exists.
			 */</span>
			curr_entry = tmp_entry-&gt;vme_next;

			<span class="enscript-keyword">if</span> (curr_entry == vm_map_to_entry(curr_map) ||
			    (curr_entry-&gt;vme_start &gt;=
			     curr_address + curr_max_above)) {
				<span class="enscript-comment">/* no next entry at this level: stop looking */</span>
				<span class="enscript-keyword">if</span> (not_in_kdp) {
					vm_map_unlock_read(curr_map);
				}
				curr_entry = NULL;
				curr_map = NULL;
				curr_skip = 0;
				curr_offset = 0;
				curr_depth = 0;
				curr_max_above = 0;
				curr_max_below = 0;
				<span class="enscript-keyword">break</span>;
			}

			<span class="enscript-comment">/* adjust current address and offset */</span>
			skip = curr_entry-&gt;vme_start - curr_address;
			curr_address = curr_entry-&gt;vme_start;
			curr_skip += skip;
			curr_offset += skip;
			curr_max_above -= skip;
			curr_max_below = 0;
		}

		<span class="enscript-comment">/*
		 * Is the next entry at this level closer to the address (or
		 * deeper in the submap chain) than the one we had
		 * so far ?
		 */</span>
		tmp_entry = curr_entry-&gt;vme_next;
		<span class="enscript-keyword">if</span> (tmp_entry == vm_map_to_entry(curr_map)) {
			<span class="enscript-comment">/* no next entry at this level */</span>
		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (tmp_entry-&gt;vme_start &gt;=
			   curr_address + curr_max_above) {
			<span class="enscript-comment">/*
			 * tmp_entry is beyond the scope of what we mapped of
			 * this submap in the upper level: ignore it.
			 */</span>
		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> ((next_entry == NULL) ||
			   (tmp_entry-&gt;vme_start + curr_offset &lt;=
			    next_entry-&gt;vme_start + next_offset)) {
			<span class="enscript-comment">/*
			 * We didn't have a &quot;next_entry&quot; or this one is
			 * closer to the address we're looking for:
			 * use this &quot;tmp_entry&quot; as the new &quot;next_entry&quot;.
			 */</span>
			<span class="enscript-keyword">if</span> (next_entry != NULL) {
				<span class="enscript-comment">/* unlock the last &quot;next_map&quot; */</span>
				<span class="enscript-keyword">if</span> (next_map != curr_map &amp;&amp; not_in_kdp) {
					vm_map_unlock_read(next_map);
				}
			}
			next_entry = tmp_entry;
			next_map = curr_map;
			next_depth = curr_depth;
			next_address = next_entry-&gt;vme_start;
			next_skip = curr_skip;
			next_skip += (next_address - curr_address);
			next_offset = curr_offset;
			next_offset += (next_address - curr_address);
			next_max_above = MIN(next_max_above, curr_max_above);
			next_max_above = MIN(next_max_above,
					     next_entry-&gt;vme_end - next_address);
			next_max_below = MIN(next_max_below, curr_max_below);
			next_max_below = MIN(next_max_below,
					     next_address - next_entry-&gt;vme_start);
		}

		<span class="enscript-comment">/*
		 * &quot;curr_max_{above,below}&quot; allow us to keep track of the
		 * portion of the submap that is actually mapped at this level:
		 * the rest of that submap is irrelevant to us, since it's not
		 * mapped here.
		 * The relevant portion of the map starts at
		 * &quot;VME_OFFSET(curr_entry)&quot; up to the size of &quot;curr_entry&quot;.
		 */</span>
		curr_max_above = MIN(curr_max_above,
				     curr_entry-&gt;vme_end - curr_address);
		curr_max_below = MIN(curr_max_below,
				     curr_address - curr_entry-&gt;vme_start);

		<span class="enscript-keyword">if</span> (!curr_entry-&gt;is_sub_map ||
		    curr_depth &gt;= user_max_depth) {
			<span class="enscript-comment">/*
			 * We hit a leaf map or we reached the maximum depth
			 * we could, so stop looking.  Keep the current map
			 * locked.
			 */</span>
			<span class="enscript-keyword">break</span>;
		}

		<span class="enscript-comment">/*
		 * Get down to the next submap level.
		 */</span>

		<span class="enscript-comment">/*
		 * Lock the next level and unlock the current level,
		 * unless we need to keep it locked to access the &quot;next_entry&quot;
		 * later.
		 */</span>
		<span class="enscript-keyword">if</span> (not_in_kdp) {
			vm_map_lock_read(VME_SUBMAP(curr_entry));
		}
		<span class="enscript-keyword">if</span> (curr_map == next_map) {
			<span class="enscript-comment">/* keep &quot;next_map&quot; locked in case we need it */</span>
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/* release this map */</span>
			<span class="enscript-keyword">if</span> (not_in_kdp)
				vm_map_unlock_read(curr_map);
		}

		<span class="enscript-comment">/*
		 * Adjust the offset.  &quot;curr_entry&quot; maps the submap
		 * at relative address &quot;curr_entry-&gt;vme_start&quot; in the
		 * curr_map but skips the first &quot;VME_OFFSET(curr_entry)&quot;
		 * bytes of the submap.
		 * &quot;curr_offset&quot; always represents the offset of a virtual
		 * address in the curr_map relative to the absolute address
		 * space (i.e. the top-level VM map).
		 */</span>
		curr_offset +=
			(VME_OFFSET(curr_entry) - curr_entry-&gt;vme_start);
		curr_address = user_address + curr_offset;
		<span class="enscript-comment">/* switch to the submap */</span>
		curr_map = VME_SUBMAP(curr_entry);
		curr_depth++;
		curr_entry = NULL;
	}

	<span class="enscript-keyword">if</span> (curr_entry == NULL) {
		<span class="enscript-comment">/* no VM region contains the address... */</span>
		<span class="enscript-keyword">if</span> (next_entry == NULL) {
			<span class="enscript-comment">/* ... and no VM region follows it either */</span>
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}
		<span class="enscript-comment">/* ... gather info about the next VM region */</span>
		curr_entry = next_entry;
		curr_map = next_map;	<span class="enscript-comment">/* still locked ... */</span>
		curr_address = next_address;
		curr_skip = next_skip;
		curr_offset = next_offset;
		curr_depth = next_depth;
		curr_max_above = next_max_above;
		curr_max_below = next_max_below;
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-comment">/* we won't need &quot;next_entry&quot; after all */</span>
		<span class="enscript-keyword">if</span> (next_entry != NULL) {
			<span class="enscript-comment">/* release &quot;next_map&quot; */</span>
			<span class="enscript-keyword">if</span> (next_map != curr_map &amp;&amp; not_in_kdp) {
				vm_map_unlock_read(next_map);
			}
		}
	}
	next_entry = NULL;
	next_map = NULL;
	next_offset = 0;
	next_skip = 0;
	next_depth = 0;
	next_max_below = -1;
	next_max_above = -1;

	<span class="enscript-keyword">if</span> (curr_entry-&gt;is_sub_map &amp;&amp;
	    curr_depth &lt; user_max_depth) {
		<span class="enscript-comment">/*
		 * We're not as deep as we could be:  we must have
		 * gone back up after not finding anything mapped
		 * below the original top-level map entry's.
		 * Let's move &quot;curr_address&quot; forward and recurse again.
		 */</span>
		user_address = curr_address;
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">recurse_again</span>;
	}

	*nesting_depth = curr_depth;
	*size = curr_max_above + curr_max_below;
	*address = user_address + curr_skip - curr_max_below;

<span class="enscript-comment">// LP64todo: all the current tools are 32bit, obviously never worked for 64b
</span><span class="enscript-comment">// so probably should be a real 32b ID vs. ptr.
</span><span class="enscript-comment">// Current users just check for equality
</span>#<span class="enscript-reference">define</span> <span class="enscript-function-name">INFO_MAKE_OBJECT_ID</span>(p)	((uint32_t)(uintptr_t)VM_KERNEL_ADDRPERM(p))

	<span class="enscript-keyword">if</span> (look_for_pages) {
		submap_info-&gt;user_tag = VME_ALIAS(curr_entry);
		submap_info-&gt;offset = VME_OFFSET(curr_entry); 
		submap_info-&gt;protection = curr_entry-&gt;protection;
		submap_info-&gt;inheritance = curr_entry-&gt;inheritance;
		submap_info-&gt;max_protection = curr_entry-&gt;max_protection;
		submap_info-&gt;behavior = curr_entry-&gt;behavior;
		submap_info-&gt;user_wired_count = curr_entry-&gt;user_wired_count;
		submap_info-&gt;is_submap = curr_entry-&gt;is_sub_map;
		submap_info-&gt;object_id = INFO_MAKE_OBJECT_ID(VME_OBJECT(curr_entry));
	} <span class="enscript-keyword">else</span> {
		short_info-&gt;user_tag = VME_ALIAS(curr_entry);
		short_info-&gt;offset = VME_OFFSET(curr_entry); 
		short_info-&gt;protection = curr_entry-&gt;protection;
		short_info-&gt;inheritance = curr_entry-&gt;inheritance;
		short_info-&gt;max_protection = curr_entry-&gt;max_protection;
		short_info-&gt;behavior = curr_entry-&gt;behavior;
		short_info-&gt;user_wired_count = curr_entry-&gt;user_wired_count;
		short_info-&gt;is_submap = curr_entry-&gt;is_sub_map;
		short_info-&gt;object_id = INFO_MAKE_OBJECT_ID(VME_OBJECT(curr_entry));
	}

	extended.pages_resident = 0;
	extended.pages_swapped_out = 0;
	extended.pages_shared_now_private = 0;
	extended.pages_dirtied = 0;
	extended.pages_reusable = 0;
	extended.external_pager = 0;
	extended.shadow_depth = 0;
	extended.share_mode = SM_EMPTY;
	extended.ref_count = 0;

	<span class="enscript-keyword">if</span> (not_in_kdp) {
		<span class="enscript-keyword">if</span> (!curr_entry-&gt;is_sub_map) {
			vm_map_offset_t range_start, range_end;
			range_start = MAX((curr_address - curr_max_below),
					  curr_entry-&gt;vme_start);
			range_end = MIN((curr_address + curr_max_above),
					curr_entry-&gt;vme_end);
			vm_map_region_walk(curr_map,
					   range_start,
					   curr_entry,
					   (VME_OFFSET(curr_entry) +
					    (range_start -
					     curr_entry-&gt;vme_start)),
					   range_end - range_start,
					   &amp;extended,
					   look_for_pages, VM_REGION_EXTENDED_INFO_COUNT);
			<span class="enscript-keyword">if</span> (extended.external_pager &amp;&amp;
			    extended.ref_count == 2 &amp;&amp;
			    extended.share_mode == SM_SHARED) {
				extended.share_mode = SM_PRIVATE;
			}
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> (curr_entry-&gt;use_pmap) {
				extended.share_mode = SM_TRUESHARED;
			} <span class="enscript-keyword">else</span> {
				extended.share_mode = SM_PRIVATE;
			}
			extended.ref_count = VME_SUBMAP(curr_entry)-&gt;ref_count;
		}
	}

	<span class="enscript-keyword">if</span> (look_for_pages) {
		submap_info-&gt;pages_resident = extended.pages_resident;
		submap_info-&gt;pages_swapped_out = extended.pages_swapped_out;
		submap_info-&gt;pages_shared_now_private =
			extended.pages_shared_now_private;
		submap_info-&gt;pages_dirtied = extended.pages_dirtied;
		submap_info-&gt;external_pager = extended.external_pager;
		submap_info-&gt;shadow_depth = extended.shadow_depth;
		submap_info-&gt;share_mode = extended.share_mode;
		submap_info-&gt;ref_count = extended.ref_count;
		
		<span class="enscript-keyword">if</span> (original_count &gt;= VM_REGION_SUBMAP_INFO_V1_COUNT_64) {
			submap_info-&gt;pages_reusable = extended.pages_reusable;
		}
	} <span class="enscript-keyword">else</span> {
		short_info-&gt;external_pager = extended.external_pager;
		short_info-&gt;shadow_depth = extended.shadow_depth;
		short_info-&gt;share_mode = extended.share_mode;
		short_info-&gt;ref_count = extended.ref_count;
	}

	<span class="enscript-keyword">if</span> (not_in_kdp) {
		vm_map_unlock_read(curr_map);
	}

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 *	vm_region:
 *
 *	User call to obtain information about a region in
 *	a task's address map. Currently, only one flavor is
 *	supported.
 *
 *	XXX The reserved and behavior fields cannot be filled
 *	    in until the vm merge from the IK is completed, and
 *	    vm_reserve is implemented.
 */</span>

kern_return_t
<span class="enscript-function-name">vm_map_region</span>(
	vm_map_t		 map,
	vm_map_offset_t	*address,		<span class="enscript-comment">/* IN/OUT */</span>
	vm_map_size_t		*size,			<span class="enscript-comment">/* OUT */</span>
	vm_region_flavor_t	 flavor,		<span class="enscript-comment">/* IN */</span>
	vm_region_info_t	 info,			<span class="enscript-comment">/* OUT */</span>
	mach_msg_type_number_t	*count,	<span class="enscript-comment">/* IN/OUT */</span>
	mach_port_t		*object_name)		<span class="enscript-comment">/* OUT */</span>
{
	vm_map_entry_t		tmp_entry;
	vm_map_entry_t		entry;
	vm_map_offset_t		start;

	<span class="enscript-keyword">if</span> (map == VM_MAP_NULL) 
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

	<span class="enscript-keyword">switch</span> (flavor) {

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_REGION_BASIC_INFO</span>:
		<span class="enscript-comment">/* legacy for old 32-bit objects info */</span>
	{
		vm_region_basic_info_t	basic;

		<span class="enscript-keyword">if</span> (*count &lt; VM_REGION_BASIC_INFO_COUNT)
			<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

		basic = (vm_region_basic_info_t) info;
		*count = VM_REGION_BASIC_INFO_COUNT;

		vm_map_lock_read(map);

		start = *address;
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;tmp_entry)) {
			<span class="enscript-keyword">if</span> ((entry = tmp_entry-&gt;vme_next) == vm_map_to_entry(map)) {
				vm_map_unlock_read(map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}
		} <span class="enscript-keyword">else</span> {
			entry = tmp_entry;
		}

		start = entry-&gt;vme_start;

		basic-&gt;offset = (uint32_t)VME_OFFSET(entry);
		basic-&gt;protection = entry-&gt;protection;
		basic-&gt;inheritance = entry-&gt;inheritance;
		basic-&gt;max_protection = entry-&gt;max_protection;
		basic-&gt;behavior = entry-&gt;behavior;
		basic-&gt;user_wired_count = entry-&gt;user_wired_count;
		basic-&gt;reserved = entry-&gt;is_sub_map;
		*address = start;
		*size = (entry-&gt;vme_end - start);

		<span class="enscript-keyword">if</span> (object_name) *object_name = IP_NULL;
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			basic-&gt;shared = FALSE;
		} <span class="enscript-keyword">else</span> {
			basic-&gt;shared = entry-&gt;is_shared;
		}

		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_REGION_BASIC_INFO_64</span>:
	{
		vm_region_basic_info_64_t	basic;

		<span class="enscript-keyword">if</span> (*count &lt; VM_REGION_BASIC_INFO_COUNT_64)
			<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

		basic = (vm_region_basic_info_64_t) info;
		*count = VM_REGION_BASIC_INFO_COUNT_64;

		vm_map_lock_read(map);

		start = *address;
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;tmp_entry)) {
			<span class="enscript-keyword">if</span> ((entry = tmp_entry-&gt;vme_next) == vm_map_to_entry(map)) {
				vm_map_unlock_read(map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}
		} <span class="enscript-keyword">else</span> {
			entry = tmp_entry;
		}

		start = entry-&gt;vme_start;

		basic-&gt;offset = VME_OFFSET(entry);
		basic-&gt;protection = entry-&gt;protection;
		basic-&gt;inheritance = entry-&gt;inheritance;
		basic-&gt;max_protection = entry-&gt;max_protection;
		basic-&gt;behavior = entry-&gt;behavior;
		basic-&gt;user_wired_count = entry-&gt;user_wired_count;
		basic-&gt;reserved = entry-&gt;is_sub_map;
		*address = start;
		*size = (entry-&gt;vme_end - start);

		<span class="enscript-keyword">if</span> (object_name) *object_name = IP_NULL;
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			basic-&gt;shared = FALSE;
		} <span class="enscript-keyword">else</span> {
			basic-&gt;shared = entry-&gt;is_shared;
		}

		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_REGION_EXTENDED_INFO</span>:
		<span class="enscript-keyword">if</span> (*count &lt; VM_REGION_EXTENDED_INFO_COUNT)
			<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);
		<span class="enscript-comment">/*fallthru*/</span>
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_REGION_EXTENDED_INFO__legacy</span>:
		<span class="enscript-keyword">if</span> (*count &lt; VM_REGION_EXTENDED_INFO_COUNT__legacy)
			<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;

	{
		vm_region_extended_info_t	extended;
		mach_msg_type_number_t original_count;

		extended = (vm_region_extended_info_t) info;

		vm_map_lock_read(map);

		start = *address;
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;tmp_entry)) {
			<span class="enscript-keyword">if</span> ((entry = tmp_entry-&gt;vme_next) == vm_map_to_entry(map)) {
				vm_map_unlock_read(map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}
		} <span class="enscript-keyword">else</span> {
			entry = tmp_entry;
		}
		start = entry-&gt;vme_start;

		extended-&gt;protection = entry-&gt;protection;
		extended-&gt;user_tag = VME_ALIAS(entry);
		extended-&gt;pages_resident = 0;
		extended-&gt;pages_swapped_out = 0;
		extended-&gt;pages_shared_now_private = 0;
		extended-&gt;pages_dirtied = 0;
		extended-&gt;external_pager = 0;
		extended-&gt;shadow_depth = 0;

		original_count = *count;
		<span class="enscript-keyword">if</span> (flavor == VM_REGION_EXTENDED_INFO__legacy) {
			*count = VM_REGION_EXTENDED_INFO_COUNT__legacy;
		} <span class="enscript-keyword">else</span> {
			extended-&gt;pages_reusable = 0;
			*count = VM_REGION_EXTENDED_INFO_COUNT;
		}

		vm_map_region_walk(map, start, entry, VME_OFFSET(entry), entry-&gt;vme_end - start, extended, TRUE, *count);

		<span class="enscript-keyword">if</span> (extended-&gt;external_pager &amp;&amp; extended-&gt;ref_count == 2 &amp;&amp; extended-&gt;share_mode == SM_SHARED)
			extended-&gt;share_mode = SM_PRIVATE;

		<span class="enscript-keyword">if</span> (object_name)
			*object_name = IP_NULL;
		*address = start;
		*size = (entry-&gt;vme_end - start);

		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_REGION_TOP_INFO</span>:
	{   
		vm_region_top_info_t	top;

		<span class="enscript-keyword">if</span> (*count &lt; VM_REGION_TOP_INFO_COUNT)
			<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

		top = (vm_region_top_info_t) info;
		*count = VM_REGION_TOP_INFO_COUNT;

		vm_map_lock_read(map);

		start = *address;
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;tmp_entry)) {
			<span class="enscript-keyword">if</span> ((entry = tmp_entry-&gt;vme_next) == vm_map_to_entry(map)) {
				vm_map_unlock_read(map);
				<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
			}
		} <span class="enscript-keyword">else</span> {
			entry = tmp_entry;

		}
		start = entry-&gt;vme_start;

		top-&gt;private_pages_resident = 0;
		top-&gt;shared_pages_resident = 0;

		vm_map_region_top_walk(entry, top);

		<span class="enscript-keyword">if</span> (object_name)
			*object_name = IP_NULL;
		*address = start;
		*size = (entry-&gt;vme_end - start);

		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);
	}
	<span class="enscript-reference">default</span>:
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);
	}
}

#<span class="enscript-reference">define</span> <span class="enscript-function-name">OBJ_RESIDENT_COUNT</span>(obj, entry_size)				\
	MIN((entry_size),						\
	    ((obj)-&gt;all_reusable ?					\
	     (obj)-&gt;wired_page_count :					\
	     (obj)-&gt;resident_page_count - (obj)-&gt;reusable_page_count))

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_region_top_walk</span>(
        vm_map_entry_t		   entry,
	vm_region_top_info_t       top)
{

	<span class="enscript-keyword">if</span> (VME_OBJECT(entry) == 0 || entry-&gt;is_sub_map) {
		top-&gt;share_mode = SM_EMPTY;
		top-&gt;ref_count = 0;
		top-&gt;obj_id = 0;
		<span class="enscript-keyword">return</span>;
	}

	{
	        <span class="enscript-type">struct</span>	vm_object *obj, *tmp_obj;
		<span class="enscript-type">int</span>		ref_count;
		uint32_t	entry_size;

		entry_size = (uint32_t) ((entry-&gt;vme_end - entry-&gt;vme_start) / PAGE_SIZE_64);

		obj = VME_OBJECT(entry);

		vm_object_lock(obj);

		<span class="enscript-keyword">if</span> ((ref_count = obj-&gt;ref_count) &gt; 1 &amp;&amp; obj-&gt;paging_in_progress)
			ref_count--;

		assert(obj-&gt;reusable_page_count &lt;= obj-&gt;resident_page_count);
		<span class="enscript-keyword">if</span> (obj-&gt;shadow) {
			<span class="enscript-keyword">if</span> (ref_count == 1)
				top-&gt;private_pages_resident =
					OBJ_RESIDENT_COUNT(obj, entry_size);
			<span class="enscript-keyword">else</span>
				top-&gt;shared_pages_resident =
					OBJ_RESIDENT_COUNT(obj, entry_size);
			top-&gt;ref_count  = ref_count;
			top-&gt;share_mode = SM_COW;
	    
			<span class="enscript-keyword">while</span> ((tmp_obj = obj-&gt;shadow)) {
				vm_object_lock(tmp_obj);
				vm_object_unlock(obj);
				obj = tmp_obj;

				<span class="enscript-keyword">if</span> ((ref_count = obj-&gt;ref_count) &gt; 1 &amp;&amp; obj-&gt;paging_in_progress)
					ref_count--;

				assert(obj-&gt;reusable_page_count &lt;= obj-&gt;resident_page_count);
				top-&gt;shared_pages_resident +=
					OBJ_RESIDENT_COUNT(obj, entry_size);
				top-&gt;ref_count += ref_count - 1;
			}
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> (entry-&gt;superpage_size) {
				top-&gt;share_mode = SM_LARGE_PAGE;
				top-&gt;shared_pages_resident = 0;
				top-&gt;private_pages_resident = entry_size;
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (entry-&gt;needs_copy) {
				top-&gt;share_mode = SM_COW;
				top-&gt;shared_pages_resident =
					OBJ_RESIDENT_COUNT(obj, entry_size);
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-keyword">if</span> (ref_count == 1 ||
				    (ref_count == 2 &amp;&amp; !(obj-&gt;pager_trusted) &amp;&amp; !(obj-&gt;internal))) {
					top-&gt;share_mode = SM_PRIVATE;
						top-&gt;private_pages_resident =
							OBJ_RESIDENT_COUNT(obj,
									   entry_size);
				} <span class="enscript-keyword">else</span> {
					top-&gt;share_mode = SM_SHARED;
					top-&gt;shared_pages_resident =
						OBJ_RESIDENT_COUNT(obj,
								  entry_size);
				}
			}
			top-&gt;ref_count = ref_count;
		}
		<span class="enscript-comment">/* XXX K64: obj_id will be truncated */</span>
		top-&gt;obj_id = (<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>) (uintptr_t)VM_KERNEL_ADDRPERM(obj);

		vm_object_unlock(obj);
	}
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_region_walk</span>(
	vm_map_t		   	map,
	vm_map_offset_t			va,
	vm_map_entry_t			entry,
	vm_object_offset_t		offset,
	vm_object_size_t		range,
	vm_region_extended_info_t	extended,
	boolean_t			look_for_pages,
	mach_msg_type_number_t count)
{
        <span class="enscript-type">register</span> <span class="enscript-type">struct</span> vm_object *obj, *tmp_obj;
	<span class="enscript-type">register</span> vm_map_offset_t       last_offset;
	<span class="enscript-type">register</span> <span class="enscript-type">int</span>               i;
	<span class="enscript-type">register</span> <span class="enscript-type">int</span>               ref_count;
	<span class="enscript-type">struct</span> vm_object	*shadow_object;
	<span class="enscript-type">int</span>			shadow_depth;

	<span class="enscript-keyword">if</span> ((VME_OBJECT(entry) == 0) ||
	    (entry-&gt;is_sub_map) ||
	    (VME_OBJECT(entry)-&gt;phys_contiguous &amp;&amp;
	     !entry-&gt;superpage_size)) {
		extended-&gt;share_mode = SM_EMPTY;
		extended-&gt;ref_count = 0;
		<span class="enscript-keyword">return</span>;
	}

	<span class="enscript-keyword">if</span> (entry-&gt;superpage_size) {
		extended-&gt;shadow_depth = 0;
		extended-&gt;share_mode = SM_LARGE_PAGE;
		extended-&gt;ref_count = 1;
		extended-&gt;external_pager = 0;
		extended-&gt;pages_resident = (<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>)(range &gt;&gt; PAGE_SHIFT);
		extended-&gt;shadow_depth = 0;
		<span class="enscript-keyword">return</span>;
	}

	{
		obj = VME_OBJECT(entry);

		vm_object_lock(obj);

		<span class="enscript-keyword">if</span> ((ref_count = obj-&gt;ref_count) &gt; 1 &amp;&amp; obj-&gt;paging_in_progress)
			ref_count--;

		<span class="enscript-keyword">if</span> (look_for_pages) {
			<span class="enscript-keyword">for</span> (last_offset = offset + range;
			     offset &lt; last_offset;
			     offset += PAGE_SIZE_64, va += PAGE_SIZE) {
					vm_map_region_look_for_page(map, va, obj,
								    offset, ref_count,
								    0, extended, count);
			}
		} <span class="enscript-keyword">else</span> {
			shadow_object = obj-&gt;shadow;
			shadow_depth = 0;

			<span class="enscript-keyword">if</span> ( !(obj-&gt;pager_trusted) &amp;&amp; !(obj-&gt;internal))
				extended-&gt;external_pager = 1;

			<span class="enscript-keyword">if</span> (shadow_object != VM_OBJECT_NULL) {
				vm_object_lock(shadow_object);
				<span class="enscript-keyword">for</span> (;
				     shadow_object != VM_OBJECT_NULL;
				     shadow_depth++) {
					vm_object_t	next_shadow;

					<span class="enscript-keyword">if</span> ( !(shadow_object-&gt;pager_trusted) &amp;&amp;
					     !(shadow_object-&gt;internal))
						extended-&gt;external_pager = 1;

					next_shadow = shadow_object-&gt;shadow;
					<span class="enscript-keyword">if</span> (next_shadow) {
						vm_object_lock(next_shadow);
					}
					vm_object_unlock(shadow_object);
					shadow_object = next_shadow;
				}
			}
			extended-&gt;shadow_depth = shadow_depth;
		}

		<span class="enscript-keyword">if</span> (extended-&gt;shadow_depth || entry-&gt;needs_copy)
			extended-&gt;share_mode = SM_COW;
		<span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> (ref_count == 1)
				extended-&gt;share_mode = SM_PRIVATE;
			<span class="enscript-keyword">else</span> {
				<span class="enscript-keyword">if</span> (obj-&gt;true_share)
					extended-&gt;share_mode = SM_TRUESHARED;
				<span class="enscript-keyword">else</span>
					extended-&gt;share_mode = SM_SHARED;
			}
		}
		extended-&gt;ref_count = ref_count - extended-&gt;shadow_depth;
	    
		<span class="enscript-keyword">for</span> (i = 0; i &lt; extended-&gt;shadow_depth; i++) {
			<span class="enscript-keyword">if</span> ((tmp_obj = obj-&gt;shadow) == 0)
				<span class="enscript-keyword">break</span>;
			vm_object_lock(tmp_obj);
			vm_object_unlock(obj);

			<span class="enscript-keyword">if</span> ((ref_count = tmp_obj-&gt;ref_count) &gt; 1 &amp;&amp; tmp_obj-&gt;paging_in_progress)
				ref_count--;

			extended-&gt;ref_count += ref_count;
			obj = tmp_obj;
		}
		vm_object_unlock(obj);

		<span class="enscript-keyword">if</span> (extended-&gt;share_mode == SM_SHARED) {
			<span class="enscript-type">register</span> vm_map_entry_t	     cur;
			<span class="enscript-type">register</span> vm_map_entry_t	     last;
			<span class="enscript-type">int</span>      my_refs;

			obj = VME_OBJECT(entry);
			last = vm_map_to_entry(map);
			my_refs = 0;

			<span class="enscript-keyword">if</span> ((ref_count = obj-&gt;ref_count) &gt; 1 &amp;&amp; obj-&gt;paging_in_progress)
				ref_count--;
			<span class="enscript-keyword">for</span> (cur = vm_map_first_entry(map); cur != last; cur = cur-&gt;vme_next)
				my_refs += vm_map_region_count_obj_refs(cur, obj);

			<span class="enscript-keyword">if</span> (my_refs == ref_count)
				extended-&gt;share_mode = SM_PRIVATE_ALIASED;
			<span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (my_refs &gt; 1)
				extended-&gt;share_mode = SM_SHARED_ALIASED;
		}
	}
}


<span class="enscript-comment">/* object is locked on entry and locked on return */</span>


<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_region_look_for_page</span>(
	__unused vm_map_t		map,
	__unused vm_map_offset_t	va,
	vm_object_t			object,
	vm_object_offset_t		offset,
	<span class="enscript-type">int</span>				max_refcnt,
	<span class="enscript-type">int</span>				depth,
	vm_region_extended_info_t	extended,
	mach_msg_type_number_t count)
{
        <span class="enscript-type">register</span> vm_page_t	p;
        <span class="enscript-type">register</span> vm_object_t	shadow;
	<span class="enscript-type">register</span> <span class="enscript-type">int</span>            ref_count;
	vm_object_t		caller_object;
	kern_return_t		kr;
	shadow = object-&gt;shadow;
	caller_object = object;

	
	<span class="enscript-keyword">while</span> (TRUE) {

		<span class="enscript-keyword">if</span> ( !(object-&gt;pager_trusted) &amp;&amp; !(object-&gt;internal))
			extended-&gt;external_pager = 1;

		<span class="enscript-keyword">if</span> ((p = vm_page_lookup(object, offset)) != VM_PAGE_NULL) {
	        	<span class="enscript-keyword">if</span> (shadow &amp;&amp; (max_refcnt == 1))
		    		extended-&gt;pages_shared_now_private++;

			<span class="enscript-keyword">if</span> (!p-&gt;fictitious &amp;&amp;
			    (p-&gt;dirty || pmap_is_modified(p-&gt;phys_page)))
		    		extended-&gt;pages_dirtied++;
			<span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (count &gt;= VM_REGION_EXTENDED_INFO_COUNT) {
				<span class="enscript-keyword">if</span> (p-&gt;reusable || p-&gt;object-&gt;all_reusable) {
					extended-&gt;pages_reusable++;
				}
			}

			extended-&gt;pages_resident++;

			<span class="enscript-keyword">if</span>(object != caller_object)
				vm_object_unlock(object);

			<span class="enscript-keyword">return</span>;
		}
#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">MACH_PAGEMAP</span>
		<span class="enscript-keyword">if</span> (object-&gt;existence_map) {
	    		<span class="enscript-keyword">if</span> (vm_external_state_get(object-&gt;existence_map, offset) == VM_EXTERNAL_STATE_EXISTS) {

	        		extended-&gt;pages_swapped_out++;

				<span class="enscript-keyword">if</span>(object != caller_object)
					vm_object_unlock(object);

				<span class="enscript-keyword">return</span>;
	    		}
		} <span class="enscript-keyword">else</span>
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* MACH_PAGEMAP */</span>
		<span class="enscript-keyword">if</span> (object-&gt;internal &amp;&amp;
		    object-&gt;alive &amp;&amp;
		    !object-&gt;terminating &amp;&amp;
		    object-&gt;pager_ready) {

			<span class="enscript-keyword">if</span> (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
				<span class="enscript-keyword">if</span> (VM_COMPRESSOR_PAGER_STATE_GET(object,
								  offset)
				    == VM_EXTERNAL_STATE_EXISTS) {
					<span class="enscript-comment">/* the pager has that page */</span>
					extended-&gt;pages_swapped_out++;
					<span class="enscript-keyword">if</span> (object != caller_object)
						vm_object_unlock(object);
					<span class="enscript-keyword">return</span>;
				}
			} <span class="enscript-keyword">else</span> {
				memory_object_t pager;

				vm_object_paging_begin(object);
				pager = object-&gt;pager;
				vm_object_unlock(object);

				kr = memory_object_data_request(
					pager,
					offset + object-&gt;paging_offset,
					0, <span class="enscript-comment">/* just poke the pager */</span>
					VM_PROT_READ,
					NULL);

				vm_object_lock(object);
				vm_object_paging_end(object);

				<span class="enscript-keyword">if</span> (kr == KERN_SUCCESS) {
					<span class="enscript-comment">/* the pager has that page */</span>
					extended-&gt;pages_swapped_out++;
					<span class="enscript-keyword">if</span> (object != caller_object)
						vm_object_unlock(object);
					<span class="enscript-keyword">return</span>;
				}
			}
		}

		<span class="enscript-keyword">if</span> (shadow) {
			vm_object_lock(shadow);

			<span class="enscript-keyword">if</span> ((ref_count = shadow-&gt;ref_count) &gt; 1 &amp;&amp; shadow-&gt;paging_in_progress)
			        ref_count--;

	    		<span class="enscript-keyword">if</span> (++depth &gt; extended-&gt;shadow_depth)
	        		extended-&gt;shadow_depth = depth;

	    		<span class="enscript-keyword">if</span> (ref_count &gt; max_refcnt)
	        		max_refcnt = ref_count;
			
			<span class="enscript-keyword">if</span>(object != caller_object)
				vm_object_unlock(object);

			offset = offset + object-&gt;vo_shadow_offset;
			object = shadow;
			shadow = object-&gt;shadow;
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-keyword">if</span>(object != caller_object)
			vm_object_unlock(object);
		<span class="enscript-keyword">break</span>;
	}
}

<span class="enscript-type">static</span> <span class="enscript-type">int</span>
<span class="enscript-function-name">vm_map_region_count_obj_refs</span>(
        vm_map_entry_t    entry,
	vm_object_t       object)
{
        <span class="enscript-type">register</span> <span class="enscript-type">int</span> ref_count;
	<span class="enscript-type">register</span> vm_object_t chk_obj;
	<span class="enscript-type">register</span> vm_object_t tmp_obj;

	<span class="enscript-keyword">if</span> (VME_OBJECT(entry) == 0)
		<span class="enscript-keyword">return</span>(0);

        <span class="enscript-keyword">if</span> (entry-&gt;is_sub_map)
		<span class="enscript-keyword">return</span>(0);
	<span class="enscript-keyword">else</span> {
		ref_count = 0;

		chk_obj = VME_OBJECT(entry);
		vm_object_lock(chk_obj);

		<span class="enscript-keyword">while</span> (chk_obj) {
			<span class="enscript-keyword">if</span> (chk_obj == object)
				ref_count++;
			tmp_obj = chk_obj-&gt;shadow;
			<span class="enscript-keyword">if</span> (tmp_obj)
				vm_object_lock(tmp_obj);
			vm_object_unlock(chk_obj);

			chk_obj = tmp_obj;
		}
	}
	<span class="enscript-keyword">return</span>(ref_count);
}


<span class="enscript-comment">/*
 *	Routine:	vm_map_simplify
 *
 *	Description:
 *		Attempt to simplify the map representation in
 *		the vicinity of the given starting address.
 *	Note:
 *		This routine is intended primarily to keep the
 *		kernel maps more compact -- they generally don't
 *		benefit from the &quot;expand a map entry&quot; technology
 *		at allocation time because the adjacent entry
 *		is often wired down.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_simplify_entry</span>(
	vm_map_t	map,
	vm_map_entry_t	this_entry)
{
	vm_map_entry_t	prev_entry;

	counter(c_vm_map_simplify_entry_called++);

	prev_entry = this_entry-&gt;vme_prev;

	<span class="enscript-keyword">if</span> ((this_entry != vm_map_to_entry(map)) &amp;&amp;
	    (prev_entry != vm_map_to_entry(map)) &amp;&amp;

	    (prev_entry-&gt;vme_end == this_entry-&gt;vme_start) &amp;&amp;

	    (prev_entry-&gt;is_sub_map == this_entry-&gt;is_sub_map) &amp;&amp;
	    (VME_OBJECT(prev_entry) == VME_OBJECT(this_entry)) &amp;&amp;
	    ((VME_OFFSET(prev_entry) + (prev_entry-&gt;vme_end -
				    prev_entry-&gt;vme_start))
	     == VME_OFFSET(this_entry)) &amp;&amp;

	    (prev_entry-&gt;behavior == this_entry-&gt;behavior) &amp;&amp;
	    (prev_entry-&gt;needs_copy == this_entry-&gt;needs_copy) &amp;&amp;
	    (prev_entry-&gt;protection == this_entry-&gt;protection) &amp;&amp;
	    (prev_entry-&gt;max_protection == this_entry-&gt;max_protection) &amp;&amp;
	    (prev_entry-&gt;inheritance == this_entry-&gt;inheritance) &amp;&amp;
	    (prev_entry-&gt;use_pmap == this_entry-&gt;use_pmap) &amp;&amp;
	    (VME_ALIAS(prev_entry) == VME_ALIAS(this_entry)) &amp;&amp;
	    (prev_entry-&gt;no_cache == this_entry-&gt;no_cache) &amp;&amp;
	    (prev_entry-&gt;permanent == this_entry-&gt;permanent) &amp;&amp;
	    (prev_entry-&gt;map_aligned == this_entry-&gt;map_aligned) &amp;&amp;
	    (prev_entry-&gt;zero_wired_pages == this_entry-&gt;zero_wired_pages) &amp;&amp;
	    (prev_entry-&gt;used_for_jit == this_entry-&gt;used_for_jit) &amp;&amp;
	    <span class="enscript-comment">/* from_reserved_zone: OK if that field doesn't match */</span>
	    (prev_entry-&gt;iokit_acct == this_entry-&gt;iokit_acct) &amp;&amp;
	    (prev_entry-&gt;vme_resilient_codesign ==
	     this_entry-&gt;vme_resilient_codesign) &amp;&amp;
	    (prev_entry-&gt;vme_resilient_media ==
	     this_entry-&gt;vme_resilient_media) &amp;&amp;

	    (prev_entry-&gt;wired_count == this_entry-&gt;wired_count) &amp;&amp;
	    (prev_entry-&gt;user_wired_count == this_entry-&gt;user_wired_count) &amp;&amp;

	    (prev_entry-&gt;in_transition == FALSE) &amp;&amp;
	    (this_entry-&gt;in_transition == FALSE) &amp;&amp;
	    (prev_entry-&gt;needs_wakeup == FALSE) &amp;&amp;
	    (this_entry-&gt;needs_wakeup == FALSE) &amp;&amp;
	    (prev_entry-&gt;is_shared == FALSE) &amp;&amp;
	    (this_entry-&gt;is_shared == FALSE) &amp;&amp;
	    (prev_entry-&gt;superpage_size == FALSE) &amp;&amp;
	    (this_entry-&gt;superpage_size == FALSE)
		) {
		vm_map_store_entry_unlink(map, prev_entry);
		assert(prev_entry-&gt;vme_start &lt; this_entry-&gt;vme_end);
		<span class="enscript-keyword">if</span> (prev_entry-&gt;map_aligned)
			assert(VM_MAP_PAGE_ALIGNED(prev_entry-&gt;vme_start,
						   VM_MAP_PAGE_MASK(map)));
		this_entry-&gt;vme_start = prev_entry-&gt;vme_start;
		VME_OFFSET_SET(this_entry, VME_OFFSET(prev_entry));

		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
			vm_map_store_update_first_free(map, this_entry, TRUE);
		}

		<span class="enscript-keyword">if</span> (prev_entry-&gt;is_sub_map) {
			vm_map_deallocate(VME_SUBMAP(prev_entry));
		} <span class="enscript-keyword">else</span> {
			vm_object_deallocate(VME_OBJECT(prev_entry));
		}
		vm_map_entry_dispose(map, prev_entry);
		SAVE_HINT_MAP_WRITE(map, this_entry);
		counter(c_vm_map_simplified++);
	}
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_simplify</span>(
	vm_map_t	map,
	vm_map_offset_t	start)
{
	vm_map_entry_t	this_entry;

	vm_map_lock(map);
	<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;this_entry)) {
		vm_map_simplify_entry(map, this_entry);
		vm_map_simplify_entry(map, this_entry-&gt;vme_next);
	}
	counter(c_vm_map_simplify_called++);
	vm_map_unlock(map);
}

<span class="enscript-type">static</span> <span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_simplify_range</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end)
{
	vm_map_entry_t	entry;

	<span class="enscript-comment">/*
	 * The map should be locked (for &quot;write&quot;) by the caller.
	 */</span>

	<span class="enscript-keyword">if</span> (start &gt;= end) {
		<span class="enscript-comment">/* invalid address range */</span>
		<span class="enscript-keyword">return</span>;
	}

	start = vm_map_trunc_page(start,
				  VM_MAP_PAGE_MASK(map));
	end = vm_map_round_page(end,
				VM_MAP_PAGE_MASK(map));

	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;entry)) {
		<span class="enscript-comment">/* &quot;start&quot; is not mapped and &quot;entry&quot; ends before &quot;start&quot; */</span>
		<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(map)) {
			<span class="enscript-comment">/* start with first entry in the map */</span>
			entry = vm_map_first_entry(map);
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-comment">/* start with next entry */</span>
			entry = entry-&gt;vme_next;
		}
	}
		
	<span class="enscript-keyword">while</span> (entry != vm_map_to_entry(map) &amp;&amp;
	       entry-&gt;vme_start &lt;= end) {
		<span class="enscript-comment">/* try and coalesce &quot;entry&quot; with its previous entry */</span>
		vm_map_simplify_entry(map, entry);
		entry = entry-&gt;vme_next;
	}
}


<span class="enscript-comment">/*
 *	Routine:	vm_map_machine_attribute
 *	Purpose:
 *		Provide machine-specific attributes to mappings,
 *		such as cachability etc. for machines that provide
 *		them.  NUMA architectures and machines with big/strange
 *		caches will use this.
 *	Note:
 *		Responsibilities for locking and checking are handled here,
 *		everything else in the pmap module. If any non-volatile
 *		information must be kept, the pmap module should handle
 *		it itself. [This assumes that attributes do not
 *		need to be inherited, which seems ok to me]
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_machine_attribute</span>(
	vm_map_t			map,
	vm_map_offset_t		start,
	vm_map_offset_t		end,
	vm_machine_attribute_t	attribute,
	vm_machine_attribute_val_t* value)		<span class="enscript-comment">/* IN/OUT */</span>
{
	kern_return_t	ret;
	vm_map_size_t sync_size;
	vm_map_entry_t entry;
	
	<span class="enscript-keyword">if</span> (start &lt; vm_map_min(map) || end &gt; vm_map_max(map))
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;

	<span class="enscript-comment">/* Figure how much memory we need to flush (in page increments) */</span>
	sync_size = end - start;

	vm_map_lock(map);
	
	<span class="enscript-keyword">if</span> (attribute != MATTR_CACHE) {	
		<span class="enscript-comment">/* If we don't have to find physical addresses, we */</span>
		<span class="enscript-comment">/* don't have to do an explicit traversal here.    */</span>
		ret = pmap_attribute(map-&gt;pmap, start, end-start,
				     attribute, value);
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> ret;
	}

	ret = KERN_SUCCESS;										<span class="enscript-comment">/* Assume it all worked */</span>

	<span class="enscript-keyword">while</span>(sync_size) {
		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;entry)) {
			vm_map_size_t	sub_size;
			<span class="enscript-keyword">if</span>((entry-&gt;vme_end - start) &gt; sync_size) {
				sub_size = sync_size;
				sync_size = 0;
			} <span class="enscript-keyword">else</span> {
				sub_size = entry-&gt;vme_end - start;
				sync_size -= sub_size;
			}
			<span class="enscript-keyword">if</span>(entry-&gt;is_sub_map) {
				vm_map_offset_t sub_start;
				vm_map_offset_t sub_end;

				sub_start = (start - entry-&gt;vme_start) 
					+ VME_OFFSET(entry);
				sub_end = sub_start + sub_size;
				vm_map_machine_attribute(
					VME_SUBMAP(entry), 
					sub_start,
					sub_end,
					attribute, value);
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-keyword">if</span> (VME_OBJECT(entry)) {
					vm_page_t		m;
					vm_object_t		object;
					vm_object_t		base_object;
					vm_object_t		last_object;
					vm_object_offset_t	offset;
					vm_object_offset_t	base_offset;
					vm_map_size_t		range;
					range = sub_size;
					offset = (start - entry-&gt;vme_start)
						+ VME_OFFSET(entry);
					base_offset = offset;
					object = VME_OBJECT(entry);
					base_object = object;
					last_object = NULL;

					vm_object_lock(object);

					<span class="enscript-keyword">while</span> (range) {
						m = vm_page_lookup(
							object, offset);

						<span class="enscript-keyword">if</span> (m &amp;&amp; !m-&gt;fictitious) {
						        ret = 
								pmap_attribute_cache_sync(
									m-&gt;phys_page, 	
									PAGE_SIZE, 
									attribute, value);
							
						} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (object-&gt;shadow) {
						        offset = offset + object-&gt;vo_shadow_offset;
							last_object = object;
							object = object-&gt;shadow;
							vm_object_lock(last_object-&gt;shadow);
							vm_object_unlock(last_object);
							<span class="enscript-keyword">continue</span>;
						}
						range -= PAGE_SIZE;

						<span class="enscript-keyword">if</span> (base_object != object) {
						        vm_object_unlock(object);
							vm_object_lock(base_object);
							object = base_object;
						}
						<span class="enscript-comment">/* Bump to the next page */</span>
						base_offset += PAGE_SIZE;
						offset = base_offset;
					}
					vm_object_unlock(object);
				}
			}
			start += sub_size;
		} <span class="enscript-keyword">else</span> {
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span> KERN_FAILURE;
		}
		
	}

	vm_map_unlock(map);

	<span class="enscript-keyword">return</span> ret;
}

<span class="enscript-comment">/*
 *	vm_map_behavior_set:
 *
 *	Sets the paging reference behavior of the specified address
 *	range in the target map.  Paging reference behavior affects
 *	how pagein operations resulting from faults on the map will be 
 *	clustered.
 */</span>
kern_return_t 
<span class="enscript-function-name">vm_map_behavior_set</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end,
	vm_behavior_t	new_behavior)
{
	<span class="enscript-type">register</span> vm_map_entry_t	entry;
	vm_map_entry_t	temp_entry;

	XPR(XPR_VM_MAP,
	    <span class="enscript-string">&quot;vm_map_behavior_set, 0x%X start 0x%X end 0x%X behavior %d&quot;</span>,
	    map, start, end, new_behavior, 0);

	<span class="enscript-keyword">if</span> (start &gt; end ||
	    start &lt; vm_map_min(map) ||
	    end &gt; vm_map_max(map)) {
		<span class="enscript-keyword">return</span> KERN_NO_SPACE;
	}

	<span class="enscript-keyword">switch</span> (new_behavior) {

	<span class="enscript-comment">/*
	 * This first block of behaviors all set a persistent state on the specified
	 * memory range.  All we have to do here is to record the desired behavior
	 * in the vm_map_entry_t's.
	 */</span>

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_DEFAULT</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_RANDOM</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_SEQUENTIAL</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_RSEQNTL</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_ZERO_WIRED_PAGES</span>:
		vm_map_lock(map);
	
		<span class="enscript-comment">/*
		 *	The entire address range must be valid for the map.
		 * 	Note that vm_map_range_check() does a 
		 *	vm_map_lookup_entry() internally and returns the
		 *	entry containing the start of the address range if
		 *	the entire range is valid.
		 */</span>
		<span class="enscript-keyword">if</span> (vm_map_range_check(map, start, end, &amp;temp_entry)) {
			entry = temp_entry;
			vm_map_clip_start(map, entry, start);
		}
		<span class="enscript-keyword">else</span> {
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}
	
		<span class="enscript-keyword">while</span> ((entry != vm_map_to_entry(map)) &amp;&amp; (entry-&gt;vme_start &lt; end)) {
			vm_map_clip_end(map, entry, end);
			<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
				assert(!entry-&gt;use_pmap);
			}
	
			<span class="enscript-keyword">if</span>( new_behavior == VM_BEHAVIOR_ZERO_WIRED_PAGES ) {
				entry-&gt;zero_wired_pages = TRUE;
			} <span class="enscript-keyword">else</span> {
				entry-&gt;behavior = new_behavior;
			}
			entry = entry-&gt;vme_next;
		}
	
		vm_map_unlock(map);
		<span class="enscript-keyword">break</span>;

	<span class="enscript-comment">/*
	 * The rest of these are different from the above in that they cause
	 * an immediate action to take place as opposed to setting a behavior that 
	 * affects future actions.
	 */</span>

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_WILLNEED</span>:
		<span class="enscript-keyword">return</span> vm_map_willneed(map, start, end);

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_DONTNEED</span>:
		<span class="enscript-keyword">return</span> vm_map_msync(map, start, end - start, VM_SYNC_DEACTIVATE | VM_SYNC_CONTIGUOUS);

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_FREE</span>:
		<span class="enscript-keyword">return</span> vm_map_msync(map, start, end - start, VM_SYNC_KILLPAGES | VM_SYNC_CONTIGUOUS);

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_REUSABLE</span>:
		<span class="enscript-keyword">return</span> vm_map_reusable_pages(map, start, end);

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_REUSE</span>:
		<span class="enscript-keyword">return</span> vm_map_reuse_pages(map, start, end);

	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_CAN_REUSE</span>:
		<span class="enscript-keyword">return</span> vm_map_can_reuse(map, start, end);

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">MACH_ASSERT</span>
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_BEHAVIOR_PAGEOUT</span>:
		<span class="enscript-keyword">return</span> vm_map_pageout(map, start, end);
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* MACH_ASSERT */</span>

	<span class="enscript-reference">default</span>:
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);
	}

	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}


<span class="enscript-comment">/*
 * Internals for madvise(MADV_WILLNEED) system call.
 *
 * The present implementation is to do a read-ahead if the mapping corresponds
 * to a mapped regular file.  If it's an anonymous mapping, then we do nothing
 * and basically ignore the &quot;advice&quot; (which we are always free to do).
 */</span>


<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_willneed</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end
)
{
	vm_map_entry_t 			entry;
	vm_object_t			object;
	memory_object_t			pager;
	<span class="enscript-type">struct</span> vm_object_fault_info	fault_info;
	kern_return_t			kr;
	vm_object_size_t		len;
	vm_object_offset_t		offset;

	<span class="enscript-comment">/*
	 * Fill in static values in fault_info.  Several fields get ignored by the code
	 * we call, but we'll fill them in anyway since uninitialized fields are bad
	 * when it comes to future backwards compatibility.
	 */</span>

	fault_info.interruptible = THREAD_UNINT;		<span class="enscript-comment">/* ignored value */</span>
	fault_info.behavior      = VM_BEHAVIOR_SEQUENTIAL;
	fault_info.no_cache      = FALSE;			<span class="enscript-comment">/* ignored value */</span>
	fault_info.stealth	 = TRUE;
	fault_info.io_sync = FALSE;
	fault_info.cs_bypass = FALSE;
	fault_info.mark_zf_absent = FALSE;
	fault_info.batch_pmap_op = FALSE;

	<span class="enscript-comment">/*
	 * The MADV_WILLNEED operation doesn't require any changes to the
	 * vm_map_entry_t's, so the read lock is sufficient.
	 */</span>

	vm_map_lock_read(map);

	<span class="enscript-comment">/*
	 * The madvise semantics require that the address range be fully
	 * allocated with no holes.  Otherwise, we're required to return
	 * an error.
	 */</span>

	<span class="enscript-keyword">if</span> (! vm_map_range_check(map, start, end, &amp;entry)) {
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}

	<span class="enscript-comment">/*
	 * Examine each vm_map_entry_t in the range.
	 */</span>
	<span class="enscript-keyword">for</span> (; entry != vm_map_to_entry(map) &amp;&amp; start &lt; end; ) {
		
		<span class="enscript-comment">/*
		 * The first time through, the start address could be anywhere
		 * within the vm_map_entry we found.  So adjust the offset to
		 * correspond.  After that, the offset will always be zero to
		 * correspond to the beginning of the current vm_map_entry.
		 */</span>
		offset = (start - entry-&gt;vme_start) + VME_OFFSET(entry);

		<span class="enscript-comment">/*
		 * Set the length so we don't go beyond the end of the
		 * map_entry or beyond the end of the range we were given.
		 * This range could span also multiple map entries all of which
		 * map different files, so make sure we only do the right amount
		 * of I/O for each object.  Note that it's possible for there
		 * to be multiple map entries all referring to the same object
		 * but with different page permissions, but it's not worth
		 * trying to optimize that case.
		 */</span>
		len = MIN(entry-&gt;vme_end - start, end - start);

		<span class="enscript-keyword">if</span> ((vm_size_t) len != len) {
			<span class="enscript-comment">/* 32-bit overflow */</span>
			len = (vm_size_t) (0 - PAGE_SIZE);
		}
		fault_info.cluster_size = (vm_size_t) len;
		fault_info.lo_offset    = offset; 
		fault_info.hi_offset    = offset + len;
		fault_info.user_tag     = VME_ALIAS(entry);
		fault_info.pmap_options = 0;
		<span class="enscript-keyword">if</span> (entry-&gt;iokit_acct ||
		    (!entry-&gt;is_sub_map &amp;&amp; !entry-&gt;use_pmap)) {
			fault_info.pmap_options |= PMAP_OPTIONS_ALT_ACCT;
		}

		<span class="enscript-comment">/*
		 * If there's no read permission to this mapping, then just
		 * skip it.
		 */</span>
		<span class="enscript-keyword">if</span> ((entry-&gt;protection &amp; VM_PROT_READ) == 0) {
			entry = entry-&gt;vme_next;
			start = entry-&gt;vme_start;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-comment">/*
		 * Find the file object backing this map entry.  If there is
		 * none, then we simply ignore the &quot;will need&quot; advice for this
		 * entry and go on to the next one.
		 */</span>
		<span class="enscript-keyword">if</span> ((object = find_vnode_object(entry)) == VM_OBJECT_NULL) {
			entry = entry-&gt;vme_next;
			start = entry-&gt;vme_start;
			<span class="enscript-keyword">continue</span>;
		}

		<span class="enscript-comment">/*
		 * The data_request() could take a long time, so let's
		 * release the map lock to avoid blocking other threads.
		 */</span>
		vm_map_unlock_read(map);

		vm_object_paging_begin(object);
		pager = object-&gt;pager;
		vm_object_unlock(object);

		<span class="enscript-comment">/*
		 * Get the data from the object asynchronously.
		 *
		 * Note that memory_object_data_request() places limits on the
		 * amount of I/O it will do.  Regardless of the len we
		 * specified, it won't do more than MAX_UPL_TRANSFER_BYTES and it
		 * silently truncates the len to that size.  This isn't
		 * necessarily bad since madvise shouldn't really be used to
		 * page in unlimited amounts of data.  Other Unix variants
		 * limit the willneed case as well.  If this turns out to be an
		 * issue for developers, then we can always adjust the policy
		 * here and still be backwards compatible since this is all
		 * just &quot;advice&quot;.
		 */</span>
		kr = memory_object_data_request(
			pager,
			offset + object-&gt;paging_offset,
			0,	<span class="enscript-comment">/* ignored */</span>
			VM_PROT_READ,
			(memory_object_fault_info_t)&amp;fault_info);

		vm_object_lock(object);
		vm_object_paging_end(object);
		vm_object_unlock(object);

		<span class="enscript-comment">/*
		 * If we couldn't do the I/O for some reason, just give up on
		 * the madvise.  We still return success to the user since
		 * madvise isn't supposed to fail when the advice can't be
		 * taken.
		 */</span>
		<span class="enscript-keyword">if</span> (kr != KERN_SUCCESS) {
			<span class="enscript-keyword">return</span> KERN_SUCCESS;
		}

		start += len;
		<span class="enscript-keyword">if</span> (start &gt;= end) {
			<span class="enscript-comment">/* done */</span>
			<span class="enscript-keyword">return</span> KERN_SUCCESS;
		}

		<span class="enscript-comment">/* look up next entry */</span>
		vm_map_lock_read(map);
		<span class="enscript-keyword">if</span> (! vm_map_lookup_entry(map, start, &amp;entry)) {
			<span class="enscript-comment">/*
			 * There's a new hole in the address range.
			 */</span>
			vm_map_unlock_read(map);
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}
	}

	vm_map_unlock_read(map);
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-type">static</span> boolean_t
<span class="enscript-function-name">vm_map_entry_is_reusable</span>(
	vm_map_entry_t entry)
{
	<span class="enscript-comment">/* Only user map entries */</span>

	vm_object_t object;

	<span class="enscript-keyword">switch</span> (VME_ALIAS(entry)) {
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MEMORY_MALLOC</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MEMORY_MALLOC_SMALL</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MEMORY_MALLOC_LARGE</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MEMORY_REALLOC</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MEMORY_MALLOC_TINY</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MEMORY_MALLOC_LARGE_REUSABLE</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_MEMORY_MALLOC_LARGE_REUSED</span>:
		<span class="enscript-comment">/*
		 * This is a malloc() memory region: check if it's still
		 * in its original state and can be re-used for more
		 * malloc() allocations.
		 */</span>
		<span class="enscript-keyword">break</span>;
	<span class="enscript-reference">default</span>:
		<span class="enscript-comment">/*
		 * Not a malloc() memory region: let the caller decide if
		 * it's re-usable.
		 */</span>
		<span class="enscript-keyword">return</span> TRUE;
	}

	<span class="enscript-keyword">if</span> (entry-&gt;is_shared ||
	    entry-&gt;is_sub_map ||
	    entry-&gt;in_transition ||
	    entry-&gt;protection != VM_PROT_DEFAULT ||
	    entry-&gt;max_protection != VM_PROT_ALL ||
	    entry-&gt;inheritance != VM_INHERIT_DEFAULT ||
	    entry-&gt;no_cache ||
	    entry-&gt;permanent ||
	    entry-&gt;superpage_size != FALSE ||
	    entry-&gt;zero_wired_pages ||
	    entry-&gt;wired_count != 0 ||
	    entry-&gt;user_wired_count != 0) {
		<span class="enscript-keyword">return</span> FALSE;
	}

	object = VME_OBJECT(entry);
	<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
		<span class="enscript-keyword">return</span> TRUE;
	}
	<span class="enscript-keyword">if</span> (
#<span class="enscript-reference">if</span> 0
		<span class="enscript-comment">/*
		 * Let's proceed even if the VM object is potentially
		 * shared.
		 * We check for this later when processing the actual
		 * VM pages, so the contents will be safe if shared.
		 * 
		 * But we can still mark this memory region as &quot;reusable&quot; to
		 * acknowledge that the caller did let us know that the memory
		 * could be re-used and should not be penalized for holding
		 * on to it.  This allows its &quot;resident size&quot; to not include
		 * the reusable range.
		 */</span>
	    object-&gt;ref_count == 1 &amp;&amp;
#<span class="enscript-reference">endif</span>
	    object-&gt;wired_page_count == 0 &amp;&amp;
	    object-&gt;copy == VM_OBJECT_NULL &amp;&amp;
	    object-&gt;shadow == VM_OBJECT_NULL &amp;&amp;
	    object-&gt;copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC &amp;&amp;
	    object-&gt;internal &amp;&amp;
	    !object-&gt;true_share &amp;&amp;
	    object-&gt;wimg_bits == VM_WIMG_USE_DEFAULT &amp;&amp;
	    !object-&gt;code_signed) {
		<span class="enscript-keyword">return</span> TRUE;
	}
	<span class="enscript-keyword">return</span> FALSE;
	    
	    
}

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_reuse_pages</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end)
{
	vm_map_entry_t 			entry;
	vm_object_t			object;
	vm_object_offset_t		start_offset, end_offset;

	<span class="enscript-comment">/*
	 * The MADV_REUSE operation doesn't require any changes to the
	 * vm_map_entry_t's, so the read lock is sufficient.
	 */</span>

	vm_map_lock_read(map);
	assert(map-&gt;pmap != kernel_pmap);	<span class="enscript-comment">/* protect alias access */</span>

	<span class="enscript-comment">/*
	 * The madvise semantics require that the address range be fully
	 * allocated with no holes.  Otherwise, we're required to return
	 * an error.
	 */</span>

	<span class="enscript-keyword">if</span> (!vm_map_range_check(map, start, end, &amp;entry)) {
		vm_map_unlock_read(map);
		vm_page_stats_reusable.reuse_pages_failure++;
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}

	<span class="enscript-comment">/*
	 * Examine each vm_map_entry_t in the range.
	 */</span>
	<span class="enscript-keyword">for</span> (; entry != vm_map_to_entry(map) &amp;&amp; entry-&gt;vme_start &lt; end;
	     entry = entry-&gt;vme_next) {
		<span class="enscript-comment">/*
		 * Sanity check on the VM map entry.
		 */</span>
		<span class="enscript-keyword">if</span> (! vm_map_entry_is_reusable(entry)) {
			vm_map_unlock_read(map);
			vm_page_stats_reusable.reuse_pages_failure++;
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}

		<span class="enscript-comment">/*
		 * The first time through, the start address could be anywhere
		 * within the vm_map_entry we found.  So adjust the offset to
		 * correspond.
		 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;vme_start &lt; start) {
			start_offset = start - entry-&gt;vme_start;
		} <span class="enscript-keyword">else</span> {
			start_offset = 0;
		}
		end_offset = MIN(end, entry-&gt;vme_end) - entry-&gt;vme_start;
		start_offset += VME_OFFSET(entry);
		end_offset += VME_OFFSET(entry);

		object = VME_OBJECT(entry);
		<span class="enscript-keyword">if</span> (object != VM_OBJECT_NULL) {
			vm_object_lock(object);
			vm_object_reuse_pages(object, start_offset, end_offset,
					      TRUE);
			vm_object_unlock(object);
		}

		<span class="enscript-keyword">if</span> (VME_ALIAS(entry) == VM_MEMORY_MALLOC_LARGE_REUSABLE) {
			<span class="enscript-comment">/*
			 * XXX
			 * We do not hold the VM map exclusively here.
			 * The &quot;alias&quot; field is not that critical, so it's
			 * safe to update it here, as long as it is the only
			 * one that can be modified while holding the VM map
			 * &quot;shared&quot;.
			 */</span>
			VME_ALIAS_SET(entry, VM_MEMORY_MALLOC_LARGE_REUSED);
		}
	}
	
	vm_map_unlock_read(map);
	vm_page_stats_reusable.reuse_pages_success++;
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}


<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_reusable_pages</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end)
{
	vm_map_entry_t 			entry;
	vm_object_t			object;
	vm_object_offset_t		start_offset, end_offset;
	vm_map_offset_t			pmap_offset;

	<span class="enscript-comment">/*
	 * The MADV_REUSABLE operation doesn't require any changes to the
	 * vm_map_entry_t's, so the read lock is sufficient.
	 */</span>

	vm_map_lock_read(map);
	assert(map-&gt;pmap != kernel_pmap);	<span class="enscript-comment">/* protect alias access */</span>

	<span class="enscript-comment">/*
	 * The madvise semantics require that the address range be fully
	 * allocated with no holes.  Otherwise, we're required to return
	 * an error.
	 */</span>

	<span class="enscript-keyword">if</span> (!vm_map_range_check(map, start, end, &amp;entry)) {
		vm_map_unlock_read(map);
		vm_page_stats_reusable.reusable_pages_failure++;
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}

	<span class="enscript-comment">/*
	 * Examine each vm_map_entry_t in the range.
	 */</span>
	<span class="enscript-keyword">for</span> (; entry != vm_map_to_entry(map) &amp;&amp; entry-&gt;vme_start &lt; end;
	     entry = entry-&gt;vme_next) {
		<span class="enscript-type">int</span> kill_pages = 0;

		<span class="enscript-comment">/*
		 * Sanity check on the VM map entry.
		 */</span>
		<span class="enscript-keyword">if</span> (! vm_map_entry_is_reusable(entry)) {
			vm_map_unlock_read(map);
			vm_page_stats_reusable.reusable_pages_failure++;
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}

		<span class="enscript-comment">/*
		 * The first time through, the start address could be anywhere
		 * within the vm_map_entry we found.  So adjust the offset to
		 * correspond.
		 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;vme_start &lt; start) {
			start_offset = start - entry-&gt;vme_start;
			pmap_offset = start;
		} <span class="enscript-keyword">else</span> {
			start_offset = 0;
			pmap_offset = entry-&gt;vme_start;
		}
		end_offset = MIN(end, entry-&gt;vme_end) - entry-&gt;vme_start;
		start_offset += VME_OFFSET(entry);
		end_offset += VME_OFFSET(entry);

		object = VME_OBJECT(entry);
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL)
			<span class="enscript-keyword">continue</span>;


		vm_object_lock(object);
		<span class="enscript-keyword">if</span> (object-&gt;ref_count == 1 &amp;&amp;
		    !object-&gt;shadow &amp;&amp;
		    <span class="enscript-comment">/*
		     * &quot;iokit_acct&quot; entries are billed for their virtual size
		     * (rather than for their resident pages only), so they
		     * wouldn't benefit from making pages reusable, and it
		     * would be hard to keep track of pages that are both
		     * &quot;iokit_acct&quot; and &quot;reusable&quot; in the pmap stats and ledgers.
		     */</span>
		    !(entry-&gt;iokit_acct ||
		      (!entry-&gt;is_sub_map &amp;&amp; !entry-&gt;use_pmap)))
			kill_pages = 1;
		<span class="enscript-keyword">else</span>
			kill_pages = -1;
		<span class="enscript-keyword">if</span> (kill_pages != -1) {
			vm_object_deactivate_pages(object,
						   start_offset,
						   end_offset - start_offset,
						   kill_pages,
						   TRUE <span class="enscript-comment">/*reusable_pages*/</span>,
						   map-&gt;pmap,
						   pmap_offset);
		} <span class="enscript-keyword">else</span> {
			vm_page_stats_reusable.reusable_pages_shared++;
		}
		vm_object_unlock(object);

		<span class="enscript-keyword">if</span> (VME_ALIAS(entry) == VM_MEMORY_MALLOC_LARGE ||
		    VME_ALIAS(entry) == VM_MEMORY_MALLOC_LARGE_REUSED) {
			<span class="enscript-comment">/*
			 * XXX
			 * We do not hold the VM map exclusively here.
			 * The &quot;alias&quot; field is not that critical, so it's
			 * safe to update it here, as long as it is the only
			 * one that can be modified while holding the VM map
			 * &quot;shared&quot;.
			 */</span>
			VME_ALIAS_SET(entry, VM_MEMORY_MALLOC_LARGE_REUSABLE);
		}
	}
	
	vm_map_unlock_read(map);
	vm_page_stats_reusable.reusable_pages_success++;
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}


<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_can_reuse</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end)
{
	vm_map_entry_t 			entry;

	<span class="enscript-comment">/*
	 * The MADV_REUSABLE operation doesn't require any changes to the
	 * vm_map_entry_t's, so the read lock is sufficient.
	 */</span>

	vm_map_lock_read(map);
	assert(map-&gt;pmap != kernel_pmap);	<span class="enscript-comment">/* protect alias access */</span>

	<span class="enscript-comment">/*
	 * The madvise semantics require that the address range be fully
	 * allocated with no holes.  Otherwise, we're required to return
	 * an error.
	 */</span>

	<span class="enscript-keyword">if</span> (!vm_map_range_check(map, start, end, &amp;entry)) {
		vm_map_unlock_read(map);
		vm_page_stats_reusable.can_reuse_failure++;
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}

	<span class="enscript-comment">/*
	 * Examine each vm_map_entry_t in the range.
	 */</span>
	<span class="enscript-keyword">for</span> (; entry != vm_map_to_entry(map) &amp;&amp; entry-&gt;vme_start &lt; end;
	     entry = entry-&gt;vme_next) {
		<span class="enscript-comment">/*
		 * Sanity check on the VM map entry.
		 */</span>
		<span class="enscript-keyword">if</span> (! vm_map_entry_is_reusable(entry)) {
			vm_map_unlock_read(map);
			vm_page_stats_reusable.can_reuse_failure++;
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}
	}
	
	vm_map_unlock_read(map);
	vm_page_stats_reusable.can_reuse_success++;
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}


#<span class="enscript-reference">if</span> <span class="enscript-variable-name">MACH_ASSERT</span>
<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_pageout</span>(
	vm_map_t	map,
	vm_map_offset_t	start,
	vm_map_offset_t	end)
{
	vm_map_entry_t 			entry;

	<span class="enscript-comment">/*
	 * The MADV_PAGEOUT operation doesn't require any changes to the
	 * vm_map_entry_t's, so the read lock is sufficient.
	 */</span>

	vm_map_lock_read(map);

	<span class="enscript-comment">/*
	 * The madvise semantics require that the address range be fully
	 * allocated with no holes.  Otherwise, we're required to return
	 * an error.
	 */</span>

	<span class="enscript-keyword">if</span> (!vm_map_range_check(map, start, end, &amp;entry)) {
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}

	<span class="enscript-comment">/*
	 * Examine each vm_map_entry_t in the range.
	 */</span>
	<span class="enscript-keyword">for</span> (; entry != vm_map_to_entry(map) &amp;&amp; entry-&gt;vme_start &lt; end;
	     entry = entry-&gt;vme_next) {
		vm_object_t	object;

		<span class="enscript-comment">/*
		 * Sanity check on the VM map entry.
		 */</span>
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			vm_map_t submap;
			vm_map_offset_t submap_start;
			vm_map_offset_t submap_end;
			vm_map_entry_t submap_entry;

			submap = VME_SUBMAP(entry);
			submap_start = VME_OFFSET(entry);
			submap_end = submap_start + (entry-&gt;vme_end - 
						     entry-&gt;vme_start);

			vm_map_lock_read(submap);

			<span class="enscript-keyword">if</span> (! vm_map_range_check(submap,
						 submap_start,
						 submap_end,
						 &amp;submap_entry)) {
				vm_map_unlock_read(submap);
				vm_map_unlock_read(map);
				<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
			}

			object = VME_OBJECT(submap_entry);
			<span class="enscript-keyword">if</span> (submap_entry-&gt;is_sub_map ||
			    object == VM_OBJECT_NULL ||
			    !object-&gt;internal) {
				vm_map_unlock_read(submap);
				<span class="enscript-keyword">continue</span>;
			}

			vm_object_pageout(object);

			vm_map_unlock_read(submap);
			submap = VM_MAP_NULL;
			submap_entry = VM_MAP_ENTRY_NULL;
			<span class="enscript-keyword">continue</span>;
		}

		object = VME_OBJECT(entry);
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map ||
		    object == VM_OBJECT_NULL ||
		    !object-&gt;internal) {
			<span class="enscript-keyword">continue</span>;
		}

		vm_object_pageout(object);
	}
	
	vm_map_unlock_read(map);
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* MACH_ASSERT */</span>


<span class="enscript-comment">/*
 *	Routine:	vm_map_entry_insert
 *
 *	Descritpion:	This routine inserts a new vm_entry in a locked map.
 */</span>
vm_map_entry_t
<span class="enscript-function-name">vm_map_entry_insert</span>(
	vm_map_t		map,
	vm_map_entry_t		insp_entry,
	vm_map_offset_t		start,
	vm_map_offset_t		end,
	vm_object_t		object,
	vm_object_offset_t	offset,
	boolean_t		needs_copy,
	boolean_t		is_shared,
	boolean_t		in_transition,
	vm_prot_t		cur_protection,
	vm_prot_t		max_protection,
	vm_behavior_t		behavior,
	vm_inherit_t		inheritance,
	<span class="enscript-type">unsigned</span>		wired_count,
	boolean_t		no_cache,
	boolean_t		permanent,
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		superpage_size,
	boolean_t		clear_map_aligned,
	boolean_t		is_submap)
{
	vm_map_entry_t	new_entry;

	assert(insp_entry != (vm_map_entry_t)0);

	new_entry = vm_map_entry_create(map, !map-&gt;hdr.entries_pageable);

	<span class="enscript-keyword">if</span> (VM_MAP_PAGE_SHIFT(map) != PAGE_SHIFT) {
		new_entry-&gt;map_aligned = TRUE;
	} <span class="enscript-keyword">else</span> {
		new_entry-&gt;map_aligned = FALSE;
	}
	<span class="enscript-keyword">if</span> (clear_map_aligned &amp;&amp;
	    (! VM_MAP_PAGE_ALIGNED(start, VM_MAP_PAGE_MASK(map)) ||
	     ! VM_MAP_PAGE_ALIGNED(end, VM_MAP_PAGE_MASK(map)))) {
		new_entry-&gt;map_aligned = FALSE;
	}

	new_entry-&gt;vme_start = start;
	new_entry-&gt;vme_end = end;
	assert(page_aligned(new_entry-&gt;vme_start));
	assert(page_aligned(new_entry-&gt;vme_end));
	<span class="enscript-keyword">if</span> (new_entry-&gt;map_aligned) {
		assert(VM_MAP_PAGE_ALIGNED(new_entry-&gt;vme_start,
					   VM_MAP_PAGE_MASK(map)));
		assert(VM_MAP_PAGE_ALIGNED(new_entry-&gt;vme_end,
					   VM_MAP_PAGE_MASK(map)));
	}
	assert(new_entry-&gt;vme_start &lt; new_entry-&gt;vme_end);

	VME_OBJECT_SET(new_entry, object);
	VME_OFFSET_SET(new_entry, offset);
	new_entry-&gt;is_shared = is_shared;
	new_entry-&gt;is_sub_map = is_submap;
	new_entry-&gt;needs_copy = needs_copy;
	new_entry-&gt;in_transition = in_transition;
	new_entry-&gt;needs_wakeup = FALSE;
	new_entry-&gt;inheritance = inheritance;
	new_entry-&gt;protection = cur_protection;
	new_entry-&gt;max_protection = max_protection;
	new_entry-&gt;behavior = behavior;
	new_entry-&gt;wired_count = wired_count;
	new_entry-&gt;user_wired_count = 0;
	<span class="enscript-keyword">if</span> (is_submap) {
		<span class="enscript-comment">/*
		 * submap: &quot;use_pmap&quot; means &quot;nested&quot;.
		 * default: false.
		 */</span>
		new_entry-&gt;use_pmap = FALSE;
	} <span class="enscript-keyword">else</span> {
		<span class="enscript-comment">/*
		 * object: &quot;use_pmap&quot; means &quot;use pmap accounting&quot; for footprint.
		 * default: true.
		 */</span>
		new_entry-&gt;use_pmap = TRUE;
	}
	VME_ALIAS_SET(new_entry, 0);
	new_entry-&gt;zero_wired_pages = FALSE;
	new_entry-&gt;no_cache = no_cache;
	new_entry-&gt;permanent = permanent;
	<span class="enscript-keyword">if</span> (superpage_size)
		new_entry-&gt;superpage_size = TRUE;
	<span class="enscript-keyword">else</span>
		new_entry-&gt;superpage_size = FALSE;
	new_entry-&gt;used_for_jit = FALSE;
	new_entry-&gt;iokit_acct = FALSE;
	new_entry-&gt;vme_resilient_codesign = FALSE;
	new_entry-&gt;vme_resilient_media = FALSE;

	<span class="enscript-comment">/*
	 *	Insert the new entry into the list.
	 */</span>

	vm_map_store_entry_link(map, insp_entry, new_entry);
	map-&gt;size += end - start;

	<span class="enscript-comment">/*
	 *	Update the free space hint and the lookup hint.
	 */</span>

	SAVE_HINT_MAP_WRITE(map, new_entry);
	<span class="enscript-keyword">return</span> new_entry;
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_remap_extract
 *
 *	Descritpion:	This routine returns a vm_entry list from a map.
 */</span>
<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_remap_extract</span>(
	vm_map_t		map,
	vm_map_offset_t		addr,
	vm_map_size_t		size,
	boolean_t		copy,
	<span class="enscript-type">struct</span> vm_map_header	*map_header,
	vm_prot_t		*cur_protection,
	vm_prot_t		*max_protection,
	<span class="enscript-comment">/* What, no behavior? */</span>
	vm_inherit_t		inheritance,
	boolean_t		pageable)
{
	kern_return_t		result;
	vm_map_size_t		mapped_size;
	vm_map_size_t		tmp_size;
	vm_map_entry_t		src_entry;     <span class="enscript-comment">/* result of last map lookup */</span>
	vm_map_entry_t		new_entry;
	vm_object_offset_t	offset;
	vm_map_offset_t		map_address;
	vm_map_offset_t		src_start;     <span class="enscript-comment">/* start of entry to map */</span>
	vm_map_offset_t		src_end;       <span class="enscript-comment">/* end of region to be mapped */</span>
	vm_object_t		object;    
	vm_map_version_t	version;
	boolean_t		src_needs_copy;
	boolean_t		new_entry_needs_copy;

	assert(map != VM_MAP_NULL);
	assert(size != 0);
	assert(size == vm_map_round_page(size, PAGE_MASK));
	assert(inheritance == VM_INHERIT_NONE ||
	       inheritance == VM_INHERIT_COPY ||
	       inheritance == VM_INHERIT_SHARE);

	<span class="enscript-comment">/*
	 *	Compute start and end of region.
	 */</span>
	src_start = vm_map_trunc_page(addr, PAGE_MASK);
	src_end = vm_map_round_page(src_start + size, PAGE_MASK);


	<span class="enscript-comment">/*
	 *	Initialize map_header.
	 */</span>
	map_header-&gt;links.next = (<span class="enscript-type">struct</span> vm_map_entry *)&amp;map_header-&gt;links;
	map_header-&gt;links.prev = (<span class="enscript-type">struct</span> vm_map_entry *)&amp;map_header-&gt;links;
	map_header-&gt;nentries = 0;
	map_header-&gt;entries_pageable = pageable;
	map_header-&gt;page_shift = PAGE_SHIFT;

	vm_map_store_init( map_header );

	*cur_protection = VM_PROT_ALL;
	*max_protection = VM_PROT_ALL;

	map_address = 0;
	mapped_size = 0;
	result = KERN_SUCCESS;

	<span class="enscript-comment">/*  
	 *	The specified source virtual space might correspond to
	 *	multiple map entries, need to loop on them.
	 */</span>
	vm_map_lock(map);
	<span class="enscript-keyword">while</span> (mapped_size != size) {
		vm_map_size_t	entry_size;

		<span class="enscript-comment">/*
		 *	Find the beginning of the region.
		 */</span> 
		<span class="enscript-keyword">if</span> (! vm_map_lookup_entry(map, src_start, &amp;src_entry)) {
			result = KERN_INVALID_ADDRESS;
			<span class="enscript-keyword">break</span>;
		}

		<span class="enscript-keyword">if</span> (src_start &lt; src_entry-&gt;vme_start ||
		    (mapped_size &amp;&amp; src_start != src_entry-&gt;vme_start)) {
			result = KERN_INVALID_ADDRESS;
			<span class="enscript-keyword">break</span>;
		}

		tmp_size = size - mapped_size;
		<span class="enscript-keyword">if</span> (src_end &gt; src_entry-&gt;vme_end)
			tmp_size -= (src_end - src_entry-&gt;vme_end);

		entry_size = (vm_map_size_t)(src_entry-&gt;vme_end -
					     src_entry-&gt;vme_start);

		<span class="enscript-keyword">if</span>(src_entry-&gt;is_sub_map) {
			vm_map_reference(VME_SUBMAP(src_entry));
			object = VM_OBJECT_NULL;
		} <span class="enscript-keyword">else</span> {
			object = VME_OBJECT(src_entry);
			<span class="enscript-keyword">if</span> (src_entry-&gt;iokit_acct) {
				<span class="enscript-comment">/*
				 * This entry uses &quot;IOKit accounting&quot;.
				 */</span>
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (object != VM_OBJECT_NULL &amp;&amp;
				   object-&gt;purgable != VM_PURGABLE_DENY) {
				<span class="enscript-comment">/*
				 * Purgeable objects have their own accounting:
				 * no pmap accounting for them.
				 */</span>
				assert(!src_entry-&gt;use_pmap);
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/*
				 * Not IOKit or purgeable:
				 * must be accounted by pmap stats.
				 */</span>
				assert(src_entry-&gt;use_pmap);
			}

			<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
				object = vm_object_allocate(entry_size);
				VME_OFFSET_SET(src_entry, 0);
				VME_OBJECT_SET(src_entry, object);
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (object-&gt;copy_strategy !=
				   MEMORY_OBJECT_COPY_SYMMETRIC) {
				<span class="enscript-comment">/*
				 *	We are already using an asymmetric
				 *	copy, and therefore we already have
				 *	the right object.
				 */</span>
				assert(!src_entry-&gt;needs_copy);
			} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (src_entry-&gt;needs_copy || object-&gt;shadowed ||
				   (object-&gt;internal &amp;&amp; !object-&gt;true_share &amp;&amp;
				    !src_entry-&gt;is_shared &amp;&amp;
				    object-&gt;vo_size &gt; entry_size)) {

				VME_OBJECT_SHADOW(src_entry, entry_size);

				<span class="enscript-keyword">if</span> (!src_entry-&gt;needs_copy &amp;&amp;
				    (src_entry-&gt;protection &amp; VM_PROT_WRITE)) {
				        vm_prot_t prot;

				        prot = src_entry-&gt;protection &amp; ~VM_PROT_WRITE;

					<span class="enscript-keyword">if</span> (override_nx(map,
							VME_ALIAS(src_entry))
					    &amp;&amp; prot)
					        prot |= VM_PROT_EXECUTE;

					<span class="enscript-keyword">if</span>(map-&gt;mapped_in_other_pmaps) {
						vm_object_pmap_protect(
							VME_OBJECT(src_entry),
							VME_OFFSET(src_entry),
							entry_size,
							PMAP_NULL,
							src_entry-&gt;vme_start,
							prot);
					} <span class="enscript-keyword">else</span> {
						pmap_protect(vm_map_pmap(map),
							     src_entry-&gt;vme_start,
							     src_entry-&gt;vme_end,
							     prot);
					}
				}

				object = VME_OBJECT(src_entry);
				src_entry-&gt;needs_copy = FALSE;
			}


			vm_object_lock(object);
			vm_object_reference_locked(object); <span class="enscript-comment">/* object ref. for new entry */</span>
			<span class="enscript-keyword">if</span> (object-&gt;copy_strategy == 
			    MEMORY_OBJECT_COPY_SYMMETRIC) {
				object-&gt;copy_strategy = 
					MEMORY_OBJECT_COPY_DELAY;
			}
			vm_object_unlock(object);
		}

		offset = (VME_OFFSET(src_entry) +
			  (src_start - src_entry-&gt;vme_start));

		new_entry = _vm_map_entry_create(map_header, !map_header-&gt;entries_pageable);
		vm_map_entry_copy(new_entry, src_entry);
		<span class="enscript-keyword">if</span> (new_entry-&gt;is_sub_map) {
			<span class="enscript-comment">/* clr address space specifics */</span>
			new_entry-&gt;use_pmap = FALSE;
		}

		new_entry-&gt;map_aligned = FALSE;

		new_entry-&gt;vme_start = map_address;
		new_entry-&gt;vme_end = map_address + tmp_size;
		assert(new_entry-&gt;vme_start &lt; new_entry-&gt;vme_end);
		new_entry-&gt;inheritance = inheritance;
		VME_OFFSET_SET(new_entry, offset);

		<span class="enscript-comment">/*
		 * The new region has to be copied now if required.
		 */</span>
	<span class="enscript-reference">RestartCopy</span>:
		<span class="enscript-keyword">if</span> (!copy) {
			<span class="enscript-comment">/*
			 * Cannot allow an entry describing a JIT
			 * region to be shared across address spaces.
			 */</span>
			<span class="enscript-keyword">if</span> (src_entry-&gt;used_for_jit == TRUE) {
				result = KERN_INVALID_ARGUMENT;
				<span class="enscript-keyword">break</span>;
			}
			src_entry-&gt;is_shared = TRUE;
			new_entry-&gt;is_shared = TRUE;
			<span class="enscript-keyword">if</span> (!(new_entry-&gt;is_sub_map)) 
				new_entry-&gt;needs_copy = FALSE;

		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (src_entry-&gt;is_sub_map) {
			<span class="enscript-comment">/* make this a COW sub_map if not already */</span>
			assert(new_entry-&gt;wired_count == 0);
			new_entry-&gt;needs_copy = TRUE;
			object = VM_OBJECT_NULL;
		} <span class="enscript-keyword">else</span> <span class="enscript-keyword">if</span> (src_entry-&gt;wired_count == 0 &amp;&amp;
			   vm_object_copy_quickly(&amp;VME_OBJECT(new_entry),
						  VME_OFFSET(new_entry),
						  (new_entry-&gt;vme_end -
						   new_entry-&gt;vme_start),
						  &amp;src_needs_copy,
						  &amp;new_entry_needs_copy)) {

			new_entry-&gt;needs_copy = new_entry_needs_copy;
			new_entry-&gt;is_shared = FALSE;

			<span class="enscript-comment">/*
			 * Handle copy_on_write semantics.
			 */</span>
			<span class="enscript-keyword">if</span> (src_needs_copy &amp;&amp; !src_entry-&gt;needs_copy) {
			        vm_prot_t prot;

				prot = src_entry-&gt;protection &amp; ~VM_PROT_WRITE;

				<span class="enscript-keyword">if</span> (override_nx(map,
						VME_ALIAS(src_entry))
				    &amp;&amp; prot)
				        prot |= VM_PROT_EXECUTE;

				vm_object_pmap_protect(object,
						       offset,
						       entry_size,
						       ((src_entry-&gt;is_shared 
							 || map-&gt;mapped_in_other_pmaps) ?
							PMAP_NULL : map-&gt;pmap),
						       src_entry-&gt;vme_start,
						       prot);

				assert(src_entry-&gt;wired_count == 0);
				src_entry-&gt;needs_copy = TRUE;
			}
			<span class="enscript-comment">/*
			 * Throw away the old object reference of the new entry.
			 */</span>
			vm_object_deallocate(object);

		} <span class="enscript-keyword">else</span> {
			new_entry-&gt;is_shared = FALSE;

			<span class="enscript-comment">/*
			 * The map can be safely unlocked since we
			 * already hold a reference on the object.
			 *
			 * Record the timestamp of the map for later
			 * verification, and unlock the map.
			 */</span>
			version.main_timestamp = map-&gt;timestamp;
			vm_map_unlock(map); 	<span class="enscript-comment">/* Increments timestamp once! */</span>

			<span class="enscript-comment">/*
			 * Perform the copy.
			 */</span>
			<span class="enscript-keyword">if</span> (src_entry-&gt;wired_count &gt; 0) {
				vm_object_lock(object);
				result = vm_object_copy_slowly(
					object,
					offset,
					entry_size,
					THREAD_UNINT,
					&amp;VME_OBJECT(new_entry));

				VME_OFFSET_SET(new_entry, 0);
				new_entry-&gt;needs_copy = FALSE;
			} <span class="enscript-keyword">else</span> {
				vm_object_offset_t new_offset;

				new_offset = VME_OFFSET(new_entry);
				result = vm_object_copy_strategically(
					object,
					offset,
					entry_size,
					&amp;VME_OBJECT(new_entry),
					&amp;new_offset,
					&amp;new_entry_needs_copy);
				<span class="enscript-keyword">if</span> (new_offset != VME_OFFSET(new_entry)) {
					VME_OFFSET_SET(new_entry, new_offset);
				}

				new_entry-&gt;needs_copy = new_entry_needs_copy;
			}

			<span class="enscript-comment">/*
			 * Throw away the old object reference of the new entry.
			 */</span>
			vm_object_deallocate(object);

			<span class="enscript-keyword">if</span> (result != KERN_SUCCESS &amp;&amp;
			    result != KERN_MEMORY_RESTART_COPY) {
				_vm_map_entry_dispose(map_header, new_entry);
				<span class="enscript-keyword">break</span>;
			}

			<span class="enscript-comment">/*
			 * Verify that the map has not substantially
			 * changed while the copy was being made.
			 */</span>

			vm_map_lock(map);
			<span class="enscript-keyword">if</span> (version.main_timestamp + 1 != map-&gt;timestamp) {
				<span class="enscript-comment">/*
				 * Simple version comparison failed.
				 *
				 * Retry the lookup and verify that the
				 * same object/offset are still present.
				 */</span>
				vm_object_deallocate(VME_OBJECT(new_entry));
				_vm_map_entry_dispose(map_header, new_entry);
				<span class="enscript-keyword">if</span> (result == KERN_MEMORY_RESTART_COPY)
					result = KERN_SUCCESS;
				<span class="enscript-keyword">continue</span>;
			}

			<span class="enscript-keyword">if</span> (result == KERN_MEMORY_RESTART_COPY) {
				vm_object_reference(object);
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">RestartCopy</span>;
			}
		}

		_vm_map_store_entry_link(map_header,
				   map_header-&gt;links.prev, new_entry);

		<span class="enscript-comment">/*Protections for submap mapping are irrelevant here*/</span>
		<span class="enscript-keyword">if</span>( !src_entry-&gt;is_sub_map ) {
			*cur_protection &amp;= src_entry-&gt;protection;
			*max_protection &amp;= src_entry-&gt;max_protection;
		}
		map_address += tmp_size;
		mapped_size += tmp_size;
		src_start += tmp_size;

	} <span class="enscript-comment">/* end while */</span>

	vm_map_unlock(map);
	<span class="enscript-keyword">if</span> (result != KERN_SUCCESS) {
		<span class="enscript-comment">/*
		 * Free all allocated elements.
		 */</span>
		<span class="enscript-keyword">for</span> (src_entry = map_header-&gt;links.next;
		     src_entry != (<span class="enscript-type">struct</span> vm_map_entry *)&amp;map_header-&gt;links;
		     src_entry = new_entry) {
			new_entry = src_entry-&gt;vme_next;
			_vm_map_store_entry_unlink(map_header, src_entry);
			<span class="enscript-keyword">if</span> (src_entry-&gt;is_sub_map) {
				vm_map_deallocate(VME_SUBMAP(src_entry));
			} <span class="enscript-keyword">else</span> {
				vm_object_deallocate(VME_OBJECT(src_entry));
			}
			_vm_map_entry_dispose(map_header, src_entry);
		}
	}
	<span class="enscript-keyword">return</span> result;
}

<span class="enscript-comment">/*
 *	Routine:	vm_remap
 *
 *			Map portion of a task's address space.
 *			Mapped region must not overlap more than
 *			one vm memory object. Protections and
 *			inheritance attributes remain the same
 *			as in the original task and are	out parameters.
 *			Source and Target task can be identical
 *			Other attributes are identical as for vm_map()
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_remap</span>(
	vm_map_t		target_map,
	vm_map_address_t	*address,
	vm_map_size_t		size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	vm_map_t		src_map,
	vm_map_offset_t		memory_address,
	boolean_t		copy,
	vm_prot_t		*cur_protection,
	vm_prot_t		*max_protection,
	vm_inherit_t		inheritance)
{
	kern_return_t		result;
	vm_map_entry_t		entry;
	vm_map_entry_t		insp_entry = VM_MAP_ENTRY_NULL;
	vm_map_entry_t		new_entry;
	<span class="enscript-type">struct</span> vm_map_header	map_header;
	vm_map_offset_t		offset_in_mapping;

	<span class="enscript-keyword">if</span> (target_map == VM_MAP_NULL)
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;

	<span class="enscript-keyword">switch</span> (inheritance) {
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_INHERIT_NONE</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_INHERIT_COPY</span>:
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_INHERIT_SHARE</span>:
		<span class="enscript-keyword">if</span> (size != 0 &amp;&amp; src_map != VM_MAP_NULL)
			<span class="enscript-keyword">break</span>;
		<span class="enscript-comment">/*FALL THRU*/</span>
	<span class="enscript-reference">default</span>:
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}

	<span class="enscript-comment">/* 
	 * If the user is requesting that we return the address of the 
	 * first byte of the data (rather than the base of the page), 
	 * then we use different rounding semantics: specifically, 
	 * we assume that (memory_address, size) describes a region
	 * all of whose pages we must cover, rather than a base to be truncated
	 * down and a size to be added to that base.  So we figure out
	 * the highest page that the requested region includes and make
	 * sure that the size will cover it.
	 * 
 	 * The key example we're worried about it is of the form:
	 *
	 * 		memory_address = 0x1ff0, size = 0x20
	 * 
	 * With the old semantics, we round down the memory_address to 0x1000 
	 * and round up the size to 0x1000, resulting in our covering *only*
	 * page 0x1000.  With the new semantics, we'd realize that the region covers
	 * 0x1ff0-0x2010, and compute a size of 0x2000.  Thus, we cover both page 
	 * 0x1000 and page 0x2000 in the region we remap.
	 */</span>
	<span class="enscript-keyword">if</span> ((flags &amp; VM_FLAGS_RETURN_DATA_ADDR) != 0) {
		offset_in_mapping = memory_address - vm_map_trunc_page(memory_address, PAGE_MASK);
		size = vm_map_round_page(memory_address + size - vm_map_trunc_page(memory_address, PAGE_MASK), PAGE_MASK);
	} <span class="enscript-keyword">else</span> {
		size = vm_map_round_page(size, PAGE_MASK);
	} 

	result = vm_map_remap_extract(src_map, memory_address,
				      size, copy, &amp;map_header,
				      cur_protection,
				      max_protection,
				      inheritance,
				      target_map-&gt;hdr.entries_pageable);

	<span class="enscript-keyword">if</span> (result != KERN_SUCCESS) {
		<span class="enscript-keyword">return</span> result;
	}

	<span class="enscript-comment">/*
	 * Allocate/check a range of free virtual address
	 * space for the target
	 */</span>
	*address = vm_map_trunc_page(*address,
				     VM_MAP_PAGE_MASK(target_map));
	vm_map_lock(target_map);
	result = vm_map_remap_range_allocate(target_map, address, size,
					     mask, flags, &amp;insp_entry);

	<span class="enscript-keyword">for</span> (entry = map_header.links.next;
	     entry != (<span class="enscript-type">struct</span> vm_map_entry *)&amp;map_header.links;
	     entry = new_entry) {
		new_entry = entry-&gt;vme_next;
		_vm_map_store_entry_unlink(&amp;map_header, entry);
		<span class="enscript-keyword">if</span> (result == KERN_SUCCESS) {
			<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_RESILIENT_CODESIGN) {
				<span class="enscript-comment">/* no codesigning -&gt; read-only access */</span>
				assert(!entry-&gt;used_for_jit);
				entry-&gt;max_protection = VM_PROT_READ;
				entry-&gt;protection = VM_PROT_READ;
				entry-&gt;vme_resilient_codesign = TRUE;
			}
			entry-&gt;vme_start += *address;
			entry-&gt;vme_end += *address;
			assert(!entry-&gt;map_aligned);
			vm_map_store_entry_link(target_map, insp_entry, entry);
			insp_entry = entry;
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> (!entry-&gt;is_sub_map) {
				vm_object_deallocate(VME_OBJECT(entry));
			} <span class="enscript-keyword">else</span> {
				vm_map_deallocate(VME_SUBMAP(entry));
			}
			_vm_map_entry_dispose(&amp;map_header, entry);
		}
	}

	<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_RESILIENT_CODESIGN) {
		*cur_protection = VM_PROT_READ;
		*max_protection = VM_PROT_READ;
	}

	<span class="enscript-keyword">if</span>( target_map-&gt;disable_vmentry_reuse == TRUE) {
		<span class="enscript-keyword">if</span>( target_map-&gt;highest_entry_end &lt; insp_entry-&gt;vme_end ){
			target_map-&gt;highest_entry_end = insp_entry-&gt;vme_end;
		}
	}

	<span class="enscript-keyword">if</span> (result == KERN_SUCCESS) {
		target_map-&gt;size += size;
		SAVE_HINT_MAP_WRITE(target_map, insp_entry);
	}
	vm_map_unlock(target_map);

	<span class="enscript-keyword">if</span> (result == KERN_SUCCESS &amp;&amp; target_map-&gt;wiring_required)
		result = vm_map_wire(target_map, *address,
				     *address + size, *cur_protection | VM_PROT_MEMORY_TAG_MAKE(VM_KERN_MEMORY_MLOCK),
				     TRUE);

	<span class="enscript-comment">/* 
	 * If requested, return the address of the data pointed to by the 
	 * request, rather than the base of the resulting page.
	 */</span>
	<span class="enscript-keyword">if</span> ((flags &amp; VM_FLAGS_RETURN_DATA_ADDR) != 0) {
		*address += offset_in_mapping;
	}

	<span class="enscript-keyword">return</span> result;
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_remap_range_allocate
 *
 *	Description:
 *		Allocate a range in the specified virtual address map.
 *		returns the address and the map entry just before the allocated
 *		range
 *
 *	Map must be locked.
 */</span>

<span class="enscript-type">static</span> kern_return_t
<span class="enscript-function-name">vm_map_remap_range_allocate</span>(
	vm_map_t		map,
	vm_map_address_t	*address,	<span class="enscript-comment">/* IN/OUT */</span>
	vm_map_size_t		size,
	vm_map_offset_t		mask,
	<span class="enscript-type">int</span>			flags,
	vm_map_entry_t		*map_entry)	<span class="enscript-comment">/* OUT */</span>
{
	vm_map_entry_t	entry;
	vm_map_offset_t	start;
	vm_map_offset_t	end;
	kern_return_t	kr;
	vm_map_entry_t		hole_entry;

<span class="enscript-reference">StartAgain</span>: ;

	start = *address;

	<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_ANYWHERE)
	{
		<span class="enscript-comment">/*
		 *	Calculate the first possible address.
		 */</span>

		<span class="enscript-keyword">if</span> (start &lt; map-&gt;min_offset)
			start = map-&gt;min_offset;
		<span class="enscript-keyword">if</span> (start &gt; map-&gt;max_offset)
			<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
		
		<span class="enscript-comment">/*
		 *	Look for the first possible address;
		 *	if there's already something at this
		 *	address, we have to start after it.
		 */</span>

		<span class="enscript-keyword">if</span>( map-&gt;disable_vmentry_reuse == TRUE) {
			VM_MAP_HIGHEST_ENTRY(map, entry, start);
		} <span class="enscript-keyword">else</span> {

			<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
				hole_entry = (vm_map_entry_t)map-&gt;holes_list;

				<span class="enscript-keyword">if</span> (hole_entry == NULL) {
					<span class="enscript-comment">/*
					 * No more space in the map?
					 */</span>
					<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
				} <span class="enscript-keyword">else</span> {

					boolean_t found_hole = FALSE;

					<span class="enscript-keyword">do</span> {
						<span class="enscript-keyword">if</span> (hole_entry-&gt;vme_start &gt;= start) {
							start = hole_entry-&gt;vme_start;
							found_hole = TRUE;
							<span class="enscript-keyword">break</span>;
						}

						<span class="enscript-keyword">if</span> (hole_entry-&gt;vme_end &gt; start) {
							found_hole = TRUE;
							<span class="enscript-keyword">break</span>;
						}
						hole_entry = hole_entry-&gt;vme_next;

					} <span class="enscript-keyword">while</span> (hole_entry != (vm_map_entry_t) map-&gt;holes_list);

					<span class="enscript-keyword">if</span> (found_hole == FALSE) {
						<span class="enscript-keyword">return</span> (KERN_NO_SPACE);
					}

					entry = hole_entry;
				}
			} <span class="enscript-keyword">else</span> {
				assert(first_free_is_valid(map));
				<span class="enscript-keyword">if</span> (start == map-&gt;min_offset) {
					<span class="enscript-keyword">if</span> ((entry = map-&gt;first_free) != vm_map_to_entry(map))
						start = entry-&gt;vme_end;
				} <span class="enscript-keyword">else</span> {
					vm_map_entry_t	tmp_entry;
					<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;tmp_entry))
						start = tmp_entry-&gt;vme_end;
					entry = tmp_entry;
				}
			}
			start = vm_map_round_page(start,
						  VM_MAP_PAGE_MASK(map));
		}
		
		<span class="enscript-comment">/*
		 *	In any case, the &quot;entry&quot; always precedes
		 *	the proposed new region throughout the
		 *	loop:
		 */</span>

		<span class="enscript-keyword">while</span> (TRUE) {
			<span class="enscript-type">register</span> vm_map_entry_t	next;

			<span class="enscript-comment">/*
			 *	Find the end of the proposed new region.
			 *	Be sure we didn't go beyond the end, or
			 *	wrap around the address.
			 */</span>

			end = ((start + mask) &amp; ~mask);
			end = vm_map_round_page(end,
						VM_MAP_PAGE_MASK(map));
			<span class="enscript-keyword">if</span> (end &lt; start)
				<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
			start = end;
			end += size;

			<span class="enscript-keyword">if</span> ((end &gt; map-&gt;max_offset) || (end &lt; start)) {
				<span class="enscript-keyword">if</span> (map-&gt;wait_for_space) {
					<span class="enscript-keyword">if</span> (size &lt;= (map-&gt;max_offset -
						     map-&gt;min_offset)) {
						assert_wait((event_t) map, THREAD_INTERRUPTIBLE);
						vm_map_unlock(map);
						thread_block(THREAD_CONTINUE_NULL);
						vm_map_lock(map);
						<span class="enscript-keyword">goto</span> <span class="enscript-reference">StartAgain</span>;
					}
				}
		
				<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
			}

			next = entry-&gt;vme_next;

			<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
				<span class="enscript-keyword">if</span> (entry-&gt;vme_end &gt;= end)
					<span class="enscript-keyword">break</span>;
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-comment">/*
			 	 *	If there are no more entries, we must win.
				 *
				 *	OR
				 *
				 *	If there is another entry, it must be
				 *	after the end of the potential new region.
				 */</span>

				<span class="enscript-keyword">if</span> (next == vm_map_to_entry(map))
					<span class="enscript-keyword">break</span>;

				<span class="enscript-keyword">if</span> (next-&gt;vme_start &gt;= end)
					<span class="enscript-keyword">break</span>;
			}

			<span class="enscript-comment">/*
			 *	Didn't fit -- move to the next entry.
			 */</span>

			entry = next;

			<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {
				<span class="enscript-keyword">if</span> (entry == (vm_map_entry_t) map-&gt;holes_list) {
					<span class="enscript-comment">/*
					 * Wrapped around
					 */</span>
					<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
				}
				start = entry-&gt;vme_start;
			} <span class="enscript-keyword">else</span> {
				start = entry-&gt;vme_end;
			}
		}

		<span class="enscript-keyword">if</span> (map-&gt;holelistenabled) {

			<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, entry-&gt;vme_start, &amp;entry)) {
				panic(<span class="enscript-string">&quot;Found an existing entry (%p) instead of potential hole at address: 0x%llx.\n&quot;</span>, entry, (<span class="enscript-type">unsigned</span> <span class="enscript-type">long</span> <span class="enscript-type">long</span>)entry-&gt;vme_start);
			}
		}

		*address = start;

	} <span class="enscript-keyword">else</span> {
		vm_map_entry_t		temp_entry;
	
		<span class="enscript-comment">/*
		 *	Verify that:
		 *		the address doesn't itself violate
		 *		the mask requirement.
		 */</span>

		<span class="enscript-keyword">if</span> ((start &amp; mask) != 0)
			<span class="enscript-keyword">return</span>(KERN_NO_SPACE);


		<span class="enscript-comment">/*
		 *	...	the address is within bounds
		 */</span>

		end = start + size;

		<span class="enscript-keyword">if</span> ((start &lt; map-&gt;min_offset) ||
		    (end &gt; map-&gt;max_offset) ||
		    (start &gt;= end)) {
			<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
		}

		<span class="enscript-comment">/*
		 * If we're asked to overwrite whatever was mapped in that
		 * range, first deallocate that range.
		 */</span>
		<span class="enscript-keyword">if</span> (flags &amp; VM_FLAGS_OVERWRITE) {
			vm_map_t zap_map;

			<span class="enscript-comment">/*
			 * We use a &quot;zap_map&quot; to avoid having to unlock
			 * the &quot;map&quot; in vm_map_delete(), which would compromise
			 * the atomicity of the &quot;deallocate&quot; and then &quot;remap&quot;
			 * combination.
			 */</span>
			zap_map = vm_map_create(PMAP_NULL,
						start,
						end,
						map-&gt;hdr.entries_pageable);
			<span class="enscript-keyword">if</span> (zap_map == VM_MAP_NULL) {
				<span class="enscript-keyword">return</span> KERN_RESOURCE_SHORTAGE;
			}
			vm_map_set_page_shift(zap_map, VM_MAP_PAGE_SHIFT(map));
			vm_map_disable_hole_optimization(zap_map);

			kr = vm_map_delete(map, start, end,
					   (VM_MAP_REMOVE_SAVE_ENTRIES |
					    VM_MAP_REMOVE_NO_MAP_ALIGN),
					   zap_map);
			<span class="enscript-keyword">if</span> (kr == KERN_SUCCESS) {
				vm_map_destroy(zap_map,
					       VM_MAP_REMOVE_NO_PMAP_CLEANUP);
				zap_map = VM_MAP_NULL;
			}
		}

		<span class="enscript-comment">/*
		 *	...	the starting address isn't allocated
		 */</span>

		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, start, &amp;temp_entry))
			<span class="enscript-keyword">return</span>(KERN_NO_SPACE);

		entry = temp_entry;

		<span class="enscript-comment">/*
		 *	...	the next region doesn't overlap the
		 *		end point.
		 */</span>

		<span class="enscript-keyword">if</span> ((entry-&gt;vme_next != vm_map_to_entry(map)) &amp;&amp;
		    (entry-&gt;vme_next-&gt;vme_start &lt; end))
			<span class="enscript-keyword">return</span>(KERN_NO_SPACE);
	}
	*map_entry = entry;
	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}

<span class="enscript-comment">/*
 *	vm_map_switch:
 *
 *	Set the address map for the current thread to the specified map
 */</span>

vm_map_t
<span class="enscript-function-name">vm_map_switch</span>(
	vm_map_t	map)
{
	<span class="enscript-type">int</span>		mycpu;
	thread_t	thread = current_thread();
	vm_map_t	oldmap = thread-&gt;map;

	mp_disable_preemption();
	mycpu = cpu_number();

	<span class="enscript-comment">/*
	 *	Deactivate the current map and activate the requested map
	 */</span>
	PMAP_SWITCH_USER(thread, map, mycpu);

	mp_enable_preemption();
	<span class="enscript-keyword">return</span>(oldmap);
}


<span class="enscript-comment">/*
 *	Routine:	vm_map_write_user
 *
 *	Description:
 *		Copy out data from a kernel space into space in the
 *		destination map. The space must already exist in the
 *		destination map.
 *		NOTE:  This routine should only be called by threads
 *		which can block on a page fault. i.e. kernel mode user
 *		threads.
 *
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_write_user</span>(
	vm_map_t		map,
	<span class="enscript-type">void</span>			*src_p,
	vm_map_address_t	dst_addr,
	vm_size_t		size)
{
	kern_return_t	kr = KERN_SUCCESS;

	<span class="enscript-keyword">if</span>(current_map() == map) {
		<span class="enscript-keyword">if</span> (copyout(src_p, dst_addr, size)) {
			kr = KERN_INVALID_ADDRESS;
		}
	} <span class="enscript-keyword">else</span> {
		vm_map_t	oldmap;

		<span class="enscript-comment">/* take on the identity of the target map while doing */</span>
		<span class="enscript-comment">/* the transfer */</span>

		vm_map_reference(map);
		oldmap = vm_map_switch(map);
		<span class="enscript-keyword">if</span> (copyout(src_p, dst_addr, size)) {
			kr = KERN_INVALID_ADDRESS;
		}
		vm_map_switch(oldmap);
		vm_map_deallocate(map);
	}
	<span class="enscript-keyword">return</span> kr;
}

<span class="enscript-comment">/*
 *	Routine:	vm_map_read_user
 *
 *	Description:
 *		Copy in data from a user space source map into the
 *		kernel map. The space must already exist in the
 *		kernel map.
 *		NOTE:  This routine should only be called by threads
 *		which can block on a page fault. i.e. kernel mode user
 *		threads.
 *
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_read_user</span>(
	vm_map_t		map,
	vm_map_address_t	src_addr,
	<span class="enscript-type">void</span>			*dst_p,
	vm_size_t		size)
{
	kern_return_t	kr = KERN_SUCCESS;

	<span class="enscript-keyword">if</span>(current_map() == map) {
		<span class="enscript-keyword">if</span> (copyin(src_addr, dst_p, size)) {
			kr = KERN_INVALID_ADDRESS;
		}
	} <span class="enscript-keyword">else</span> {
		vm_map_t	oldmap;

		<span class="enscript-comment">/* take on the identity of the target map while doing */</span>
		<span class="enscript-comment">/* the transfer */</span>

		vm_map_reference(map);
		oldmap = vm_map_switch(map);
		<span class="enscript-keyword">if</span> (copyin(src_addr, dst_p, size)) {
			kr = KERN_INVALID_ADDRESS;
		}
		vm_map_switch(oldmap);
		vm_map_deallocate(map);
	}
	<span class="enscript-keyword">return</span> kr;
}


<span class="enscript-comment">/*
 *	vm_map_check_protection:
 *
 *	Assert that the target map allows the specified
 *	privilege on the entire address region given.
 *	The entire region must be allocated.
 */</span>
boolean_t
<span class="enscript-function-name">vm_map_check_protection</span>(vm_map_t map, vm_map_offset_t start,
			vm_map_offset_t end, vm_prot_t protection)
{
	vm_map_entry_t entry;
	vm_map_entry_t tmp_entry;

	vm_map_lock(map);

	<span class="enscript-keyword">if</span> (start &lt; vm_map_min(map) || end &gt; vm_map_max(map) || start &gt; end)
	{
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> (FALSE);
	}

	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;tmp_entry)) {
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span>(FALSE);
	}

	entry = tmp_entry;

	<span class="enscript-keyword">while</span> (start &lt; end) {
		<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(map)) {
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(FALSE);
		}

		<span class="enscript-comment">/*
		 *	No holes allowed!
		 */</span>

		<span class="enscript-keyword">if</span> (start &lt; entry-&gt;vme_start) {
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(FALSE);
		}

		<span class="enscript-comment">/*
		 * Check protection associated with entry.
		 */</span>

		<span class="enscript-keyword">if</span> ((entry-&gt;protection &amp; protection) != protection) {
			vm_map_unlock(map);
			<span class="enscript-keyword">return</span>(FALSE);
		}

		<span class="enscript-comment">/* go to next entry */</span>

		start = entry-&gt;vme_end;
		entry = entry-&gt;vme_next;
	}
	vm_map_unlock(map);
	<span class="enscript-keyword">return</span>(TRUE);
}

kern_return_t
<span class="enscript-function-name">vm_map_purgable_control</span>(
	vm_map_t		map,
	vm_map_offset_t		address,
	vm_purgable_t		control,
	<span class="enscript-type">int</span>			*state)
{
	vm_map_entry_t		entry;
	vm_object_t		object;
	kern_return_t		kr;
	boolean_t		was_nonvolatile;

	<span class="enscript-comment">/*
	 * Vet all the input parameters and current type and state of the
	 * underlaying object.  Return with an error if anything is amiss.
	 */</span>
	<span class="enscript-keyword">if</span> (map == VM_MAP_NULL)
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

	<span class="enscript-keyword">if</span> (control != VM_PURGABLE_SET_STATE &amp;&amp;
	    control != VM_PURGABLE_GET_STATE &amp;&amp;
	    control != VM_PURGABLE_PURGE_ALL)
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

	<span class="enscript-keyword">if</span> (control == VM_PURGABLE_PURGE_ALL) {
		vm_purgeable_object_purge_all();
		<span class="enscript-keyword">return</span> KERN_SUCCESS;
	}

	<span class="enscript-keyword">if</span> (control == VM_PURGABLE_SET_STATE &amp;&amp;
	    (((*state &amp; ~(VM_PURGABLE_ALL_MASKS)) != 0) ||
	     ((*state &amp; VM_PURGABLE_STATE_MASK) &gt; VM_PURGABLE_STATE_MASK)))
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

	vm_map_lock_read(map);

	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, address, &amp;entry) || entry-&gt;is_sub_map) {

		<span class="enscript-comment">/*
		 * Must pass a valid non-submap address.
		 */</span>
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}

	<span class="enscript-keyword">if</span> ((entry-&gt;protection &amp; VM_PROT_WRITE) == 0) {
		<span class="enscript-comment">/*
		 * Can't apply purgable controls to something you can't write.
		 */</span>
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_PROTECTION_FAILURE);
	}

	object = VME_OBJECT(entry);
	<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL ||
	    object-&gt;purgable == VM_PURGABLE_DENY) {
		<span class="enscript-comment">/*
		 * Object must already be present and be purgeable.
		 */</span>
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}
		     
	vm_object_lock(object);

#<span class="enscript-reference">if</span> 00
	<span class="enscript-keyword">if</span> (VME_OFFSET(entry) != 0 || 
	    entry-&gt;vme_end - entry-&gt;vme_start != object-&gt;vo_size) {
		<span class="enscript-comment">/*
		 * Can only apply purgable controls to the whole (existing)
		 * object at once.
		 */</span>
		vm_map_unlock_read(map);
		vm_object_unlock(object);
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}
#<span class="enscript-reference">endif</span>

	assert(!entry-&gt;is_sub_map);
	assert(!entry-&gt;use_pmap); <span class="enscript-comment">/* purgeable has its own accounting */</span>

	vm_map_unlock_read(map);

	was_nonvolatile = (object-&gt;purgable == VM_PURGABLE_NONVOLATILE);

	kr = vm_object_purgable_control(object, control, state);

	<span class="enscript-keyword">if</span> (was_nonvolatile &amp;&amp;
	    object-&gt;purgable != VM_PURGABLE_NONVOLATILE &amp;&amp;
	    map-&gt;pmap == kernel_pmap) {
#<span class="enscript-reference">if</span> <span class="enscript-variable-name">DEBUG</span>
		object-&gt;vo_purgeable_volatilizer = kernel_task;
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* DEBUG */</span>
	}

	vm_object_unlock(object);

	<span class="enscript-keyword">return</span> kr;
}

kern_return_t
<span class="enscript-function-name">vm_map_page_query_internal</span>(
	vm_map_t	target_map,
	vm_map_offset_t	offset,
	<span class="enscript-type">int</span>		*disposition,
	<span class="enscript-type">int</span>		*ref_count)
{
	kern_return_t			kr;
	vm_page_info_basic_data_t	info;
	mach_msg_type_number_t		count;

	count = VM_PAGE_INFO_BASIC_COUNT;
	kr = vm_map_page_info(target_map,
			      offset,
			      VM_PAGE_INFO_BASIC,
			      (vm_page_info_t) &amp;info,
			      &amp;count);
	<span class="enscript-keyword">if</span> (kr == KERN_SUCCESS) {
		*disposition = info.disposition;
		*ref_count = info.ref_count;
	} <span class="enscript-keyword">else</span> {
		*disposition = 0;
		*ref_count = 0;
	}

	<span class="enscript-keyword">return</span> kr;
}
		
kern_return_t
<span class="enscript-function-name">vm_map_page_info</span>(
	vm_map_t		map,
	vm_map_offset_t		offset,
	vm_page_info_flavor_t	flavor,
	vm_page_info_t		info,
	mach_msg_type_number_t	*count)
{
	vm_map_entry_t		map_entry;
	vm_object_t		object;
	vm_page_t		m;
	kern_return_t		kr;
	kern_return_t		retval = KERN_SUCCESS;
	boolean_t		top_object;
	<span class="enscript-type">int</span>			disposition;
	<span class="enscript-type">int</span> 			ref_count;
	vm_page_info_basic_t	basic_info;
	<span class="enscript-type">int</span>			depth;
	vm_map_offset_t		offset_in_page;

	<span class="enscript-keyword">switch</span> (flavor) {
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_PAGE_INFO_BASIC</span>:
		<span class="enscript-keyword">if</span> (*count != VM_PAGE_INFO_BASIC_COUNT) {
			<span class="enscript-comment">/*
			 * The &quot;vm_page_info_basic_data&quot; structure was not
			 * properly padded, so allow the size to be off by
			 * one to maintain backwards binary compatibility...
			 */</span>
			<span class="enscript-keyword">if</span> (*count != VM_PAGE_INFO_BASIC_COUNT - 1)
				<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
		}
		<span class="enscript-keyword">break</span>;
	<span class="enscript-reference">default</span>:
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}

	disposition = 0;
	ref_count = 0;
	top_object = TRUE;
	depth = 0;

	retval = KERN_SUCCESS;
	offset_in_page = offset &amp; PAGE_MASK;
	offset = vm_map_trunc_page(offset, PAGE_MASK);

	vm_map_lock_read(map);

	<span class="enscript-comment">/*
	 * First, find the map entry covering &quot;offset&quot;, going down
	 * submaps if necessary.
	 */</span>
	<span class="enscript-keyword">for</span> (;;) {
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, offset, &amp;map_entry)) {
			vm_map_unlock_read(map);
			<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
		}
		<span class="enscript-comment">/* compute offset from this map entry's start */</span>
		offset -= map_entry-&gt;vme_start;
		<span class="enscript-comment">/* compute offset into this map entry's object (or submap) */</span>
		offset += VME_OFFSET(map_entry);

		<span class="enscript-keyword">if</span> (map_entry-&gt;is_sub_map) {
			vm_map_t sub_map;

			sub_map = VME_SUBMAP(map_entry);
			vm_map_lock_read(sub_map);
			vm_map_unlock_read(map);

			map = sub_map;

			ref_count = MAX(ref_count, map-&gt;ref_count);
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-keyword">break</span>;
	}

	object = VME_OBJECT(map_entry);
	<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
		<span class="enscript-comment">/* no object -&gt; no page */</span>
		vm_map_unlock_read(map);
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
	}

	vm_object_lock(object);
	vm_map_unlock_read(map);

	<span class="enscript-comment">/*
	 * Go down the VM object shadow chain until we find the page
	 * we're looking for.
	 */</span>
	<span class="enscript-keyword">for</span> (;;) {
		ref_count = MAX(ref_count, object-&gt;ref_count);

		m = vm_page_lookup(object, offset);

		<span class="enscript-keyword">if</span> (m != VM_PAGE_NULL) {
			disposition |= VM_PAGE_QUERY_PAGE_PRESENT;
			<span class="enscript-keyword">break</span>;
		} <span class="enscript-keyword">else</span> {
#<span class="enscript-reference">if</span> <span class="enscript-variable-name">MACH_PAGEMAP</span>
			<span class="enscript-keyword">if</span> (object-&gt;existence_map) {
				<span class="enscript-keyword">if</span> (vm_external_state_get(object-&gt;existence_map,
							  offset) ==
				    VM_EXTERNAL_STATE_EXISTS) {
					<span class="enscript-comment">/*
					 * this page has been paged out
					 */</span>
				        disposition |= VM_PAGE_QUERY_PAGE_PAGED_OUT;
					<span class="enscript-keyword">break</span>;
				}
			} <span class="enscript-keyword">else</span>
#<span class="enscript-reference">endif</span>
			<span class="enscript-keyword">if</span> (object-&gt;internal &amp;&amp;
			    object-&gt;alive &amp;&amp;
			    !object-&gt;terminating &amp;&amp;
			    object-&gt;pager_ready) {

				<span class="enscript-keyword">if</span> (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
					<span class="enscript-keyword">if</span> (VM_COMPRESSOR_PAGER_STATE_GET(
						    object,
						    offset)
					    == VM_EXTERNAL_STATE_EXISTS) {
						<span class="enscript-comment">/* the pager has that page */</span>
						disposition |= VM_PAGE_QUERY_PAGE_PAGED_OUT;
						<span class="enscript-keyword">break</span>;
					}
				} <span class="enscript-keyword">else</span> {
					memory_object_t pager;

					vm_object_paging_begin(object);
					pager = object-&gt;pager;
					vm_object_unlock(object);

					<span class="enscript-comment">/*
					 * Ask the default pager if
					 * it has this page.
					 */</span>
					kr = memory_object_data_request(
						pager,
						offset + object-&gt;paging_offset,
						0, <span class="enscript-comment">/* just poke the pager */</span>
						VM_PROT_READ,
						NULL);

					vm_object_lock(object);
					vm_object_paging_end(object);

					<span class="enscript-keyword">if</span> (kr == KERN_SUCCESS) {
						<span class="enscript-comment">/* the default pager has it */</span>
						disposition |= VM_PAGE_QUERY_PAGE_PAGED_OUT;
						<span class="enscript-keyword">break</span>;
					}
				}
			}

			<span class="enscript-keyword">if</span> (object-&gt;shadow != VM_OBJECT_NULL) {
			        vm_object_t shadow;

				offset += object-&gt;vo_shadow_offset;
				shadow = object-&gt;shadow;
				
				vm_object_lock(shadow);
				vm_object_unlock(object);

				object = shadow;
				top_object = FALSE;
				depth++;
			} <span class="enscript-keyword">else</span> {
<span class="enscript-comment">//			        if (!object-&gt;internal)
</span><span class="enscript-comment">//				        break;
</span><span class="enscript-comment">//				retval = KERN_FAILURE;
</span><span class="enscript-comment">//				goto done_with_object;
</span>				<span class="enscript-keyword">break</span>;
			}
		}
	}
	<span class="enscript-comment">/* The ref_count is not strictly accurate, it measures the number   */</span>
	<span class="enscript-comment">/* of entities holding a ref on the object, they may not be mapping */</span>
	<span class="enscript-comment">/* the object or may not be mapping the section holding the         */</span>
	<span class="enscript-comment">/* target page but its still a ball park number and though an over- */</span>
	<span class="enscript-comment">/* count, it picks up the copy-on-write cases                       */</span>

	<span class="enscript-comment">/* We could also get a picture of page sharing from pmap_attributes */</span>
	<span class="enscript-comment">/* but this would under count as only faulted-in mappings would     */</span>
	<span class="enscript-comment">/* show up.							    */</span>

	<span class="enscript-keyword">if</span> (top_object == TRUE &amp;&amp; object-&gt;shadow)
		disposition |= VM_PAGE_QUERY_PAGE_COPIED;

	<span class="enscript-keyword">if</span> (! object-&gt;internal)
		disposition |= VM_PAGE_QUERY_PAGE_EXTERNAL;

	<span class="enscript-keyword">if</span> (m == VM_PAGE_NULL)
	        <span class="enscript-keyword">goto</span> <span class="enscript-reference">done_with_object</span>;

	<span class="enscript-keyword">if</span> (m-&gt;fictitious) {
		disposition |= VM_PAGE_QUERY_PAGE_FICTITIOUS;
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">done_with_object</span>;
	}
	<span class="enscript-keyword">if</span> (m-&gt;dirty || pmap_is_modified(m-&gt;phys_page))
		disposition |= VM_PAGE_QUERY_PAGE_DIRTY;

	<span class="enscript-keyword">if</span> (m-&gt;reference || pmap_is_referenced(m-&gt;phys_page))
		disposition |= VM_PAGE_QUERY_PAGE_REF;

	<span class="enscript-keyword">if</span> (m-&gt;speculative)
		disposition |= VM_PAGE_QUERY_PAGE_SPECULATIVE;

	<span class="enscript-keyword">if</span> (m-&gt;cs_validated)
		disposition |= VM_PAGE_QUERY_PAGE_CS_VALIDATED;
	<span class="enscript-keyword">if</span> (m-&gt;cs_tainted)
		disposition |= VM_PAGE_QUERY_PAGE_CS_TAINTED;
	<span class="enscript-keyword">if</span> (m-&gt;cs_nx)
		disposition |= VM_PAGE_QUERY_PAGE_CS_NX;

<span class="enscript-reference">done_with_object</span>:
	vm_object_unlock(object);
<span class="enscript-reference">done</span>:

	<span class="enscript-keyword">switch</span> (flavor) {
	<span class="enscript-keyword">case</span> <span class="enscript-reference">VM_PAGE_INFO_BASIC</span>:
		basic_info = (vm_page_info_basic_t) info;
		basic_info-&gt;disposition = disposition;
		basic_info-&gt;ref_count = ref_count;
		basic_info-&gt;object_id = (vm_object_id_t) (uintptr_t)
			VM_KERNEL_ADDRPERM(object);
		basic_info-&gt;offset =
			(memory_object_offset_t) offset + offset_in_page;
		basic_info-&gt;depth = depth;
		<span class="enscript-keyword">break</span>;
	}

	<span class="enscript-keyword">return</span> retval;
}

<span class="enscript-comment">/*
 *	vm_map_msync
 *
 *	Synchronises the memory range specified with its backing store
 *	image by either flushing or cleaning the contents to the appropriate
 *	memory manager engaging in a memory object synchronize dialog with
 *	the manager.  The client doesn't return until the manager issues
 *	m_o_s_completed message.  MIG Magically converts user task parameter
 *	to the task's address map.
 *
 *	interpretation of sync_flags
 *	VM_SYNC_INVALIDATE	- discard pages, only return precious
 *				  pages to manager.
 *
 *	VM_SYNC_INVALIDATE &amp; (VM_SYNC_SYNCHRONOUS | VM_SYNC_ASYNCHRONOUS)
 *				- discard pages, write dirty or precious
 *				  pages back to memory manager.
 *
 *	VM_SYNC_SYNCHRONOUS | VM_SYNC_ASYNCHRONOUS
 *				- write dirty or precious pages back to
 *				  the memory manager.
 *
 *	VM_SYNC_CONTIGUOUS	- does everything normally, but if there
 *				  is a hole in the region, and we would
 *				  have returned KERN_SUCCESS, return
 *				  KERN_INVALID_ADDRESS instead.
 *
 *	NOTE
 *	The memory object attributes have not yet been implemented, this
 *	function will have to deal with the invalidate attribute
 *
 *	RETURNS
 *	KERN_INVALID_TASK		Bad task parameter
 *	KERN_INVALID_ARGUMENT		both sync and async were specified.
 *	KERN_SUCCESS			The usual.
 *	KERN_INVALID_ADDRESS		There was a hole in the region.
 */</span>

kern_return_t
<span class="enscript-function-name">vm_map_msync</span>(
	vm_map_t		map,
	vm_map_address_t	address,
	vm_map_size_t		size,
	vm_sync_t		sync_flags)
{
	msync_req_t		msr;
	msync_req_t		new_msr;
	queue_chain_t		req_q;	<span class="enscript-comment">/* queue of requests for this msync */</span>
	vm_map_entry_t		entry;
	vm_map_size_t		amount_left;
	vm_object_offset_t	offset;
	boolean_t		do_sync_req;
	boolean_t		had_hole = FALSE;
	memory_object_t		pager;
	vm_map_offset_t		pmap_offset;
	
	<span class="enscript-keyword">if</span> ((sync_flags &amp; VM_SYNC_ASYNCHRONOUS) &amp;&amp;
	    (sync_flags &amp; VM_SYNC_SYNCHRONOUS))
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);

	<span class="enscript-comment">/*
	 * align address and size on page boundaries
	 */</span>
	size = (vm_map_round_page(address + size,
				  VM_MAP_PAGE_MASK(map)) -
		vm_map_trunc_page(address,
				  VM_MAP_PAGE_MASK(map)));
	address = vm_map_trunc_page(address,
				    VM_MAP_PAGE_MASK(map));

        <span class="enscript-keyword">if</span> (map == VM_MAP_NULL)
                <span class="enscript-keyword">return</span>(KERN_INVALID_TASK);

	<span class="enscript-keyword">if</span> (size == 0)
		<span class="enscript-keyword">return</span>(KERN_SUCCESS);

	queue_init(&amp;req_q);
	amount_left = size;

	<span class="enscript-keyword">while</span> (amount_left &gt; 0) {
		vm_object_size_t	flush_size;
		vm_object_t		object;

		vm_map_lock(map);
		<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map,
					 address,
					 &amp;entry)) {

			vm_map_size_t	skip;

			<span class="enscript-comment">/*
			 * hole in the address map.
			 */</span>
			had_hole = TRUE;

			<span class="enscript-comment">/*
			 * Check for empty map.
			 */</span>
			<span class="enscript-keyword">if</span> (entry == vm_map_to_entry(map) &amp;&amp;
			    entry-&gt;vme_next == entry) {
				vm_map_unlock(map);
				<span class="enscript-keyword">break</span>;
			}
			<span class="enscript-comment">/*
			 * Check that we don't wrap and that
			 * we have at least one real map entry.
			 */</span>
			<span class="enscript-keyword">if</span> ((map-&gt;hdr.nentries == 0) ||
			    (entry-&gt;vme_next-&gt;vme_start &lt; address)) {
				vm_map_unlock(map);
				<span class="enscript-keyword">break</span>;
			}
			<span class="enscript-comment">/*
			 * Move up to the next entry if needed
			 */</span>
			skip = (entry-&gt;vme_next-&gt;vme_start - address);
			<span class="enscript-keyword">if</span> (skip &gt;= amount_left)
				amount_left = 0;
			<span class="enscript-keyword">else</span>
				amount_left -= skip;
			address = entry-&gt;vme_next-&gt;vme_start;
			vm_map_unlock(map);
			<span class="enscript-keyword">continue</span>;
		}

		offset = address - entry-&gt;vme_start;
		pmap_offset = address;

		<span class="enscript-comment">/*
		 * do we have more to flush than is contained in this
		 * entry ?
		 */</span>
		<span class="enscript-keyword">if</span> (amount_left + entry-&gt;vme_start + offset &gt; entry-&gt;vme_end) {
			flush_size = entry-&gt;vme_end -
				(entry-&gt;vme_start + offset);
		} <span class="enscript-keyword">else</span> {
			flush_size = amount_left;
		}
		amount_left -= flush_size;
		address += flush_size;

		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map == TRUE) {
			vm_map_t	local_map;
			vm_map_offset_t	local_offset;

			local_map = VME_SUBMAP(entry);
			local_offset = VME_OFFSET(entry);
			vm_map_unlock(map);
			<span class="enscript-keyword">if</span> (vm_map_msync(
				    local_map,
				    local_offset,
				    flush_size,
				    sync_flags) == KERN_INVALID_ADDRESS) {
				had_hole = TRUE;
			}
			<span class="enscript-keyword">continue</span>;
		}
		object = VME_OBJECT(entry);

		<span class="enscript-comment">/*
		 * We can't sync this object if the object has not been
		 * created yet
		 */</span>
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
			vm_map_unlock(map);
			<span class="enscript-keyword">continue</span>;
		}
		offset += VME_OFFSET(entry);

                vm_object_lock(object);

		<span class="enscript-keyword">if</span> (sync_flags &amp; (VM_SYNC_KILLPAGES | VM_SYNC_DEACTIVATE)) {
		        <span class="enscript-type">int</span> kill_pages = 0;
			boolean_t reusable_pages = FALSE;

			<span class="enscript-keyword">if</span> (sync_flags &amp; VM_SYNC_KILLPAGES) {
			        <span class="enscript-keyword">if</span> (object-&gt;ref_count == 1 &amp;&amp; !object-&gt;shadow)
				        kill_pages = 1;
				<span class="enscript-keyword">else</span>
				        kill_pages = -1;
			}
			<span class="enscript-keyword">if</span> (kill_pages != -1)
			        vm_object_deactivate_pages(
					object,
					offset,
					(vm_object_size_t) flush_size,
					kill_pages,
					reusable_pages,
					map-&gt;pmap,
					pmap_offset);
			vm_object_unlock(object);
			vm_map_unlock(map);
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-comment">/*
		 * We can't sync this object if there isn't a pager.
		 * Don't bother to sync internal objects, since there can't
		 * be any &quot;permanent&quot; storage for these objects anyway.
		 */</span>
		<span class="enscript-keyword">if</span> ((object-&gt;pager == MEMORY_OBJECT_NULL) ||
		    (object-&gt;internal) || (object-&gt;private)) {
			vm_object_unlock(object);
			vm_map_unlock(map);
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-comment">/*
		 * keep reference on the object until syncing is done
		 */</span>
		vm_object_reference_locked(object);
		vm_object_unlock(object);

		vm_map_unlock(map);

		do_sync_req = vm_object_sync(object,
					     offset,
					     flush_size,
					     sync_flags &amp; VM_SYNC_INVALIDATE,
					     ((sync_flags &amp; VM_SYNC_SYNCHRONOUS) ||
					      (sync_flags &amp; VM_SYNC_ASYNCHRONOUS)),
					     sync_flags &amp; VM_SYNC_SYNCHRONOUS);
		<span class="enscript-comment">/*
		 * only send a m_o_s if we returned pages or if the entry
		 * is writable (ie dirty pages may have already been sent back)
		 */</span>
		<span class="enscript-keyword">if</span> (!do_sync_req) {
			<span class="enscript-keyword">if</span> ((sync_flags &amp; VM_SYNC_INVALIDATE) &amp;&amp; object-&gt;resident_page_count == 0) {
				<span class="enscript-comment">/*
				 * clear out the clustering and read-ahead hints
				 */</span>
				vm_object_lock(object);

				object-&gt;pages_created = 0;
				object-&gt;pages_used = 0;
				object-&gt;sequential = 0;
				object-&gt;last_alloc = 0;

				vm_object_unlock(object);
			}
			vm_object_deallocate(object);
			<span class="enscript-keyword">continue</span>;
		}
		msync_req_alloc(new_msr);

                vm_object_lock(object);
		offset += object-&gt;paging_offset;

		new_msr-&gt;offset = offset;
		new_msr-&gt;length = flush_size;
		new_msr-&gt;object = object;
		new_msr-&gt;flag = VM_MSYNC_SYNCHRONIZING;
	<span class="enscript-reference">re_iterate</span>:

		<span class="enscript-comment">/*
		 * We can't sync this object if there isn't a pager.  The
		 * pager can disappear anytime we're not holding the object
		 * lock.  So this has to be checked anytime we goto re_iterate.
		 */</span>

		pager = object-&gt;pager;

		<span class="enscript-keyword">if</span> (pager == MEMORY_OBJECT_NULL) {
			vm_object_unlock(object);
			vm_object_deallocate(object);
			msync_req_free(new_msr);
			new_msr = NULL;
			<span class="enscript-keyword">continue</span>;
		}

		queue_iterate(&amp;object-&gt;msr_q, msr, msync_req_t, msr_q) {
			<span class="enscript-comment">/*
			 * need to check for overlapping entry, if found, wait
			 * on overlapping msr to be done, then reiterate
			 */</span>
			msr_lock(msr);
			<span class="enscript-keyword">if</span> (msr-&gt;flag == VM_MSYNC_SYNCHRONIZING &amp;&amp;
			    ((offset &gt;= msr-&gt;offset &amp;&amp; 
			      offset &lt; (msr-&gt;offset + msr-&gt;length)) ||
			     (msr-&gt;offset &gt;= offset &amp;&amp;
			      msr-&gt;offset &lt; (offset + flush_size))))
			{
				assert_wait((event_t) msr,THREAD_INTERRUPTIBLE);
				msr_unlock(msr);
				vm_object_unlock(object);
				thread_block(THREAD_CONTINUE_NULL);
				vm_object_lock(object);
				<span class="enscript-keyword">goto</span> <span class="enscript-reference">re_iterate</span>;
			}
			msr_unlock(msr);
		}<span class="enscript-comment">/* queue_iterate */</span>

		queue_enter(&amp;object-&gt;msr_q, new_msr, msync_req_t, msr_q);

		vm_object_paging_begin(object);
		vm_object_unlock(object);

		queue_enter(&amp;req_q, new_msr, msync_req_t, req_q);

		(<span class="enscript-type">void</span>) memory_object_synchronize(
			pager,
			offset,
			flush_size,
			sync_flags &amp; ~VM_SYNC_CONTIGUOUS);

		vm_object_lock(object);
		vm_object_paging_end(object);
		vm_object_unlock(object);
	}<span class="enscript-comment">/* while */</span>

	<span class="enscript-comment">/*
	 * wait for memory_object_sychronize_completed messages from pager(s)
	 */</span>

	<span class="enscript-keyword">while</span> (!queue_empty(&amp;req_q)) {
		msr = (msync_req_t)queue_first(&amp;req_q);
		msr_lock(msr);
		<span class="enscript-keyword">while</span>(msr-&gt;flag != VM_MSYNC_DONE) {
			assert_wait((event_t) msr, THREAD_INTERRUPTIBLE);
			msr_unlock(msr);
			thread_block(THREAD_CONTINUE_NULL);
			msr_lock(msr);
		}<span class="enscript-comment">/* while */</span>
		queue_remove(&amp;req_q, msr, msync_req_t, req_q);
		msr_unlock(msr);
		vm_object_deallocate(msr-&gt;object);
		msync_req_free(msr);
	}<span class="enscript-comment">/* queue_iterate */</span>

	<span class="enscript-comment">/* for proper msync() behaviour */</span>
	<span class="enscript-keyword">if</span> (had_hole == TRUE &amp;&amp; (sync_flags &amp; VM_SYNC_CONTIGUOUS))
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);

	<span class="enscript-keyword">return</span>(KERN_SUCCESS);
}<span class="enscript-comment">/* vm_msync */</span>

<span class="enscript-comment">/*
 *	Routine:	convert_port_entry_to_map
 *	Purpose:
 *		Convert from a port specifying an entry or a task
 *		to a map. Doesn't consume the port ref; produces a map ref,
 *		which may be null.  Unlike convert_port_to_map, the
 *		port may be task or a named entry backed.
 *	Conditions:
 *		Nothing locked.
 */</span>


vm_map_t
<span class="enscript-function-name">convert_port_entry_to_map</span>(
	ipc_port_t	port)
{
	vm_map_t map;
	vm_named_entry_t	named_entry;
	uint32_t	try_failed_count = 0;

	<span class="enscript-keyword">if</span>(IP_VALID(port) &amp;&amp; (ip_kotype(port) == IKOT_NAMED_ENTRY)) {
		<span class="enscript-keyword">while</span>(TRUE) {
			ip_lock(port);
			<span class="enscript-keyword">if</span>(ip_active(port) &amp;&amp; (ip_kotype(port) 
					       == IKOT_NAMED_ENTRY)) {
				named_entry =
					(vm_named_entry_t)port-&gt;ip_kobject;
				<span class="enscript-keyword">if</span> (!(lck_mtx_try_lock(&amp;(named_entry)-&gt;Lock))) {
                       			ip_unlock(port);

					try_failed_count++;
                       			mutex_pause(try_failed_count);
                       			<span class="enscript-keyword">continue</span>;
                		}
				named_entry-&gt;ref_count++;
				lck_mtx_unlock(&amp;(named_entry)-&gt;Lock);
				ip_unlock(port);
				<span class="enscript-keyword">if</span> ((named_entry-&gt;is_sub_map) &amp;&amp;
				    (named_entry-&gt;protection 
				     &amp; VM_PROT_WRITE)) {
					map = named_entry-&gt;backing.map;
				} <span class="enscript-keyword">else</span> {
					mach_destroy_memory_entry(port);
					<span class="enscript-keyword">return</span> VM_MAP_NULL;
				}
				vm_map_reference_swap(map);
				mach_destroy_memory_entry(port);
				<span class="enscript-keyword">break</span>;
			}
			<span class="enscript-keyword">else</span> 
				<span class="enscript-keyword">return</span> VM_MAP_NULL;
		}
	}
	<span class="enscript-keyword">else</span>
		map = convert_port_to_map(port);

	<span class="enscript-keyword">return</span> map;
}

<span class="enscript-comment">/*
 *	Routine:	convert_port_entry_to_object
 *	Purpose:
 *		Convert from a port specifying a named entry to an
 *		object. Doesn't consume the port ref; produces a map ref,
 *		which may be null. 
 *	Conditions:
 *		Nothing locked.
 */</span>


vm_object_t
<span class="enscript-function-name">convert_port_entry_to_object</span>(
	ipc_port_t	port)
{
	vm_object_t		object = VM_OBJECT_NULL;
	vm_named_entry_t	named_entry;
	uint32_t		try_failed_count = 0;

	<span class="enscript-keyword">if</span> (IP_VALID(port) &amp;&amp;
	    (ip_kotype(port) == IKOT_NAMED_ENTRY)) {
	<span class="enscript-reference">try_again</span>:
		ip_lock(port);
		<span class="enscript-keyword">if</span> (ip_active(port) &amp;&amp;
		    (ip_kotype(port) == IKOT_NAMED_ENTRY)) {
			named_entry = (vm_named_entry_t)port-&gt;ip_kobject;
			<span class="enscript-keyword">if</span> (!(lck_mtx_try_lock(&amp;(named_entry)-&gt;Lock))) {
				ip_unlock(port);
				try_failed_count++;
				mutex_pause(try_failed_count);
                       		<span class="enscript-keyword">goto</span> <span class="enscript-reference">try_again</span>;
			}
			named_entry-&gt;ref_count++;
			lck_mtx_unlock(&amp;(named_entry)-&gt;Lock);
			ip_unlock(port);
			<span class="enscript-keyword">if</span> (!(named_entry-&gt;is_sub_map) &amp;&amp;
			    !(named_entry-&gt;is_pager) &amp;&amp;
			    !(named_entry-&gt;is_copy) &amp;&amp;
			    (named_entry-&gt;protection &amp; VM_PROT_WRITE)) {
				object = named_entry-&gt;backing.object;
				vm_object_reference(object);
			}
			mach_destroy_memory_entry(port);
		}
	}

	<span class="enscript-keyword">return</span> object;
}

<span class="enscript-comment">/*
 * Export routines to other components for the things we access locally through
 * macros.
 */</span>
#<span class="enscript-reference">undef</span> <span class="enscript-variable-name">current_map</span>
vm_map_t
<span class="enscript-function-name">current_map</span>(<span class="enscript-type">void</span>)
{
	<span class="enscript-keyword">return</span> (current_map_fast());
}

<span class="enscript-comment">/*
 *	vm_map_reference:
 *
 *	Most code internal to the osfmk will go through a
 *	macro defining this.  This is always here for the
 *	use of other kernel components.
 */</span>
#<span class="enscript-reference">undef</span> <span class="enscript-variable-name">vm_map_reference</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_reference</span>(
	<span class="enscript-type">register</span> vm_map_t	map)
{
	<span class="enscript-keyword">if</span> (map == VM_MAP_NULL)
		<span class="enscript-keyword">return</span>;

	lck_mtx_lock(&amp;map-&gt;s_lock);
#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">TASK_SWAPPER</span>
	assert(map-&gt;res_count &gt; 0);
	assert(map-&gt;ref_count &gt;= map-&gt;res_count);
	map-&gt;res_count++;
#<span class="enscript-reference">endif</span>
	map-&gt;ref_count++;
	lck_mtx_unlock(&amp;map-&gt;s_lock);
}

<span class="enscript-comment">/*
 *	vm_map_deallocate:
 *
 *	Removes a reference from the specified map,
 *	destroying it if no references remain.
 *	The map should not be locked.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_deallocate</span>(
	<span class="enscript-type">register</span> vm_map_t	map)
{
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>		ref;

	<span class="enscript-keyword">if</span> (map == VM_MAP_NULL)
		<span class="enscript-keyword">return</span>;

	lck_mtx_lock(&amp;map-&gt;s_lock);
	ref = --map-&gt;ref_count;
	<span class="enscript-keyword">if</span> (ref &gt; 0) {
		vm_map_res_deallocate(map);
		lck_mtx_unlock(&amp;map-&gt;s_lock);
		<span class="enscript-keyword">return</span>;
	}
	assert(map-&gt;ref_count == 0);
	lck_mtx_unlock(&amp;map-&gt;s_lock);

#<span class="enscript-reference">if</span>	<span class="enscript-variable-name">TASK_SWAPPER</span>
	<span class="enscript-comment">/*
	 * The map residence count isn't decremented here because
	 * the vm_map_delete below will traverse the entire map, 
	 * deleting entries, and the residence counts on objects
	 * and sharing maps will go away then.
	 */</span>
#<span class="enscript-reference">endif</span>

	vm_map_destroy(map, VM_MAP_NO_FLAGS);
}


<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_disable_NX</span>(vm_map_t map)
{
        <span class="enscript-keyword">if</span> (map == NULL)
	        <span class="enscript-keyword">return</span>;
        <span class="enscript-keyword">if</span> (map-&gt;pmap == NULL)
	        <span class="enscript-keyword">return</span>;

        pmap_disable_NX(map-&gt;pmap);
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_disallow_data_exec</span>(vm_map_t map)
{
    <span class="enscript-keyword">if</span> (map == NULL)
        <span class="enscript-keyword">return</span>;

    map-&gt;map_disallow_data_exec = TRUE;
}

<span class="enscript-comment">/* XXX Consider making these constants (VM_MAX_ADDRESS and MACH_VM_MAX_ADDRESS)
 * more descriptive.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_set_32bit</span>(vm_map_t map)
{
	map-&gt;max_offset = (vm_map_offset_t)VM_MAX_ADDRESS;
}


<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_set_64bit</span>(vm_map_t map)
{
	map-&gt;max_offset = (vm_map_offset_t)MACH_VM_MAX_ADDRESS;
}

vm_map_offset_t
<span class="enscript-function-name">vm_compute_max_offset</span>(boolean_t is64)
{
	<span class="enscript-keyword">return</span> (is64 ? (vm_map_offset_t)MACH_VM_MAX_ADDRESS : (vm_map_offset_t)VM_MAX_ADDRESS);
}

uint64_t
<span class="enscript-function-name">vm_map_get_max_aslr_slide_pages</span>(vm_map_t map) 
{
	<span class="enscript-keyword">return</span> (1 &lt;&lt; (vm_map_is_64bit(map) ? 16 : 8));
}

boolean_t
<span class="enscript-function-name">vm_map_is_64bit</span>(
		vm_map_t map)
{
	<span class="enscript-keyword">return</span> map-&gt;max_offset &gt; ((vm_map_offset_t)VM_MAX_ADDRESS);
}

boolean_t
<span class="enscript-function-name">vm_map_has_hard_pagezero</span>(
		vm_map_t 	map,
		vm_map_offset_t	pagezero_size)
{
	<span class="enscript-comment">/*
	 * XXX FBDP
	 * We should lock the VM map (for read) here but we can get away
	 * with it for now because there can't really be any race condition:
	 * the VM map's min_offset is changed only when the VM map is created
	 * and when the zero page is established (when the binary gets loaded),
	 * and this routine gets called only when the task terminates and the
	 * VM map is being torn down, and when a new map is created via
	 * load_machfile()/execve().
	 */</span>
	<span class="enscript-keyword">return</span> (map-&gt;min_offset &gt;= pagezero_size);
}

<span class="enscript-comment">/*
 * Raise a VM map's maximun offset.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_raise_max_offset</span>(
	vm_map_t	map,
	vm_map_offset_t	new_max_offset)
{
	kern_return_t	ret;

	vm_map_lock(map);
	ret = KERN_INVALID_ADDRESS;

	<span class="enscript-keyword">if</span> (new_max_offset &gt;= map-&gt;max_offset) {
		<span class="enscript-keyword">if</span> (!vm_map_is_64bit(map)) { 
			<span class="enscript-keyword">if</span> (new_max_offset &lt;= (vm_map_offset_t)VM_MAX_ADDRESS) {
				map-&gt;max_offset = new_max_offset;
				ret = KERN_SUCCESS;
			}
		} <span class="enscript-keyword">else</span> {
			<span class="enscript-keyword">if</span> (new_max_offset &lt;= (vm_map_offset_t)MACH_VM_MAX_ADDRESS) {
				map-&gt;max_offset = new_max_offset;
				ret = KERN_SUCCESS;
			}
		}
	}

	vm_map_unlock(map);
	<span class="enscript-keyword">return</span> ret;
}


<span class="enscript-comment">/*
 * Raise a VM map's minimum offset.
 * To strictly enforce &quot;page zero&quot; reservation.
 */</span>
kern_return_t
<span class="enscript-function-name">vm_map_raise_min_offset</span>(
	vm_map_t	map,
	vm_map_offset_t	new_min_offset)
{
	vm_map_entry_t	first_entry;

	new_min_offset = vm_map_round_page(new_min_offset,
					   VM_MAP_PAGE_MASK(map));

	vm_map_lock(map);

	<span class="enscript-keyword">if</span> (new_min_offset &lt; map-&gt;min_offset) {
		<span class="enscript-comment">/*
		 * Can't move min_offset backwards, as that would expose
		 * a part of the address space that was previously, and for
		 * possibly good reasons, inaccessible.
		 */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}
	<span class="enscript-keyword">if</span> (new_min_offset &gt;= map-&gt;max_offset) {
		<span class="enscript-comment">/* can't go beyond the end of the address space */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ADDRESS;
	}

	first_entry = vm_map_first_entry(map);
	<span class="enscript-keyword">if</span> (first_entry != vm_map_to_entry(map) &amp;&amp;
	    first_entry-&gt;vme_start &lt; new_min_offset) {
		<span class="enscript-comment">/*
		 * Some memory was already allocated below the new
		 * minimun offset.  It's too late to change it now...
		 */</span>
		vm_map_unlock(map);
		<span class="enscript-keyword">return</span> KERN_NO_SPACE;
	}

	map-&gt;min_offset = new_min_offset;

	assert(map-&gt;holes_list);
	map-&gt;holes_list-&gt;start = new_min_offset;
	assert(new_min_offset &lt; map-&gt;holes_list-&gt;end);

	vm_map_unlock(map);

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-comment">/*
 * Set the limit on the maximum amount of user wired memory allowed for this map.
 * This is basically a copy of the MEMLOCK rlimit value maintained by the BSD side of
 * the kernel.  The limits are checked in the mach VM side, so we keep a copy so we
 * don't have to reach over to the BSD data structures.
 */</span>

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_set_user_wire_limit</span>(vm_map_t 	map,
			   vm_size_t	limit)
{
	map-&gt;user_wire_limit = limit;
}


<span class="enscript-type">void</span> <span class="enscript-function-name">vm_map_switch_protect</span>(vm_map_t	map, 
			   boolean_t	val) 
{
	vm_map_lock(map);
	map-&gt;switch_protect=val;
	vm_map_unlock(map);
}

<span class="enscript-comment">/*
 * IOKit has mapped a region into this map; adjust the pmap's ledgers appropriately.
 * phys_footprint is a composite limit consisting of iokit + physmem, so we need to
 * bump both counters.
 */</span>
<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_iokit_mapped_region</span>(vm_map_t map, vm_size_t bytes)
{
	pmap_t pmap = vm_map_pmap(map);

	ledger_credit(pmap-&gt;ledger, task_ledgers.iokit_mapped, bytes);
	ledger_credit(pmap-&gt;ledger, task_ledgers.phys_footprint, bytes);		
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_iokit_unmapped_region</span>(vm_map_t map, vm_size_t bytes)
{
	pmap_t pmap = vm_map_pmap(map);

	ledger_debit(pmap-&gt;ledger, task_ledgers.iokit_mapped, bytes);
	ledger_debit(pmap-&gt;ledger, task_ledgers.phys_footprint, bytes);	
}

<span class="enscript-comment">/* Add (generate) code signature for memory range */</span>
#<span class="enscript-reference">if</span> <span class="enscript-variable-name">CONFIG_DYNAMIC_CODE_SIGNING</span>
kern_return_t <span class="enscript-function-name">vm_map_sign</span>(vm_map_t map, 
		 vm_map_offset_t start, 
		 vm_map_offset_t end)
{
	vm_map_entry_t entry;
	vm_page_t m;
	vm_object_t object;
	
	<span class="enscript-comment">/*
	 * Vet all the input parameters and current type and state of the
	 * underlaying object.  Return with an error if anything is amiss.
	 */</span>
	<span class="enscript-keyword">if</span> (map == VM_MAP_NULL)
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);
		
	vm_map_lock_read(map);
	
	<span class="enscript-keyword">if</span> (!vm_map_lookup_entry(map, start, &amp;entry) || entry-&gt;is_sub_map) {
		<span class="enscript-comment">/*
		 * Must pass a valid non-submap address.
		 */</span>
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ADDRESS);
	}
	
	<span class="enscript-keyword">if</span>((entry-&gt;vme_start &gt; start) || (entry-&gt;vme_end &lt; end)) {
		<span class="enscript-comment">/*
		 * Map entry doesn't cover the requested range. Not handling
		 * this situation currently.
		 */</span>
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span>(KERN_INVALID_ARGUMENT);
	}
	
	object = VME_OBJECT(entry);
	<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
		<span class="enscript-comment">/*
		 * Object must already be present or we can't sign.
		 */</span>
		vm_map_unlock_read(map);
		<span class="enscript-keyword">return</span> KERN_INVALID_ARGUMENT;
	}
	
	vm_object_lock(object);
	vm_map_unlock_read(map);
	
	<span class="enscript-keyword">while</span>(start &lt; end) {
		uint32_t refmod;
		
		m = vm_page_lookup(object,
				   start - entry-&gt;vme_start + VME_OFFSET(entry));
		<span class="enscript-keyword">if</span> (m==VM_PAGE_NULL) {
			<span class="enscript-comment">/* shoud we try to fault a page here? we can probably 
			 * demand it exists and is locked for this request */</span>
			vm_object_unlock(object);
			<span class="enscript-keyword">return</span> KERN_FAILURE;
		}
		<span class="enscript-comment">/* deal with special page status */</span>
		<span class="enscript-keyword">if</span> (m-&gt;busy || 
		    (m-&gt;unusual &amp;&amp; (m-&gt;error || m-&gt;restart || m-&gt;private || m-&gt;absent))) {
			vm_object_unlock(object);
			<span class="enscript-keyword">return</span> KERN_FAILURE;
		}
		
		<span class="enscript-comment">/* Page is OK... now &quot;validate&quot; it */</span>
		<span class="enscript-comment">/* This is the place where we'll call out to create a code 
		 * directory, later */</span>
		m-&gt;cs_validated = TRUE;

		<span class="enscript-comment">/* The page is now &quot;clean&quot; for codesigning purposes. That means
		 * we don't consider it as modified (wpmapped) anymore. But 
		 * we'll disconnect the page so we note any future modification
		 * attempts. */</span>
		m-&gt;wpmapped = FALSE;
		refmod = pmap_disconnect(m-&gt;phys_page);
		
		<span class="enscript-comment">/* Pull the dirty status from the pmap, since we cleared the 
		 * wpmapped bit */</span>
		<span class="enscript-keyword">if</span> ((refmod &amp; VM_MEM_MODIFIED) &amp;&amp; !m-&gt;dirty) {
			SET_PAGE_DIRTY(m, FALSE);
		}
		
		<span class="enscript-comment">/* On to the next page */</span>
		start += PAGE_SIZE;
	}
	vm_object_unlock(object);
	
	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}
#<span class="enscript-reference">endif</span>

kern_return_t <span class="enscript-function-name">vm_map_partial_reap</span>(vm_map_t map, <span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *reclaimed_resident, <span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *reclaimed_compressed)
{	
	vm_map_entry_t	entry = VM_MAP_ENTRY_NULL;
	vm_map_entry_t next_entry;
	kern_return_t	kr = KERN_SUCCESS;
	vm_map_t 	zap_map;

	vm_map_lock(map);

	<span class="enscript-comment">/*
	 * We use a &quot;zap_map&quot; to avoid having to unlock
	 * the &quot;map&quot; in vm_map_delete().
	 */</span>
	zap_map = vm_map_create(PMAP_NULL,
				map-&gt;min_offset,
				map-&gt;max_offset,
				map-&gt;hdr.entries_pageable);

	<span class="enscript-keyword">if</span> (zap_map == VM_MAP_NULL) {
		<span class="enscript-keyword">return</span> KERN_RESOURCE_SHORTAGE;
	}

	vm_map_set_page_shift(zap_map, 
			      VM_MAP_PAGE_SHIFT(map));
	vm_map_disable_hole_optimization(zap_map);

	<span class="enscript-keyword">for</span> (entry = vm_map_first_entry(map);
	     entry != vm_map_to_entry(map);
	     entry = next_entry) {
		next_entry = entry-&gt;vme_next;
		
		<span class="enscript-keyword">if</span> (VME_OBJECT(entry) &amp;&amp;
		    !entry-&gt;is_sub_map &amp;&amp;
		    (VME_OBJECT(entry)-&gt;internal == TRUE) &amp;&amp;
		    (VME_OBJECT(entry)-&gt;ref_count == 1)) {

			*reclaimed_resident += VME_OBJECT(entry)-&gt;resident_page_count;
			*reclaimed_compressed += vm_compressor_pager_get_count(VME_OBJECT(entry)-&gt;pager);

			(<span class="enscript-type">void</span>)vm_map_delete(map, 
					    entry-&gt;vme_start, 
					    entry-&gt;vme_end, 
					    VM_MAP_REMOVE_SAVE_ENTRIES,
					    zap_map);
		}
	}

	vm_map_unlock(map);

        <span class="enscript-comment">/*
	 * Get rid of the &quot;zap_maps&quot; and all the map entries that
         * they may still contain.
         */</span>
        <span class="enscript-keyword">if</span> (zap_map != VM_MAP_NULL) {
                vm_map_destroy(zap_map, VM_MAP_REMOVE_NO_PMAP_CLEANUP);
                zap_map = VM_MAP_NULL;
        }

	<span class="enscript-keyword">return</span> kr;
}

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">CONFIG_FREEZE</span>

kern_return_t <span class="enscript-function-name">vm_map_freeze_walk</span>(
             	vm_map_t map,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *purgeable_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *wired_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *clean_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *dirty_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>  dirty_budget,
             	boolean_t *has_shared)
{
	vm_map_entry_t entry;
	
	vm_map_lock_read(map);
	
	*purgeable_count = *wired_count = *clean_count = *dirty_count = 0;
	*has_shared = FALSE;
	
	<span class="enscript-keyword">for</span> (entry = vm_map_first_entry(map);
	     entry != vm_map_to_entry(map);
	     entry = entry-&gt;vme_next) {
		<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> purgeable, clean, dirty, wired;
		boolean_t shared;

		<span class="enscript-keyword">if</span> ((VME_OBJECT(entry) == 0) ||
		    (entry-&gt;is_sub_map) ||
		    (VME_OBJECT(entry)-&gt;phys_contiguous)) {
			<span class="enscript-keyword">continue</span>;
		}

		default_freezer_pack(&amp;purgeable, &amp;wired, &amp;clean, &amp;dirty, dirty_budget, &amp;shared, VME_OBJECT(entry), NULL);
		
		*purgeable_count += purgeable;
		*wired_count += wired;
		*clean_count += clean;
		*dirty_count += dirty;
		
		<span class="enscript-keyword">if</span> (shared) {
			*has_shared = TRUE;
		}
		
		<span class="enscript-comment">/* Adjust pageout budget and finish up if reached */</span>
		<span class="enscript-keyword">if</span> (dirty_budget) {
			dirty_budget -= dirty;
			<span class="enscript-keyword">if</span> (dirty_budget == 0) {
				<span class="enscript-keyword">break</span>;
			}
		}
	}

	vm_map_unlock_read(map);

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-type">int</span> c_freezer_swapout_count;
<span class="enscript-type">int</span> c_freezer_compression_count = 0;
AbsoluteTime c_freezer_last_yield_ts = 0;

kern_return_t <span class="enscript-function-name">vm_map_freeze</span>(
             	vm_map_t map,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *purgeable_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *wired_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *clean_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> *dirty_count,
             	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> dirty_budget,
             	boolean_t *has_shared)
{	
	vm_map_entry_t	entry2 = VM_MAP_ENTRY_NULL;
	kern_return_t	kr = KERN_SUCCESS;
	boolean_t	default_freezer_active = TRUE;

	*purgeable_count = *wired_count = *clean_count = *dirty_count = 0;
	*has_shared = FALSE;

	<span class="enscript-comment">/*
	 * We need the exclusive lock here so that we can
	 * block any page faults or lookups while we are
	 * in the middle of freezing this vm map.
	 */</span>
	vm_map_lock(map);

	<span class="enscript-keyword">if</span> (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
		default_freezer_active = FALSE;
				
		<span class="enscript-keyword">if</span> (vm_compressor_low_on_space() || vm_swap_low_on_space()) {
			kr = KERN_NO_SPACE;
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;	
		}
	}
	assert(default_freezer_active == FALSE);
	
	<span class="enscript-keyword">if</span> (default_freezer_active) {
		<span class="enscript-keyword">if</span> (map-&gt;default_freezer_handle == NULL) {	
			map-&gt;default_freezer_handle = default_freezer_handle_allocate();
		}
	
		<span class="enscript-keyword">if</span> ((kr = default_freezer_handle_init(map-&gt;default_freezer_handle)) != KERN_SUCCESS) {
			<span class="enscript-comment">/*
			 * Can happen if default_freezer_handle passed in is NULL
			 * Or, a table has already been allocated and associated
			 * with this handle, i.e. the map is already frozen.
			 */</span>
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">done</span>;
		}
	}
	c_freezer_compression_count = 0;
	clock_get_uptime(&amp;c_freezer_last_yield_ts);

	<span class="enscript-keyword">for</span> (entry2 = vm_map_first_entry(map);
	     entry2 != vm_map_to_entry(map);
	     entry2 = entry2-&gt;vme_next) {
	
		vm_object_t	src_object = VME_OBJECT(entry2);

		<span class="enscript-keyword">if</span> (VME_OBJECT(entry2) &amp;&amp;
		    !entry2-&gt;is_sub_map &amp;&amp;
		    !VME_OBJECT(entry2)-&gt;phys_contiguous) {
			<span class="enscript-comment">/* If eligible, scan the entry, moving eligible pages over to our parent object */</span>
			<span class="enscript-keyword">if</span> (default_freezer_active) {
				<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span> purgeable, clean, dirty, wired;
				boolean_t shared;
			
				default_freezer_pack(&amp;purgeable, &amp;wired, &amp;clean, &amp;dirty, dirty_budget, &amp;shared,
								src_object, map-&gt;default_freezer_handle);
										 
				*purgeable_count += purgeable;
				*wired_count += wired;
				*clean_count += clean;
				*dirty_count += dirty;
				
				<span class="enscript-comment">/* Adjust pageout budget and finish up if reached */</span>
				<span class="enscript-keyword">if</span> (dirty_budget) {
					dirty_budget -= dirty;
					<span class="enscript-keyword">if</span> (dirty_budget == 0) {
						<span class="enscript-keyword">break</span>;
					}
				}

				<span class="enscript-keyword">if</span> (shared) {
					*has_shared = TRUE;
				}
			} <span class="enscript-keyword">else</span> {
				<span class="enscript-keyword">if</span> (VME_OBJECT(entry2)-&gt;internal == TRUE) {
					
					<span class="enscript-keyword">if</span> (DEFAULT_FREEZER_COMPRESSED_PAGER_IS_SWAPBACKED) {
						<span class="enscript-comment">/*
						 * Pages belonging to this object could be swapped to disk.
						 * Make sure it's not a shared object because we could end
						 * up just bringing it back in again.
						 */</span>
						<span class="enscript-keyword">if</span> (VME_OBJECT(entry2)-&gt;ref_count &gt; 1) {
							<span class="enscript-keyword">continue</span>;
						}
					}
					vm_object_compressed_freezer_pageout(VME_OBJECT(entry2));
				}

				<span class="enscript-keyword">if</span> (vm_compressor_low_on_space() || vm_swap_low_on_space()) {
					kr = KERN_NO_SPACE;
					<span class="enscript-keyword">break</span>;	
				}
			}
		}
	}

	<span class="enscript-keyword">if</span> (default_freezer_active) {
		<span class="enscript-comment">/* Finally, throw out the pages to swap */</span>
		default_freezer_pageout(map-&gt;default_freezer_handle);
	}

<span class="enscript-reference">done</span>:
	vm_map_unlock(map);
	
	<span class="enscript-keyword">if</span> (!default_freezer_active) {
		vm_object_compressed_freezer_done();
	}
	<span class="enscript-keyword">if</span> (DEFAULT_FREEZER_COMPRESSED_PAGER_IS_SWAPBACKED) {
		<span class="enscript-comment">/*
		 * reset the counter tracking the # of swapped c_segs
		 * because we are now done with this freeze session and task.
		 */</span>
		c_freezer_swapout_count = 0;
	}
	<span class="enscript-keyword">return</span> kr;
}

kern_return_t
<span class="enscript-function-name">vm_map_thaw</span>(
	vm_map_t map)
{
	kern_return_t kr = KERN_SUCCESS;

	<span class="enscript-keyword">if</span> (COMPRESSED_PAGER_IS_ACTIVE || DEFAULT_FREEZER_COMPRESSED_PAGER_IS_ACTIVE) {
		<span class="enscript-comment">/*
		 * We will on-demand thaw in the presence of the compressed pager.
		 */</span>
		<span class="enscript-keyword">return</span> kr;
	}

	vm_map_lock(map);

	<span class="enscript-keyword">if</span> (map-&gt;default_freezer_handle == NULL) {
		<span class="enscript-comment">/*
		 * This map is not in a frozen state.
		 */</span>
		kr = KERN_FAILURE;		
		<span class="enscript-keyword">goto</span> <span class="enscript-reference">out</span>;
	}

	kr = default_freezer_unpack(map-&gt;default_freezer_handle);	
<span class="enscript-reference">out</span>:
	vm_map_unlock(map);
	
	<span class="enscript-keyword">return</span> kr;
}
#<span class="enscript-reference">endif</span>

<span class="enscript-comment">/*
 * vm_map_entry_should_cow_for_true_share:
 *
 * Determines if the map entry should be clipped and setup for copy-on-write
 * to avoid applying &quot;true_share&quot; to a large VM object when only a subset is
 * targeted.
 *
 * For now, we target only the map entries created for the Objective C
 * Garbage Collector, which initially have the following properties:
 *	- alias == VM_MEMORY_MALLOC
 * 	- wired_count == 0
 * 	- !needs_copy
 * and a VM object with:
 * 	- internal
 * 	- copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC
 * 	- !true_share
 * 	- vo_size == ANON_CHUNK_SIZE
 *
 * Only non-kernel map entries.
 */</span>
boolean_t
<span class="enscript-function-name">vm_map_entry_should_cow_for_true_share</span>(
	vm_map_entry_t	entry)
{
	vm_object_t	object;

	<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
		<span class="enscript-comment">/* entry does not point at a VM object */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (entry-&gt;needs_copy) {
		<span class="enscript-comment">/* already set for copy_on_write: done! */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (VME_ALIAS(entry) != VM_MEMORY_MALLOC &amp;&amp;
	    VME_ALIAS(entry) != VM_MEMORY_MALLOC_SMALL) {
		<span class="enscript-comment">/* not a malloc heap or Obj-C Garbage Collector heap */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (entry-&gt;wired_count) {
		<span class="enscript-comment">/* wired: can't change the map entry... */</span>
		vm_counters.should_cow_but_wired++;
		<span class="enscript-keyword">return</span> FALSE;
	}

	object = VME_OBJECT(entry);

	<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
		<span class="enscript-comment">/* no object yet... */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (!object-&gt;internal) {
		<span class="enscript-comment">/* not an internal object */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (object-&gt;copy_strategy != MEMORY_OBJECT_COPY_SYMMETRIC) {
		<span class="enscript-comment">/* not the default copy strategy */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (object-&gt;true_share) {
		<span class="enscript-comment">/* already true_share: too late to avoid it */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (VME_ALIAS(entry) == VM_MEMORY_MALLOC &amp;&amp;
	    object-&gt;vo_size != ANON_CHUNK_SIZE) {
		<span class="enscript-comment">/* ... not an object created for the ObjC Garbage Collector */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-keyword">if</span> (VME_ALIAS(entry) == VM_MEMORY_MALLOC_SMALL &amp;&amp;
	    object-&gt;vo_size != 2048 * 4096) {
		<span class="enscript-comment">/* ... not a &quot;MALLOC_SMALL&quot; heap */</span>
		<span class="enscript-keyword">return</span> FALSE;
	}

	<span class="enscript-comment">/*
	 * All the criteria match: we have a large object being targeted for &quot;true_share&quot;.
	 * To limit the adverse side-effects linked with &quot;true_share&quot;, tell the caller to
	 * try and avoid setting up the entire object for &quot;true_share&quot; by clipping the
	 * targeted range and setting it up for copy-on-write.
	 */</span>
	<span class="enscript-keyword">return</span> TRUE;
}

vm_map_offset_t	
<span class="enscript-function-name">vm_map_round_page_mask</span>(
 	vm_map_offset_t	offset,
	vm_map_offset_t	mask)
{
	<span class="enscript-keyword">return</span> VM_MAP_ROUND_PAGE(offset, mask);
}

vm_map_offset_t	
<span class="enscript-function-name">vm_map_trunc_page_mask</span>(
	vm_map_offset_t	offset,
	vm_map_offset_t	mask)
{
	<span class="enscript-keyword">return</span> VM_MAP_TRUNC_PAGE(offset, mask);
}

boolean_t
<span class="enscript-function-name">vm_map_page_aligned</span>(
	vm_map_offset_t	offset,
	vm_map_offset_t	mask)
{
	<span class="enscript-keyword">return</span> ((offset) &amp; mask) == 0;
}

<span class="enscript-type">int</span>
<span class="enscript-function-name">vm_map_page_shift</span>(
	vm_map_t map)
{
	<span class="enscript-keyword">return</span> VM_MAP_PAGE_SHIFT(map);
}

<span class="enscript-type">int</span>
<span class="enscript-function-name">vm_map_page_size</span>(
	vm_map_t map)
{
	<span class="enscript-keyword">return</span> VM_MAP_PAGE_SIZE(map);
}

vm_map_offset_t
<span class="enscript-function-name">vm_map_page_mask</span>(
	vm_map_t map)
{
	<span class="enscript-keyword">return</span> VM_MAP_PAGE_MASK(map);
}

kern_return_t
<span class="enscript-function-name">vm_map_set_page_shift</span>(
	vm_map_t  	map,
	<span class="enscript-type">int</span>		pageshift)
{
	<span class="enscript-keyword">if</span> (map-&gt;hdr.nentries != 0) {
		<span class="enscript-comment">/* too late to change page size */</span>
		<span class="enscript-keyword">return</span> KERN_FAILURE;
	}

	map-&gt;hdr.page_shift = pageshift;

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-type">int</span>
<span class="enscript-function-name">vm_map_purge</span>(
	vm_map_t	map)
{
	<span class="enscript-type">int</span>		num_object_purged;
	vm_map_entry_t	entry;
	vm_map_offset_t	next_address;
	vm_object_t	object;
	<span class="enscript-type">int</span>		state;
	kern_return_t	kr;

	num_object_purged = 0;

	vm_map_lock_read(map);
	entry = vm_map_first_entry(map);
	<span class="enscript-keyword">while</span> (entry != vm_map_to_entry(map)) {
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">next</span>;
		}
		<span class="enscript-keyword">if</span> (! (entry-&gt;protection &amp; VM_PROT_WRITE)) {
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">next</span>;
		}
		object = VME_OBJECT(entry);
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">next</span>;
		}
		<span class="enscript-keyword">if</span> (object-&gt;purgable != VM_PURGABLE_VOLATILE) {
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">next</span>;
		}

		vm_object_lock(object);
#<span class="enscript-reference">if</span> 00
		<span class="enscript-keyword">if</span> (VME_OFFSET(entry) != 0 ||
		    (entry-&gt;vme_end - entry-&gt;vme_start) != object-&gt;vo_size) {
			vm_object_unlock(object);
			<span class="enscript-keyword">goto</span> <span class="enscript-reference">next</span>;
		}
#<span class="enscript-reference">endif</span>
		next_address = entry-&gt;vme_end;
		vm_map_unlock_read(map);
		state = VM_PURGABLE_EMPTY;
		kr = vm_object_purgable_control(object,
						VM_PURGABLE_SET_STATE,
						&amp;state);
		<span class="enscript-keyword">if</span> (kr == KERN_SUCCESS) {
			num_object_purged++;
		}
		vm_object_unlock(object);

		vm_map_lock_read(map);
		<span class="enscript-keyword">if</span> (vm_map_lookup_entry(map, next_address, &amp;entry)) {
			<span class="enscript-keyword">continue</span>;
		}
	<span class="enscript-reference">next</span>:
		entry = entry-&gt;vme_next;
	}
	vm_map_unlock_read(map);

	<span class="enscript-keyword">return</span> num_object_purged;
}

kern_return_t
<span class="enscript-function-name">vm_map_query_volatile</span>(
	vm_map_t	map,
	mach_vm_size_t	*volatile_virtual_size_p,
	mach_vm_size_t	*volatile_resident_size_p,
	mach_vm_size_t	*volatile_compressed_size_p,
	mach_vm_size_t	*volatile_pmap_size_p,
	mach_vm_size_t	*volatile_compressed_pmap_size_p)
{
	mach_vm_size_t	volatile_virtual_size;
	mach_vm_size_t	volatile_resident_count;
	mach_vm_size_t	volatile_compressed_count;
	mach_vm_size_t	volatile_pmap_count;
	mach_vm_size_t	volatile_compressed_pmap_count;
	mach_vm_size_t	resident_count;
	<span class="enscript-type">unsigned</span> <span class="enscript-type">int</span>	compressed_count;
	vm_map_entry_t	entry;
	vm_object_t	object;

	<span class="enscript-comment">/* map should be locked by caller */</span>

	volatile_virtual_size = 0;
	volatile_resident_count = 0;
	volatile_compressed_count = 0;
	volatile_pmap_count = 0;
	volatile_compressed_pmap_count = 0;

	<span class="enscript-keyword">for</span> (entry = vm_map_first_entry(map);
	     entry != vm_map_to_entry(map);
	     entry = entry-&gt;vme_next) {
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-keyword">if</span> (! (entry-&gt;protection &amp; VM_PROT_WRITE)) {
			<span class="enscript-keyword">continue</span>;
		}
		object = VME_OBJECT(entry);
		<span class="enscript-keyword">if</span> (object == VM_OBJECT_NULL) {
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-keyword">if</span> (object-&gt;purgable != VM_PURGABLE_VOLATILE &amp;&amp;
		    object-&gt;purgable != VM_PURGABLE_EMPTY) {
			<span class="enscript-keyword">continue</span>;
		}
		<span class="enscript-keyword">if</span> (VME_OFFSET(entry)) {
			<span class="enscript-comment">/*
			 * If the map entry has been split and the object now
			 * appears several times in the VM map, we don't want
			 * to count the object's resident_page_count more than
			 * once.  We count it only for the first one, starting
			 * at offset 0 and ignore the other VM map entries.
			 */</span>
			<span class="enscript-keyword">continue</span>;
		}
		resident_count = object-&gt;resident_page_count;
		<span class="enscript-keyword">if</span> ((VME_OFFSET(entry) / PAGE_SIZE) &gt;= resident_count) {
			resident_count = 0;
		} <span class="enscript-keyword">else</span> {
			resident_count -= (VME_OFFSET(entry) / PAGE_SIZE);
		}

		volatile_virtual_size += entry-&gt;vme_end - entry-&gt;vme_start;
		volatile_resident_count += resident_count;
		<span class="enscript-keyword">if</span> (object-&gt;pager) {
			volatile_compressed_count +=
				vm_compressor_pager_get_count(object-&gt;pager);
		}
		compressed_count = 0;
		volatile_pmap_count += pmap_query_resident(map-&gt;pmap,
							   entry-&gt;vme_start,
							   entry-&gt;vme_end,
							   &amp;compressed_count);
		volatile_compressed_pmap_count += compressed_count;
	}

	<span class="enscript-comment">/* map is still locked on return */</span>

	*volatile_virtual_size_p = volatile_virtual_size;
	*volatile_resident_size_p = volatile_resident_count * PAGE_SIZE;
	*volatile_compressed_size_p = volatile_compressed_count * PAGE_SIZE;
	*volatile_pmap_size_p = volatile_pmap_count * PAGE_SIZE;
	*volatile_compressed_pmap_size_p = volatile_compressed_pmap_count * PAGE_SIZE;

	<span class="enscript-keyword">return</span> KERN_SUCCESS;
}

<span class="enscript-type">void</span>
<span class="enscript-function-name">vm_map_sizes</span>(vm_map_t map,
		vm_map_size_t * psize,
		vm_map_size_t * pfree,
		vm_map_size_t * plargest_free)
{
    vm_map_entry_t  entry;
    vm_map_offset_t prev;
    vm_map_size_t   free, total_free, largest_free;
    boolean_t       end;

    total_free = largest_free = 0;

    vm_map_lock_read(map);
    <span class="enscript-keyword">if</span> (psize) *psize = map-&gt;max_offset - map-&gt;min_offset;

    prev = map-&gt;min_offset;
    <span class="enscript-keyword">for</span> (entry = vm_map_first_entry(map);; entry = entry-&gt;vme_next)
    {
	end = (entry == vm_map_to_entry(map));

	<span class="enscript-keyword">if</span> (end) free = entry-&gt;vme_end   - prev;
	<span class="enscript-keyword">else</span>     free = entry-&gt;vme_start - prev;

	total_free += free;
	<span class="enscript-keyword">if</span> (free &gt; largest_free) largest_free = free;

	<span class="enscript-keyword">if</span> (end) <span class="enscript-keyword">break</span>;
	prev = entry-&gt;vme_end;
    }
    vm_map_unlock_read(map);
    <span class="enscript-keyword">if</span> (pfree)         *pfree = total_free;
    <span class="enscript-keyword">if</span> (plargest_free) *plargest_free = largest_free;
}

#<span class="enscript-reference">if</span> <span class="enscript-variable-name">VM_SCAN_FOR_SHADOW_CHAIN</span>
<span class="enscript-type">int</span> <span class="enscript-function-name">vm_map_shadow_max</span>(vm_map_t map);
<span class="enscript-type">int</span> <span class="enscript-function-name">vm_map_shadow_max</span>(
	vm_map_t map)
{
	<span class="enscript-type">int</span>		shadows, shadows_max;
	vm_map_entry_t	entry;
	vm_object_t	object, next_object;

	<span class="enscript-keyword">if</span> (map == NULL)
		<span class="enscript-keyword">return</span> 0;

	shadows_max = 0;

	vm_map_lock_read(map);
	
	<span class="enscript-keyword">for</span> (entry = vm_map_first_entry(map);
	     entry != vm_map_to_entry(map);
	     entry = entry-&gt;vme_next) {
		<span class="enscript-keyword">if</span> (entry-&gt;is_sub_map) {
			<span class="enscript-keyword">continue</span>;
		}
		object = VME_OBJECT(entry);
		<span class="enscript-keyword">if</span> (object == NULL) {
			<span class="enscript-keyword">continue</span>;
		}
		vm_object_lock_shared(object);
		<span class="enscript-keyword">for</span> (shadows = 0;
		     object-&gt;shadow != NULL;
		     shadows++, object = next_object) {
			next_object = object-&gt;shadow;
			vm_object_lock_shared(next_object);
			vm_object_unlock(object);
		}
		vm_object_unlock(object);
		<span class="enscript-keyword">if</span> (shadows &gt; shadows_max) {
			shadows_max = shadows;
		}
	}

	vm_map_unlock_read(map);

	<span class="enscript-keyword">return</span> shadows_max;
}
#<span class="enscript-reference">endif</span> <span class="enscript-comment">/* VM_SCAN_FOR_SHADOW_CHAIN */</span>
</pre>
<hr />
</body></html>